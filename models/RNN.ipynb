{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0df2c52f-23eb-48e4-ad5a-a206809eb210",
   "metadata": {},
   "source": [
    "## Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f270a44d-8d25-4c5f-9ed6-a77eca0e21ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944917fa-0623-4d69-8b2c-6e0de6e80409",
   "metadata": {},
   "source": [
    "## Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d3867f0-613c-4e8a-87e6-e96621e4f89d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed train data shape: (17731, 30, 14)\n",
      "processed train target shape: (17731,)\n",
      "processed test data shape: (100, 30, 14)\n",
      "true rul shape: (100,)\n"
     ]
    }
   ],
   "source": [
    "# define the directory where the pickle files are stored\n",
    "folder_path = '../data/batched_data_pickle_files/'\n",
    "\n",
    "# define the filenames for the pickle files\n",
    "file_names = ['processed_train_data.pkl', 'processed_train_targets.pkl', 'processed_test_data.pkl', 'true_rul.pkl']\n",
    "\n",
    "# loop through each file and load its contents as arrays\n",
    "for file_name in file_names:\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    \n",
    "    # read the pickle file\n",
    "    with open(file_path, 'rb') as file:\n",
    "        data = pickle.load(file)\n",
    "    \n",
    "    # ensure data is a numpy array, if not convert it\n",
    "    if isinstance(data, np.ndarray):\n",
    "        globals()[file_name.replace('.pkl', '')] = data\n",
    "    else:\n",
    "        globals()[file_name.replace('.pkl', '')] = np.array(data)\n",
    "\n",
    "# print the shapes of the loaded data\n",
    "print(\"processed train data shape:\", processed_train_data.shape)\n",
    "print(\"processed train target shape:\", processed_train_targets.shape)\n",
    "print(\"processed test data shape:\", processed_test_data.shape)\n",
    "print(\"true rul shape:\", true_rul.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259d4aa2-b2ee-4aaa-883e-6beddf9acfbb",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "327f4e3d-42e1-4eb1-b971-a42a4e2db834",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-21 07:14:31,254] A new study created in memory with name: no-name-bfeaf0e5-a313-4091-b218-438f36625d08\n",
      "/tmp/ipykernel_9994/934372295.py:32: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)  # Learning rate\n",
      "[I 2024-11-21 07:15:39,638] Trial 0 finished with value: 18.26929014223116 and parameters: {'hidden_size': 128, 'num_layers': 1, 'learning_rate': 0.0034954850746481783, 'batch_size': 32}. Best is trial 0 with value: 18.26929014223116.\n",
      "[I 2024-11-21 07:19:11,973] Trial 1 finished with value: 81.99670818873814 and parameters: {'hidden_size': 128, 'num_layers': 3, 'learning_rate': 1.2913086955327018e-05, 'batch_size': 128}. Best is trial 0 with value: 18.26929014223116.\n",
      "[I 2024-11-21 07:20:07,505] Trial 2 finished with value: 81.99715151105609 and parameters: {'hidden_size': 32, 'num_layers': 3, 'learning_rate': 8.4153976329118e-05, 'batch_size': 128}. Best is trial 0 with value: 18.26929014223116.\n",
      "[I 2024-11-21 07:29:03,564] Trial 3 finished with value: 50.597114426749094 and parameters: {'hidden_size': 128, 'num_layers': 3, 'learning_rate': 0.00039970812321555644, 'batch_size': 64}. Best is trial 0 with value: 18.26929014223116.\n",
      "[I 2024-11-21 07:29:50,160] Trial 4 finished with value: 70.27797753470284 and parameters: {'hidden_size': 64, 'num_layers': 1, 'learning_rate': 7.141529949888368e-05, 'batch_size': 64}. Best is trial 0 with value: 18.26929014223116.\n",
      "[I 2024-11-21 07:36:52,542] Trial 5 finished with value: 17.645047282313442 and parameters: {'hidden_size': 128, 'num_layers': 3, 'learning_rate': 0.0001206687008047001, 'batch_size': 32}. Best is trial 5 with value: 17.645047282313442.\n",
      "[I 2024-11-21 07:38:19,635] Trial 6 finished with value: 17.949977108410426 and parameters: {'hidden_size': 32, 'num_layers': 3, 'learning_rate': 0.0023079985287492904, 'batch_size': 64}. Best is trial 5 with value: 17.645047282313442.\n",
      "[I 2024-11-21 07:43:46,776] Trial 7 finished with value: 77.13022586277553 and parameters: {'hidden_size': 128, 'num_layers': 3, 'learning_rate': 1.623259116355229e-05, 'batch_size': 64}. Best is trial 5 with value: 17.645047282313442.\n",
      "[I 2024-11-21 07:46:21,950] Trial 8 finished with value: 20.39316850095182 and parameters: {'hidden_size': 128, 'num_layers': 2, 'learning_rate': 0.0046546132872746625, 'batch_size': 32}. Best is trial 5 with value: 17.645047282313442.\n",
      "[I 2024-11-21 07:47:35,567] Trial 9 finished with value: 83.65900938851493 and parameters: {'hidden_size': 64, 'num_layers': 2, 'learning_rate': 2.4670463977309012e-05, 'batch_size': 128}. Best is trial 5 with value: 17.645047282313442.\n",
      "[I 2024-11-21 07:49:43,138] Trial 10 finished with value: 23.597743936487145 and parameters: {'hidden_size': 96, 'num_layers': 2, 'learning_rate': 0.0004137006128711413, 'batch_size': 96}. Best is trial 5 with value: 17.645047282313442.\n",
      "[I 2024-11-21 07:51:56,018] Trial 11 finished with value: 14.427471349905202 and parameters: {'hidden_size': 32, 'num_layers': 3, 'learning_rate': 0.0008968894969160791, 'batch_size': 32}. Best is trial 11 with value: 14.427471349905202.\n",
      "[I 2024-11-21 07:56:06,703] Trial 12 finished with value: 17.620343672262656 and parameters: {'hidden_size': 96, 'num_layers': 3, 'learning_rate': 0.001068664952566299, 'batch_size': 32}. Best is trial 11 with value: 14.427471349905202.\n",
      "[I 2024-11-21 07:58:13,043] Trial 13 finished with value: 17.594926851289767 and parameters: {'hidden_size': 96, 'num_layers': 2, 'learning_rate': 0.0011438014661926264, 'batch_size': 32}. Best is trial 11 with value: 14.427471349905202.\n",
      "[I 2024-11-21 07:59:24,994] Trial 14 finished with value: 18.404775207107132 and parameters: {'hidden_size': 64, 'num_layers': 2, 'learning_rate': 0.0009519192397393097, 'batch_size': 96}. Best is trial 11 with value: 14.427471349905202.\n",
      "[I 2024-11-21 08:00:29,511] Trial 15 finished with value: 17.879953367216093 and parameters: {'hidden_size': 96, 'num_layers': 1, 'learning_rate': 0.0014226107959127285, 'batch_size': 32}. Best is trial 11 with value: 14.427471349905202.\n",
      "[I 2024-11-21 08:01:57,261] Trial 16 finished with value: 18.23255302669766 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.006871417103432853, 'batch_size': 32}. Best is trial 11 with value: 14.427471349905202.\n",
      "[I 2024-11-21 08:03:14,385] Trial 17 finished with value: 19.45039326435811 and parameters: {'hidden_size': 64, 'num_layers': 2, 'learning_rate': 0.0006048369597145335, 'batch_size': 96}. Best is trial 11 with value: 14.427471349905202.\n",
      "[I 2024-11-21 08:04:19,659] Trial 18 finished with value: 28.088258232389176 and parameters: {'hidden_size': 96, 'num_layers': 1, 'learning_rate': 0.00020057041950444714, 'batch_size': 64}. Best is trial 11 with value: 14.427471349905202.\n",
      "[I 2024-11-21 08:05:46,101] Trial 19 finished with value: 16.781884485536867 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.002239813619350319, 'batch_size': 32}. Best is trial 11 with value: 14.427471349905202.\n",
      "[I 2024-11-21 08:06:37,718] Trial 20 finished with value: 18.854682530675614 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.007853533206682549, 'batch_size': 64}. Best is trial 11 with value: 14.427471349905202.\n",
      "[I 2024-11-21 08:08:03,157] Trial 21 finished with value: 17.418994473981428 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.0026588804106187774, 'batch_size': 32}. Best is trial 11 with value: 14.427471349905202.\n",
      "[I 2024-11-21 08:09:28,669] Trial 22 finished with value: 17.901973028440732 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.0024079773140732643, 'batch_size': 32}. Best is trial 11 with value: 14.427471349905202.\n",
      "[I 2024-11-21 08:10:54,548] Trial 23 finished with value: 16.255720499399548 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.0021242947948904222, 'batch_size': 32}. Best is trial 11 with value: 14.427471349905202.\n",
      "[I 2024-11-21 08:11:45,619] Trial 24 finished with value: 17.97293422458408 and parameters: {'hidden_size': 32, 'num_layers': 1, 'learning_rate': 0.0017888664387461519, 'batch_size': 32}. Best is trial 11 with value: 14.427471349905202.\n",
      "[I 2024-11-21 08:13:10,285] Trial 25 finished with value: 13.188297646386284 and parameters: {'hidden_size': 64, 'num_layers': 2, 'learning_rate': 0.0007873570161414351, 'batch_size': 64}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 08:14:34,552] Trial 26 finished with value: 18.129638280187333 and parameters: {'hidden_size': 64, 'num_layers': 2, 'learning_rate': 0.0006525603485842986, 'batch_size': 64}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 08:17:40,129] Trial 27 finished with value: 50.39444371816274 and parameters: {'hidden_size': 64, 'num_layers': 3, 'learning_rate': 0.00026159809651673876, 'batch_size': 96}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 08:20:55,069] Trial 28 finished with value: 38.871063573019846 and parameters: {'hidden_size': 64, 'num_layers': 3, 'learning_rate': 0.0005729939153738538, 'batch_size': 64}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 08:21:44,922] Trial 29 finished with value: 18.111502509933334 and parameters: {'hidden_size': 32, 'num_layers': 1, 'learning_rate': 0.004420067376772651, 'batch_size': 32}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 08:22:31,014] Trial 30 finished with value: 55.09934445789882 and parameters: {'hidden_size': 64, 'num_layers': 1, 'learning_rate': 0.00018816218217644309, 'batch_size': 64}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 08:23:56,656] Trial 31 finished with value: 17.266466707796663 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.0035791588302249524, 'batch_size': 32}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 08:25:28,365] Trial 32 finished with value: 13.536478845922797 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.00086202058540801, 'batch_size': 32}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 08:27:00,515] Trial 33 finished with value: 13.246066196544751 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.0008875019227554725, 'batch_size': 32}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 08:28:35,904] Trial 34 finished with value: 17.426677214132773 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.00042071304289740646, 'batch_size': 32}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 08:30:20,873] Trial 35 finished with value: 41.82448761803763 and parameters: {'hidden_size': 32, 'num_layers': 3, 'learning_rate': 0.0008363402706242494, 'batch_size': 64}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 08:32:23,674] Trial 36 finished with value: 17.63049621410198 and parameters: {'hidden_size': 64, 'num_layers': 2, 'learning_rate': 0.0003431723992402962, 'batch_size': 32}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 08:33:03,341] Trial 37 finished with value: 26.353507791246688 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.0014517346675774205, 'batch_size': 128}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 08:34:42,345] Trial 38 finished with value: 74.29088456290108 and parameters: {'hidden_size': 32, 'num_layers': 3, 'learning_rate': 0.00010441153924133874, 'batch_size': 64}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 08:36:29,764] Trial 39 finished with value: 73.9866589137486 and parameters: {'hidden_size': 64, 'num_layers': 2, 'learning_rate': 4.877430775246577e-05, 'batch_size': 64}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 08:39:01,154] Trial 40 finished with value: 18.389854757635444 and parameters: {'hidden_size': 32, 'num_layers': 3, 'learning_rate': 0.0006638512253549368, 'batch_size': 32}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 08:40:29,132] Trial 41 finished with value: 16.764531384717237 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.0015871437607595828, 'batch_size': 32}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 08:41:59,010] Trial 42 finished with value: 16.711961127616263 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.0008873007807780179, 'batch_size': 32}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 08:43:30,560] Trial 43 finished with value: 14.329900801718772 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.0005419737240553957, 'batch_size': 32}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 08:45:04,667] Trial 44 finished with value: 13.657247062201973 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.0004885962737128061, 'batch_size': 32}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 08:46:41,380] Trial 45 finished with value: 34.014014905637445 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.0002400523453680341, 'batch_size': 32}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 08:48:42,375] Trial 46 finished with value: 16.65467732231896 and parameters: {'hidden_size': 64, 'num_layers': 2, 'learning_rate': 0.0004534550065723328, 'batch_size': 32}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 08:49:41,425] Trial 47 finished with value: 33.61631778308323 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.0004943915348271962, 'batch_size': 64}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 08:51:01,263] Trial 48 finished with value: 47.76932195715002 and parameters: {'hidden_size': 64, 'num_layers': 2, 'learning_rate': 0.00035719425106240255, 'batch_size': 96}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 08:52:38,474] Trial 49 finished with value: 56.24463557337855 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.00014759816829813932, 'batch_size': 32}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 08:55:04,189] Trial 50 finished with value: 18.254117037798906 and parameters: {'hidden_size': 128, 'num_layers': 2, 'learning_rate': 0.0013148803599334793, 'batch_size': 32}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 08:56:35,278] Trial 51 finished with value: 13.846786730998272 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.0007557028904506299, 'batch_size': 32}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 08:58:06,451] Trial 52 finished with value: 16.582150871689255 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.0007644029045854584, 'batch_size': 32}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 08:59:36,350] Trial 53 finished with value: 17.22699313120799 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.0010824266999393231, 'batch_size': 32}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 09:01:13,132] Trial 54 finished with value: 27.86729754198779 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.0002871938997422179, 'batch_size': 32}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 09:02:47,785] Trial 55 finished with value: 17.757658107860667 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.000500855079533416, 'batch_size': 32}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 09:04:32,655] Trial 56 finished with value: 17.525203193937028 and parameters: {'hidden_size': 96, 'num_layers': 2, 'learning_rate': 0.0010911093484253728, 'batch_size': 128}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 09:05:33,228] Trial 57 finished with value: 41.26214006968907 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.0006937226458317873, 'batch_size': 64}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 09:07:00,947] Trial 58 finished with value: 17.028583964786016 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.0018023986407908529, 'batch_size': 32}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 09:09:24,536] Trial 59 finished with value: 80.89869793041332 and parameters: {'hidden_size': 64, 'num_layers': 2, 'learning_rate': 1.1694961271805489e-05, 'batch_size': 32}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 09:10:23,997] Trial 60 finished with value: 29.882393223898752 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.000535755868699553, 'batch_size': 64}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 09:11:54,640] Trial 61 finished with value: 17.285234915243613 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.0008964812228662447, 'batch_size': 32}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 09:13:23,603] Trial 62 finished with value: 16.84856313413328 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.001310881638396012, 'batch_size': 32}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 09:16:03,007] Trial 63 finished with value: 18.193621910370148 and parameters: {'hidden_size': 32, 'num_layers': 3, 'learning_rate': 0.0007592162019229872, 'batch_size': 32}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 09:17:38,712] Trial 64 finished with value: 19.3151019843849 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.0003826804383221111, 'batch_size': 32}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 09:19:25,216] Trial 65 finished with value: 18.414492521200096 and parameters: {'hidden_size': 64, 'num_layers': 2, 'learning_rate': 0.0028000097093427534, 'batch_size': 32}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 09:20:17,738] Trial 66 finished with value: 17.831219217798733 and parameters: {'hidden_size': 32, 'num_layers': 1, 'learning_rate': 0.0010078211038441194, 'batch_size': 32}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 09:21:03,300] Trial 67 finished with value: 46.942739950644004 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.0006128286448163534, 'batch_size': 96}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 09:41:55,191] Trial 68 finished with value: 41.785654978709175 and parameters: {'hidden_size': 96, 'num_layers': 3, 'learning_rate': 0.0003063764765547167, 'batch_size': 32}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 09:45:51,293] Trial 69 finished with value: 16.649433393736143 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.0018296510246695625, 'batch_size': 32}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 09:50:24,223] Trial 70 finished with value: 62.23601382119315 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.0002102280814717856, 'batch_size': 64}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 09:55:05,456] Trial 71 finished with value: 16.08829539530986 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.002061560256978783, 'batch_size': 32}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 09:58:13,355] Trial 72 finished with value: 17.079998772423547 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.0038129532449024837, 'batch_size': 32}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 10:01:30,926] Trial 73 finished with value: 17.958008018699854 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.002924525898688821, 'batch_size': 32}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 10:05:42,421] Trial 74 finished with value: 18.549395483893317 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.006172361325715448, 'batch_size': 32}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 10:10:21,077] Trial 75 finished with value: 18.30234653026134 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.0012319932127727638, 'batch_size': 32}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 10:14:41,560] Trial 76 finished with value: 17.156109569308995 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.0008276950562989556, 'batch_size': 32}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 10:18:11,257] Trial 77 finished with value: 18.33171104740452 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.0004219473532445999, 'batch_size': 32}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 10:25:15,010] Trial 78 finished with value: 18.293320810472643 and parameters: {'hidden_size': 32, 'num_layers': 3, 'learning_rate': 0.0005630615027381257, 'batch_size': 32}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 10:29:19,102] Trial 79 finished with value: 17.40306762286595 and parameters: {'hidden_size': 64, 'num_layers': 2, 'learning_rate': 0.0019447907444251524, 'batch_size': 64}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 10:32:30,497] Trial 80 finished with value: 13.53264332676793 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.001469514104503506, 'batch_size': 32}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 10:36:04,126] Trial 81 finished with value: 16.936936928345276 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.0014711542767368844, 'batch_size': 32}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 10:40:41,914] Trial 82 finished with value: 14.239687747783488 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.0009138834315944984, 'batch_size': 32}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 10:45:15,784] Trial 83 finished with value: 18.35338289243681 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.0010167745615747496, 'batch_size': 32}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 10:49:02,133] Trial 84 finished with value: 13.908407280036995 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.00071270356941159, 'batch_size': 32}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 10:52:23,587] Trial 85 finished with value: 17.978851687800777 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.0006623766815724807, 'batch_size': 32}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 10:56:04,830] Trial 86 finished with value: 17.078854775643563 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.0004937816140558831, 'batch_size': 32}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 11:00:41,740] Trial 87 finished with value: 16.80024459770134 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.0007433086895911476, 'batch_size': 32}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 11:05:19,736] Trial 88 finished with value: 12.834748358339876 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.0011843203255513526, 'batch_size': 32}. Best is trial 88 with value: 12.834748358339876.\n",
      "[I 2024-11-21 11:09:11,079] Trial 89 finished with value: 18.29032701200193 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.0015448313947718122, 'batch_size': 32}. Best is trial 88 with value: 12.834748358339876.\n",
      "[I 2024-11-21 11:15:25,754] Trial 90 finished with value: 18.437728684227746 and parameters: {'hidden_size': 64, 'num_layers': 2, 'learning_rate': 0.0011962566226361186, 'batch_size': 32}. Best is trial 88 with value: 12.834748358339876.\n",
      "[I 2024-11-21 11:20:08,026] Trial 91 finished with value: 16.46591075691017 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.0009987879437785335, 'batch_size': 32}. Best is trial 88 with value: 12.834748358339876.\n",
      "[I 2024-11-21 11:23:21,594] Trial 92 finished with value: 16.551094158275706 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.0008779987276004864, 'batch_size': 32}. Best is trial 88 with value: 12.834748358339876.\n",
      "[I 2024-11-21 11:26:43,468] Trial 93 finished with value: 17.754048081131668 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.0006335740981650382, 'batch_size': 32}. Best is trial 88 with value: 12.834748358339876.\n",
      "[I 2024-11-21 11:30:54,687] Trial 94 finished with value: 17.638701206928975 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.0004450921085714084, 'batch_size': 32}. Best is trial 88 with value: 12.834748358339876.\n",
      "[I 2024-11-21 11:40:28,203] Trial 95 finished with value: 18.063094542907166 and parameters: {'hidden_size': 128, 'num_layers': 2, 'learning_rate': 0.0007376231694127611, 'batch_size': 32}. Best is trial 88 with value: 12.834748358339876.\n",
      "[I 2024-11-21 11:45:15,730] Trial 96 finished with value: 16.909656902691264 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.0005551123936856655, 'batch_size': 32}. Best is trial 88 with value: 12.834748358339876.\n",
      "[I 2024-11-21 11:51:21,412] Trial 97 finished with value: 18.44897043382799 and parameters: {'hidden_size': 96, 'num_layers': 2, 'learning_rate': 0.0011600887591107136, 'batch_size': 32}. Best is trial 88 with value: 12.834748358339876.\n",
      "[I 2024-11-21 11:55:36,144] Trial 98 finished with value: 83.9870410646711 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 2.739950929039685e-05, 'batch_size': 64}. Best is trial 88 with value: 12.834748358339876.\n",
      "[I 2024-11-21 11:59:27,117] Trial 99 finished with value: 43.426560582341374 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.0008722134632950634, 'batch_size': 96}. Best is trial 88 with value: 12.834748358339876.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.0011843203255513526, 'batch_size': 32}\n",
      "Best Validation RMSE: 12.834748358339876\n",
      "Epoch [1/20], Validation Loss: 72.3708\n",
      "Epoch [2/20], Validation Loss: 65.1407\n",
      "Epoch [3/20], Validation Loss: 56.8781\n",
      "Epoch [4/20], Validation Loss: 48.4448\n",
      "Epoch [5/20], Validation Loss: 42.7630\n",
      "Epoch [6/20], Validation Loss: 41.8939\n",
      "Epoch [7/20], Validation Loss: 41.7637\n",
      "Epoch [8/20], Validation Loss: 41.7608\n",
      "Epoch [9/20], Validation Loss: 40.0594\n",
      "Epoch [10/20], Validation Loss: 20.2189\n",
      "Epoch [11/20], Validation Loss: 18.4146\n",
      "Epoch [12/20], Validation Loss: 18.4378\n",
      "Epoch [13/20], Validation Loss: 18.0692\n",
      "Epoch [14/20], Validation Loss: 18.1642\n",
      "Epoch [15/20], Validation Loss: 17.8939\n",
      "Epoch [16/20], Validation Loss: 17.6741\n",
      "Epoch [17/20], Validation Loss: 17.7719\n",
      "Epoch [18/20], Validation Loss: 17.1611\n",
      "Epoch [19/20], Validation Loss: 16.8547\n",
      "Epoch [20/20], Validation Loss: 16.9993\n",
      "Best RNN model saved to 'best_rnn_model_optuna.pth'\n",
      "Test RMSE: 18.9342\n"
     ]
    }
   ],
   "source": [
    "# first, we assign the processed data to X and y\n",
    "# X contains the features (input data) and y contains the target variable (processed_train_targets)\n",
    "X = processed_train_data\n",
    "y = processed_train_targets\n",
    "\n",
    "# convert the numpy arrays to PyTorch tensors, since PyTorch models expect inputs as tensors\n",
    "X_tensor = torch.FloatTensor(X)\n",
    "y_tensor = torch.FloatTensor(y)\n",
    "\n",
    "# split the data into training and validation sets\n",
    "# here, we use sklearn's train_test_split to create a training set and a validation set\n",
    "# 80% of the data goes into the training set, and 20% goes into the validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = TensorDataset(X_train, y_train)  # create training dataset\n",
    "val_dataset = TensorDataset(X_val, y_val)  # create validation dataset\n",
    "\n",
    "# define the RNN model class\n",
    "class RNNModel(nn.Module):\n",
    "    \"\"\"\n",
    "    This class defines the structure of the RNN model for predicting values\n",
    "    from sequential input data. It uses a simple RNN layer followed by a \n",
    "    fully connected layer for the output.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(RNNModel, self).__init__()\n",
    "        \n",
    "        # store hyperparameters for later use\n",
    "        self.hidden_size = hidden_size  # size of the hidden state in RNN\n",
    "        self.num_layers = num_layers  # number of layers in the RNN\n",
    "        \n",
    "        # define the RNN layer; batch_first=True ensures input/output tensors\n",
    "        # have the shape (batch_size, seq_length, features)\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        \n",
    "        # define a fully connected (linear) layer to map the final RNN output to the desired output size\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the model. The input tensor `x` is passed through the RNN\n",
    "        and then through the fully connected layer to produce the final output.\n",
    "        \n",
    "        Args:\n",
    "            x: input tensor of shape (batch_size, seq_length, input_size)\n",
    "        \n",
    "        Returns:\n",
    "            output: tensor of shape (batch_size, output_size) from the fully connected layer\n",
    "        \"\"\"\n",
    "        # initialize the hidden state to zeros, one for each layer in the RNN\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        # pass the input through the RNN; out is the output from each time step\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        \n",
    "        # we are interested in the output of the last time step, hence we select it using [-1]\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# define the objective function for hyperparameter optimization using Optuna\n",
    "# this function defines the objective for Optuna's hyperparameter optimization.\n",
    "# it sets up the model with hyperparameters suggested by Optuna, trains the model, and returns the validation loss (RMSE)\n",
    "# as the objective to minimize.\n",
    "\n",
    "def objective(trial):\n",
    "    # suggest hyperparameters using optuna\n",
    "    hidden_size = trial.suggest_int('hidden_size', 32, 128, step=32)  # number of hidden units in the rnn\n",
    "    num_layers = trial.suggest_int('num_layers', 1, 3)  # number of rnn layers\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)  # learning rate\n",
    "    batch_size = trial.suggest_int('batch_size', 32, 128, step=32)  # batch size for training\n",
    "    \n",
    "    # create the model with the suggested hyperparameters\n",
    "    model = RNNModel(input_size=X.shape[2], hidden_size=hidden_size, num_layers=num_layers, output_size=1)\n",
    "    \n",
    "    # define the loss function (rmse) and optimizer\n",
    "    criterion = rmse_loss  # rmse loss function\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # create the dataloader objects for the training and validation datasets\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # training loop: run for a set number of epochs\n",
    "    num_epochs = 20  \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # set the model to training mode\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()  # zero the gradients before the backward pass\n",
    "            outputs = model(batch_X)  # get model predictions\n",
    "            loss = criterion(outputs.squeeze(), batch_y)  # compute the loss\n",
    "            loss.backward()  # backpropagate the gradients\n",
    "            optimizer.step()  # update the model parameters\n",
    "    \n",
    "    # validation loop: evaluate model on the validation set\n",
    "    model.eval()  # set the model to evaluation mode\n",
    "    with torch.no_grad():  # no need to compute gradients during evaluation\n",
    "        val_losses = []\n",
    "        for batch_X, batch_y in val_loader:\n",
    "            outputs = model(batch_X)  # get model predictions\n",
    "            val_loss = criterion(outputs.squeeze(), batch_y)  # compute validation loss\n",
    "            val_losses.append(val_loss.item())  # store the loss for each batch\n",
    "        avg_val_loss = np.mean(val_losses)  # compute the average validation loss\n",
    "    \n",
    "    # return the average validation loss as the optimization objective\n",
    "    return avg_val_loss\n",
    "\n",
    "# create an optuna study to optimize the objective function\n",
    "study = optuna.create_study(direction='minimize')  # we want to minimize the validation loss\n",
    "study.optimize(objective, n_trials=100)  # run the optimization for 100 trials\n",
    "\n",
    "\n",
    "# print the best hyperparameters found by Optuna and the corresponding validation loss\n",
    "print(\"Best Hyperparameters:\", study.best_params)\n",
    "print(\"Best Validation RMSE:\", study.best_value)\n",
    "\n",
    "# extract the best hyperparameters from the study's best trial\n",
    "best_trial = study.best_trial\n",
    "best_hidden_size = best_trial.params['hidden_size']\n",
    "best_num_layers = best_trial.params['num_layers']\n",
    "best_learning_rate = best_trial.params['learning_rate']\n",
    "best_batch_size = best_trial.params['batch_size']\n",
    "\n",
    "# instantiate the final model with the best hyperparameters found\n",
    "best_model = RNNModel(input_size=X.shape[2], hidden_size=best_hidden_size, num_layers=best_num_layers, output_size=1)\n",
    "\n",
    "# define optimizer and loss function for the final model\n",
    "optimizer = optim.Adam(best_model.parameters(), lr=best_learning_rate)\n",
    "criterion = rmse_loss\n",
    "\n",
    "# create dataloaders for the final training and validation datasets with the best batch size\n",
    "train_loader = DataLoader(train_dataset, batch_size=best_batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=best_batch_size, shuffle=False)\n",
    "\n",
    "# move the model to gpu if avaialble for faster training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "best_model.to(device)\n",
    "\n",
    "# final training loop with the best hyperparameters\n",
    "num_epochs = 20 \n",
    "for epoch in range(num_epochs):\n",
    "    best_model.train()  # set model to training mode\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()  # reset gradients\n",
    "        batch_X = batch_X.to(device)  # move data to the correct device (CPU/GPU)\n",
    "        batch_y = batch_y.to(device)  # move target labels to the correct device\n",
    "        outputs = best_model(batch_X)  # model prediction\n",
    "        loss = criterion(outputs.squeeze(), batch_y)  # compute loss\n",
    "        loss.backward()  # backpropagation\n",
    "        optimizer.step()  # optimizer step (update weights)\n",
    "\n",
    "    # validation loop\n",
    "    best_model.eval()  # set model to evaluation mode\n",
    "    with torch.no_grad():  # don't compute gradients during evaluation\n",
    "        val_losses = []\n",
    "        for batch_X, batch_y in val_loader:\n",
    "            batch_X = batch_X.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            outputs = best_model(batch_X)  # model prediction\n",
    "            val_loss = criterion(outputs.squeeze(), batch_y)  # compute loss\n",
    "            val_losses.append(val_loss.item())  # store loss\n",
    "        avg_val_loss = np.mean(val_losses)  # compute average validation loss\n",
    "\n",
    "    # print validation loss after each epoch\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Validation Loss: {avg_val_loss:.4f}')\n",
    "\n",
    "# save the best model to a file for later use\n",
    "torch.save(best_model.state_dict(), 'best_rnn_model_optuna.pth')\n",
    "print(\"Best RNN model saved to 'best_rnn_model_optuna.pth'\")\n",
    "\n",
    "# evaluate the final model on test data (optional)\n",
    "best_model.eval()  # set model to evaluation mode\n",
    "test_tensor = torch.FloatTensor(processed_test_data).to(device)  # convert test data to tensor\n",
    "with torch.no_grad():  # no need to compute gradients\n",
    "    test_predictions = best_model(test_tensor).numpy().squeeze()  # model predictions\n",
    "\n",
    "# calculate the RMSE on the test data to evaluate model performance\n",
    "test_rmse = np.sqrt(np.mean((test_predictions - true_rul)**2))\n",
    "print(f'Test RMSE: {test_rmse:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5dc0da4a-03ad-4569-a44e-9066804b61d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Validation Loss: 72.8571\n",
      "Epoch [2/30], Validation Loss: 60.1122\n",
      "Epoch [3/30], Validation Loss: 50.5848\n",
      "Epoch [4/30], Validation Loss: 44.9487\n",
      "Epoch [5/30], Validation Loss: 42.5667\n",
      "Epoch [6/30], Validation Loss: 41.8887\n",
      "Epoch [7/30], Validation Loss: 41.7864\n",
      "Epoch [8/30], Validation Loss: 22.7499\n",
      "Epoch [9/30], Validation Loss: 21.5080\n",
      "Epoch [10/30], Validation Loss: 18.3313\n",
      "Epoch [11/30], Validation Loss: 17.5140\n",
      "Epoch [12/30], Validation Loss: 17.0022\n",
      "Epoch [13/30], Validation Loss: 16.6556\n",
      "Epoch [14/30], Validation Loss: 16.9519\n",
      "Epoch [15/30], Validation Loss: 17.5586\n",
      "Epoch [16/30], Validation Loss: 16.4147\n",
      "Epoch [17/30], Validation Loss: 16.8203\n",
      "Epoch [18/30], Validation Loss: 16.9057\n",
      "Epoch [19/30], Validation Loss: 16.8761\n",
      "Epoch [20/30], Validation Loss: 16.3438\n",
      "Epoch [21/30], Validation Loss: 16.4679\n",
      "Epoch [22/30], Validation Loss: 16.4167\n",
      "Epoch [23/30], Validation Loss: 16.4471\n",
      "Epoch [24/30], Validation Loss: 19.6358\n",
      "Epoch [25/30], Validation Loss: 16.3047\n",
      "Epoch [26/30], Validation Loss: 16.0472\n",
      "Epoch [27/30], Validation Loss: 16.0645\n",
      "Epoch [28/30], Validation Loss: 17.3121\n",
      "Epoch [29/30], Validation Loss: 16.2764\n",
      "Epoch [30/30], Validation Loss: 16.3513\n",
      "Best RNN model saved to 'best_rnn_model_optuna.pth'\n",
      "Test RMSE: 18.0790\n"
     ]
    }
   ],
   "source": [
    "# Extract the best hyperparameters from the best trial\n",
    "best_trial = study.best_trial\n",
    "best_hidden_size = best_trial.params['hidden_size']\n",
    "best_num_layers = best_trial.params['num_layers']\n",
    "best_learning_rate = best_trial.params['learning_rate']\n",
    "best_batch_size = best_trial.params['batch_size']\n",
    "\n",
    "# Instantiate the model with the best hyperparameters\n",
    "best_model = RNNModel(input_size=X.shape[2], hidden_size=best_hidden_size, num_layers=best_num_layers, output_size=1)\n",
    "\n",
    "# Define optimizer and loss function\n",
    "optimizer = optim.Adam(best_model.parameters(), lr=best_learning_rate)\n",
    "criterion = rmse_loss\n",
    "\n",
    "# Create DataLoaders with the best batch size\n",
    "train_loader = DataLoader(train_dataset, batch_size=best_batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=best_batch_size, shuffle=False)\n",
    "\n",
    "# Final model training with the best hyperparameters\n",
    "num_epochs = 30  \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "best_model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    best_model.train()\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        batch_X = batch_X.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "        outputs = best_model(batch_X)\n",
    "        loss = criterion(outputs.squeeze(), batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validation loop\n",
    "    best_model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_losses = []\n",
    "        for batch_X, batch_y in val_loader:\n",
    "            batch_X = batch_X.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            outputs = best_model(batch_X)\n",
    "            val_loss = criterion(outputs.squeeze(), batch_y)\n",
    "            val_losses.append(val_loss.item())\n",
    "        avg_val_loss = np.mean(val_losses)\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Validation Loss: {avg_val_loss:.4f}')\n",
    "\n",
    "# Save the best model to a file\n",
    "torch.save(best_model.state_dict(), 'best_rnn_model_optuna.pth')\n",
    "print(\"Best RNN model saved to 'best_rnn_model_optuna.pth'\")\n",
    "\n",
    "# Evaluate on test data (optional)\n",
    "best_model.eval()\n",
    "test_tensor = torch.FloatTensor(processed_test_data).to(device)\n",
    "with torch.no_grad():\n",
    "    test_predictions = best_model(test_tensor).numpy().squeeze()\n",
    "\n",
    "# Calculate RMSE on test data\n",
    "test_rmse = np.sqrt(np.mean((test_predictions - true_rul)**2))\n",
    "print(f'Test RMSE: {test_rmse:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fb25c0-089e-4d20-8979-dbf48c01ba44",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c30fb12-6ad7-4916-8c64-11579c40f289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[114.43959 ]\n",
      " [120.48697 ]\n",
      " [ 61.45211 ]\n",
      " [ 88.70154 ]\n",
      " [103.07733 ]\n",
      " [115.62491 ]\n",
      " [103.4091  ]\n",
      " [ 98.803474]\n",
      " [115.1178  ]\n",
      " [ 93.94371 ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2796/3945363154.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  best_model.load_state_dict(torch.load('best_rnn_model_optuna.pth', map_location='cpu'))  # use 'cuda' for GPU\n"
     ]
    }
   ],
   "source": [
    "# test data (processed test data and true RUL values)\n",
    "X_test = processed_test_data  # processed test data\n",
    "y_test = true_rul  # true remaining useful life values\n",
    "\n",
    "# convert the test data and labels to pytorch tensors\n",
    "X_test_tensor = torch.FloatTensor(X_test)  # convert test data to float tensor\n",
    "y_test_tensor = torch.FloatTensor(y_test)  # convert true RUL values to float tensor\n",
    "\n",
    "# create a DataLoader for the test data (used for batch processing)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)  # combine features and labels into a dataset\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)  # DataLoader for test data (no shuffling)\n",
    "\n",
    "# define the best hyperparameters found from optuna\n",
    "best_hidden_size = 32  # best hidden layer size \n",
    "best_num_layers = 2  # best number of RNN layers\n",
    "best_learning_rate = 0.0011843203255513526  # best learning rate \n",
    "best_batch_size = 32  # best batch size\n",
    "\n",
    "# instantiate the model with the best hyperparameters\n",
    "best_model = RNNModel(input_size=X_test.shape[2], hidden_size=best_hidden_size, \n",
    "                      num_layers=best_num_layers, output_size=1)\n",
    "\n",
    "# load the best model weights saved after training with optuna\n",
    "best_model.load_state_dict(torch.load('best_rnn_model_optuna.pth', map_location='cpu'))  # use 'cuda' for GPU\n",
    "\n",
    "# define a function to get predictions from the model\n",
    "def get_predictions(model, test_loader):\n",
    "    \"\"\"\n",
    "    This function performs inference on the test set and returns the model's predictions.\n",
    "    \n",
    "    Args:\n",
    "        model: the trained RNN model\n",
    "        test_loader: DataLoader containing the test data\n",
    "\n",
    "    Returns:\n",
    "        all_predictions: a numpy array of predictions for the test set\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # choose device (GPU if available)\n",
    "    model.to(device)  # move the model to the selected device (GPU/CPU)\n",
    "    model.eval()  # set the model to evaluation mode (disables dropout layers, etc.)\n",
    "    \n",
    "    all_predictions = []  # this will hold the predictions from all batches\n",
    "    \n",
    "    with torch.no_grad():  # turn off gradient computation since we are just doing inference\n",
    "        for batch_X, _ in test_loader:  # loop through batches in the test_loader\n",
    "            batch_X = batch_X.to(device)  # move input batch to the same device as the model\n",
    "            predictions = model(batch_X)  # forward pass to get predictions\n",
    "            all_predictions.append(predictions.cpu().numpy())  # store predictions (move them to CPU and convert to numpy)\n",
    "\n",
    "    # concatenate all predictions into a single array (flatten list of arrays)\n",
    "    all_predictions = np.concatenate(all_predictions, axis=0)\n",
    "    \n",
    "    return all_predictions  # return the final array of predictions\n",
    "\n",
    "# get the model's predictions on the test set\n",
    "predictions = get_predictions(best_model, test_loader)\n",
    "\n",
    "# print or inspect the predictions\n",
    "print(predictions[:10])  # print the first 10 predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8547c20f-a7e5-4ebd-a5dd-0975fc11b25d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MAE': 13.405092282295227,\n",
       " 'MSE': 326.84914252787433,\n",
       " 'RMSE': 18.07896962019336,\n",
       " 'MAPE': 147.84653424608157}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def evaluate_rul_metrics(true, predicted):\n",
    "    \n",
    "    true = np.array(true)\n",
    "    predicted = np.array(predicted)\n",
    "    \n",
    "    mae = float(mean_absolute_error(true, predicted))\n",
    "    mse = float(mean_squared_error(true, predicted))\n",
    "    rmse = float(np.sqrt(mse))\n",
    "    mape = float(np.mean(np.abs((true - predicted) / true)) * 100)\n",
    "    \n",
    "    return {\n",
    "        \"MAE\": mae,\n",
    "        \"MSE\": mse,\n",
    "        \"RMSE\": rmse,\n",
    "        \"MAPE\": mape\n",
    "    }\n",
    "    \n",
    "metrics = evaluate_rul_metrics(true_rul, predictions)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785e70fb-756a-40b6-96a1-1fd437f31deb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
