{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98074c75-7c4f-4e0f-a6ed-6a7597a911b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras_tuner as kt\n",
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "from keras import backend as K\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66a3de2d-b170-4def-bb00-0099405d2e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Train Data Shape: (17731, 30, 14)\n",
      "Processed Train Target Shape: (17731,)\n",
      "Processed Test Data Shape: (100, 30, 14)\n",
      "True RUL Shape: (100,)\n"
     ]
    }
   ],
   "source": [
    "# define the directory where the pickle files are stored\n",
    "folder_path = '../batched_data_pickle_files/'\n",
    "\n",
    "# define the filenames for the pickle files\n",
    "file_names = ['processed_train_data.pkl', 'processed_train_targets.pkl', 'processed_test_data.pkl', 'true_rul.pkl']\n",
    "\n",
    "# loop through each file and load its contents as arrays\n",
    "for file_name in file_names:\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    \n",
    "    # read the pickle file\n",
    "    with open(file_path, 'rb') as file:\n",
    "        data = pickle.load(file)\n",
    "    \n",
    "    # ensure data is a numpy array, if not convert it\n",
    "    if isinstance(data, np.ndarray):\n",
    "        globals()[file_name.replace('.pkl', '')] = data\n",
    "    else:\n",
    "        globals()[file_name.replace('.pkl', '')] = np.array(data)\n",
    "\n",
    "print(\"Processed Train Data Shape:\", processed_train_data.shape)\n",
    "print(\"Processed Train Target Shape:\", processed_train_targets.shape)\n",
    "print(\"Processed Test Data Shape:\", processed_test_data.shape)\n",
    "print(\"True RUL Shape:\", true_rul.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb4b271c-fa2d-4970-ae3c-e4039646483d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed train data shape:  (14184, 30, 14)\n",
      "Processed validation data shape:  (3547, 30, 14)\n",
      "Processed train targets shape:  (14184,)\n",
      "Processed validation targets shape:  (3547,)\n"
     ]
    }
   ],
   "source": [
    "processed_train_data, processed_val_data, processed_train_targets, processed_val_targets = train_test_split(processed_train_data,\n",
    "                                                                                                            processed_train_targets,\n",
    "                                                                                                            test_size = 0.2,\n",
    "                                                                                                            random_state = 83)\n",
    "print(\"Processed train data shape: \", processed_train_data.shape)\n",
    "print(\"Processed validation data shape: \", processed_val_data.shape)\n",
    "print(\"Processed train targets shape: \", processed_train_targets.shape)\n",
    "print(\"Processed validation targets shape: \", processed_val_targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01badac-d1a7-483f-bd98-d1c3947821e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "import kerastuner as kt\n",
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "# After training a model\n",
    "K.clear_session()\n",
    "\n",
    "\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "\n",
    "    # Fixed batch size\n",
    "    batch_size = 128\n",
    "\n",
    "    # Determine number of LSTM layers\n",
    "    num_lstm_layers = hp.Int('num_lstm_layers', 1, 3)  # Choose between 1 and 3 LSTM layers\n",
    "\n",
    "    # Adding LSTM layers based on the chosen number of layers\n",
    "    for i in range(num_lstm_layers):\n",
    "        if i == 0:\n",
    "            # First LSTM Layer\n",
    "            model.add(layers.LSTM(\n",
    "                units=hp.Int('units_1', min_value=128, max_value=256, step=32),\n",
    "                input_shape=(processed_train_data.shape[1], processed_train_data.shape[2]),\n",
    "                return_sequences=True,\n",
    "                activation=\"tanh\"\n",
    "            ))\n",
    "        else:\n",
    "            # Subsequent LSTM Layers\n",
    "            model.add(layers.LSTM(\n",
    "                units=hp.Int(f'units_{i + 1}', min_value=32, max_value=128, step=32),\n",
    "                return_sequences=True if i < num_lstm_layers - 1 else False,\n",
    "                activation=\"tanh\"\n",
    "            ))\n",
    "        \n",
    "        # Remove the dropout and BatchNormalization layers\n",
    "        # Only LSTM layers remain in this part\n",
    "\n",
    "    # Determine number of Dense layers\n",
    "    num_dense_layers = hp.Int('num_dense_layers', 1, 2)  # Choose between 1 and 2 Dense layers\n",
    "\n",
    "    # Adding Dense layers based on the chosen number of layers\n",
    "    for j in range(num_dense_layers):\n",
    "        model.add(layers.Dense(\n",
    "            units=hp.Int(f'dense_units_{j + 1}', min_value=32, max_value=256, step=32),\n",
    "            activation=\"relu\"\n",
    "        ))\n",
    "\n",
    "    model.add(layers.Dense(1))  # Final output layer\n",
    "\n",
    "    # Compile Model\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(\n",
    "            hp.Float('learning_rate', min_value=1e-4, max_value=1e-2)  # No log scaling\n",
    "        ), \n",
    "        loss=\"mse\"\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Set up Keras Tuner with Hyperband\n",
    "tuner = kt.Hyperband(\n",
    "    build_model,\n",
    "    objective=\"val_loss\",\n",
    "    max_epochs=10,\n",
    "    factor=3,\n",
    "    directory=\"hyperparam_tuning\",\n",
    "    project_name=\"lstm_tuning_refined\"\n",
    ")\n",
    "\n",
    "# Run the tuner search\n",
    "tuner.search(\n",
    "    processed_train_data, processed_train_targets,\n",
    "    epochs=10,\n",
    "    validation_data=(processed_val_data, processed_val_targets),\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)],\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Get the best model and evaluate it\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "val_loss = best_model.evaluate(processed_val_data, processed_val_targets, verbose=0)\n",
    "print(f\"\\nBest model validation MSE: {val_loss:.4f}\")\n",
    "\n",
    "# Get the best hyperparameters from the tuner\n",
    "best_hyperparameters = tuner.get_best_hyperparameters(1)[0]\n",
    "print(\"Best hyperparameters:\", best_hyperparameters.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25547717-a6be-4add-9472-980b9710c2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Trial 30 Complete [00h 03m 33s] val_loss: 128.57681274414062\n",
    "\n",
    "Best val_loss So Far: 45.899444580078125 Total elapsed time: 00h 41m 41s\n",
    "\n",
    "Best model validation MSE: 45.8994 Best hyperparameters: {'num_lstm_layers': 2, 'units_1': 224, 'num_dense_layers': 1, 'dense_units_1': 160, 'learning_rate': 0.0019205962288415628, 'units_2': 32, 'dense_units_2': 192, 'units_3': 96, 'tuner/epochs': 10, 'tuner/initial_epoch': 4, 'tuner/bracket': 2, 'tuner/round': 2, 'tuner/trial_id': '0012'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "407a270a-41b7-427c-aa57-bc691e83bcae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 1/10\n",
      "444/444 - 22s - 49ms/step - loss: 2450.4880 - val_loss: 724.2559 - learning_rate: 1.0000e-03\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 2/10\n",
      "444/444 - 19s - 42ms/step - loss: 474.3666 - val_loss: 382.4680 - learning_rate: 1.0000e-03\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 3/10\n",
      "444/444 - 19s - 42ms/step - loss: 247.2537 - val_loss: 207.7093 - learning_rate: 1.0000e-03\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 4/10\n",
      "444/444 - 21s - 46ms/step - loss: 198.3183 - val_loss: 163.2923 - learning_rate: 1.0000e-03\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 5/10\n",
      "444/444 - 19s - 43ms/step - loss: 185.5225 - val_loss: 193.0704 - learning_rate: 1.0000e-03\n",
      "\n",
      "Epoch 6: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 6/10\n",
      "444/444 - 20s - 46ms/step - loss: 160.5922 - val_loss: 150.9580 - learning_rate: 1.0000e-04\n",
      "\n",
      "Epoch 7: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 7/10\n",
      "444/444 - 19s - 43ms/step - loss: 155.8247 - val_loss: 149.6245 - learning_rate: 1.0000e-04\n",
      "\n",
      "Epoch 8: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 8/10\n",
      "444/444 - 20s - 46ms/step - loss: 154.7598 - val_loss: 148.6209 - learning_rate: 1.0000e-04\n",
      "\n",
      "Epoch 9: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 9/10\n",
      "444/444 - 19s - 42ms/step - loss: 153.3784 - val_loss: 146.1172 - learning_rate: 1.0000e-04\n",
      "\n",
      "Epoch 10: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 10/10\n",
      "444/444 - 19s - 43ms/step - loss: 152.3302 - val_loss: 147.2899 - learning_rate: 1.0000e-04\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 99ms/step\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras import layers, Sequential\n",
    "from keras.callbacks import EarlyStopping, LearningRateScheduler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Best hyperparameters\n",
    "best_hyperparameters = {\n",
    "    'num_lstm_layers': 2,         # Two LSTM layers\n",
    "    'units_1': 224,               # First LSTM layer units\n",
    "    'num_dense_layers': 1,        # One Dense layer\n",
    "    'dense_units_1': 160,         # Dense layer units\n",
    "    'learning_rate': 0.001,       # 0.0019205962288415628\n",
    "    'units_2': 32,                # Second LSTM layer units\n",
    "}\n",
    "\n",
    "# Define the function to build the LSTM model with the best hyperparameters\n",
    "def build_best_model(hyperparams, input_shape):\n",
    "    model = Sequential()\n",
    "\n",
    "    # First LSTM Layer\n",
    "    model.add(layers.LSTM(\n",
    "        units=hyperparams['units_1'],\n",
    "        input_shape=input_shape,\n",
    "        return_sequences=True\n",
    "    ))\n",
    "\n",
    "    # Second LSTM Layer (last layer, don't return sequences)\n",
    "    model.add(layers.LSTM(units=hyperparams['units_2']))\n",
    "\n",
    "    # Dense Layer\n",
    "    model.add(layers.Dense(hyperparams['dense_units_1'], activation='relu'))\n",
    "\n",
    "    # Output Layer\n",
    "    model.add(layers.Dense(1))  # Assuming a regression task\n",
    "\n",
    "    # Compile Model\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=hyperparams['learning_rate']),\n",
    "        loss='mse'\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "# Learning rate scheduler function\n",
    "def lr_scheduler(epoch):\n",
    "    if epoch < 5:\n",
    "        return best_hyperparameters['learning_rate']  # Use initial learning rate\n",
    "    else:\n",
    "        return 0.0001  # Set to 0.0001 after epoch 5\n",
    "\n",
    "# Example input shape (timesteps, features), modify based on your data\n",
    "input_shape = (processed_train_data.shape[1], processed_train_data.shape[2])  # Define your input shape\n",
    "\n",
    "# Build the model\n",
    "best_model = build_best_model(best_hyperparameters, input_shape)\n",
    "\n",
    "# Set up early stopping\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',  # Monitor validation loss\n",
    "    patience=3,          # Stop training after 3 epochs of no improvement\n",
    "    restore_best_weights=True  # Restore model weights from the epoch with the best validation loss\n",
    ")\n",
    "\n",
    "# Set up learning rate scheduler\n",
    "lr_scheduler_callback = LearningRateScheduler(lr_scheduler, verbose=1)\n",
    "\n",
    "# Train the model\n",
    "history = best_model.fit(\n",
    "    processed_train_data, \n",
    "    processed_train_targets,\n",
    "    epochs=10,  # Set your desired number of epochs (tuner/epochs)\n",
    "    validation_data=(processed_val_data, processed_val_targets),\n",
    "    callbacks=[early_stopping, lr_scheduler_callback],\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Make predictions (replace test_data with your actual test dataset)\n",
    "predictions = best_model.predict(processed_test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4020e865-2760-4f85-be8c-e9ebfa203914",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2049bfb5-90dd-49aa-bcd4-89ee7ea9edcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MAE': 10.672525138854981,\n",
       " 'MSE': 212.84601973255033,\n",
       " 'RMSE': 14.589243288551684,\n",
       " 'MAPE': 148.36926763230426}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def evaluate_rul_metrics(true, predicted):\n",
    "    \n",
    "    true = np.array(true)\n",
    "    predicted = np.array(predicted)\n",
    "    \n",
    "    mae = float(mean_absolute_error(true, predicted))\n",
    "    mse = float(mean_squared_error(true, predicted))\n",
    "    rmse = float(np.sqrt(mse))\n",
    "    mape = float(np.mean(np.abs((true - predicted) / true)) * 100)\n",
    "    \n",
    "    return {\n",
    "        \"MAE\": mae,\n",
    "        \"MSE\": mse,\n",
    "        \"RMSE\": rmse,\n",
    "        \"MAPE\": mape\n",
    "    }\n",
    "    \n",
    "metrics = evaluate_rul_metrics(true_rul, predictions)\n",
    "metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "921d23ee-83ee-4942-bc2e-54caf4081ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Trial Contents:\n",
      "{\n",
      "  \"trial_id\": \"0016\",\n",
      "  \"hyperparameters\": {\n",
      "    \"space\": [\n",
      "      {\n",
      "        \"class_name\": \"Int\",\n",
      "        \"config\": {\n",
      "          \"name\": \"num_lstm_layers\",\n",
      "          \"default\": null,\n",
      "          \"conditions\": [],\n",
      "          \"min_value\": 1,\n",
      "          \"max_value\": 3,\n",
      "          \"step\": 1,\n",
      "          \"sampling\": \"linear\"\n",
      "        }\n",
      "      },\n",
      "      {\n",
      "        \"class_name\": \"Int\",\n",
      "        \"config\": {\n",
      "          \"name\": \"units_1\",\n",
      "          \"default\": null,\n",
      "          \"conditions\": [],\n",
      "          \"min_value\": 128,\n",
      "          \"max_value\": 256,\n",
      "          \"step\": 32,\n",
      "          \"sampling\": \"linear\"\n",
      "        }\n",
      "      },\n",
      "      {\n",
      "        \"class_name\": \"Int\",\n",
      "        \"config\": {\n",
      "          \"name\": \"num_dense_layers\",\n",
      "          \"default\": null,\n",
      "          \"conditions\": [],\n",
      "          \"min_value\": 1,\n",
      "          \"max_value\": 2,\n",
      "          \"step\": 1,\n",
      "          \"sampling\": \"linear\"\n",
      "        }\n",
      "      },\n",
      "      {\n",
      "        \"class_name\": \"Int\",\n",
      "        \"config\": {\n",
      "          \"name\": \"dense_units_1\",\n",
      "          \"default\": null,\n",
      "          \"conditions\": [],\n",
      "          \"min_value\": 32,\n",
      "          \"max_value\": 256,\n",
      "          \"step\": 32,\n",
      "          \"sampling\": \"linear\"\n",
      "        }\n",
      "      },\n",
      "      {\n",
      "        \"class_name\": \"Float\",\n",
      "        \"config\": {\n",
      "          \"name\": \"learning_rate\",\n",
      "          \"default\": 0.0001,\n",
      "          \"conditions\": [],\n",
      "          \"min_value\": 0.0001,\n",
      "          \"max_value\": 0.01,\n",
      "          \"step\": null,\n",
      "          \"sampling\": \"linear\"\n",
      "        }\n",
      "      },\n",
      "      {\n",
      "        \"class_name\": \"Int\",\n",
      "        \"config\": {\n",
      "          \"name\": \"units_2\",\n",
      "          \"default\": null,\n",
      "          \"conditions\": [],\n",
      "          \"min_value\": 32,\n",
      "          \"max_value\": 128,\n",
      "          \"step\": 32,\n",
      "          \"sampling\": \"linear\"\n",
      "        }\n",
      "      },\n",
      "      {\n",
      "        \"class_name\": \"Int\",\n",
      "        \"config\": {\n",
      "          \"name\": \"dense_units_2\",\n",
      "          \"default\": null,\n",
      "          \"conditions\": [],\n",
      "          \"min_value\": 32,\n",
      "          \"max_value\": 256,\n",
      "          \"step\": 32,\n",
      "          \"sampling\": \"linear\"\n",
      "        }\n",
      "      },\n",
      "      {\n",
      "        \"class_name\": \"Int\",\n",
      "        \"config\": {\n",
      "          \"name\": \"units_3\",\n",
      "          \"default\": null,\n",
      "          \"conditions\": [],\n",
      "          \"min_value\": 32,\n",
      "          \"max_value\": 128,\n",
      "          \"step\": 32,\n",
      "          \"sampling\": \"linear\"\n",
      "        }\n",
      "      }\n",
      "    ],\n",
      "    \"values\": {\n",
      "      \"num_lstm_layers\": 2,\n",
      "      \"units_1\": 224,\n",
      "      \"num_dense_layers\": 1,\n",
      "      \"dense_units_1\": 160,\n",
      "      \"learning_rate\": 0.0019205962288415628,\n",
      "      \"units_2\": 32,\n",
      "      \"dense_units_2\": 192,\n",
      "      \"units_3\": 96,\n",
      "      \"tuner/epochs\": 10,\n",
      "      \"tuner/initial_epoch\": 4,\n",
      "      \"tuner/bracket\": 2,\n",
      "      \"tuner/round\": 2,\n",
      "      \"tuner/trial_id\": \"0012\"\n",
      "    }\n",
      "  },\n",
      "  \"metrics\": {\n",
      "    \"metrics\": {\n",
      "      \"loss\": {\n",
      "        \"direction\": \"min\",\n",
      "        \"observations\": [\n",
      "          {\n",
      "            \"value\": [\n",
      "              42.00425338745117\n",
      "            ],\n",
      "            \"step\": 5\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      \"val_loss\": {\n",
      "        \"direction\": \"min\",\n",
      "        \"observations\": [\n",
      "          {\n",
      "            \"value\": [\n",
      "              45.899444580078125\n",
      "            ],\n",
      "            \"step\": 5\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    }\n",
      "  },\n",
      "  \"score\": 45.899444580078125,\n",
      "  \"best_step\": 5,\n",
      "  \"status\": \"COMPLETED\",\n",
      "  \"message\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Path to your tuning logs directory\n",
    "log_dir = '../ml_models/hyperparam_tuning/lstm_tuning_refined/'\n",
    "\n",
    "# Initialize variables to keep track of the best trial and its score\n",
    "best_trial = None\n",
    "best_score = float('inf')  # Assuming lower score is better, adjust if higher is better\n",
    "\n",
    "# Walk through the log directory and find all the trial folders\n",
    "for trial_folder in os.listdir(log_dir):\n",
    "    trial_folder_path = os.path.join(log_dir, trial_folder)\n",
    "    \n",
    "    # Check if this is a folder (trial folder)\n",
    "    if os.path.isdir(trial_folder_path):\n",
    "        trial_json_path = os.path.join(trial_folder_path, 'trial.json')\n",
    "        \n",
    "        # Check if the trial.json file exists in this folder\n",
    "        if os.path.isfile(trial_json_path):\n",
    "            # Read the JSON file\n",
    "            with open(trial_json_path, 'r') as f:\n",
    "                trial_data = json.load(f)\n",
    "                \n",
    "                # Extract the performance score (e.g., validation loss or other metric)\n",
    "                score = trial_data.get('score')\n",
    "                \n",
    "                # Check if this is the best trial based on the score\n",
    "                if score is not None and score < best_score:\n",
    "                    best_score = score\n",
    "                    best_trial = trial_data\n",
    "\n",
    "# After the loop, best_trial contains the best trial's hyperparameters\n",
    "if best_trial is not None:\n",
    "    print(\"Best Trial Contents:\")\n",
    "    print(json.dumps(best_trial, indent=2))\n",
    "else:\n",
    "    print(\"No trials found.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ff4d11-0303-4d92-9d27-5295b4735a1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
