2024-11-24 21:49:57,575 INFO    MainThread:29056 [wandb_setup.py:_flush():79] Current SDK version is 0.18.6
2024-11-24 21:49:57,576 INFO    MainThread:29056 [wandb_setup.py:_flush():79] Configure stats pid to 29056
2024-11-24 21:49:57,576 INFO    MainThread:29056 [wandb_setup.py:_flush():79] Loading settings from C:\Users\e_nkanungo\.config\wandb\settings
2024-11-24 21:49:57,576 INFO    MainThread:29056 [wandb_setup.py:_flush():79] Loading settings from c:\Users\e_nkanungo\Desktop\MADS_Capstone\RUL_Prediction\models\wandb\settings
2024-11-24 21:49:57,576 INFO    MainThread:29056 [wandb_setup.py:_flush():79] Loading settings from environment variables: {}
2024-11-24 21:49:57,576 INFO    MainThread:29056 [wandb_setup.py:_flush():79] Applying setup settings: {'mode': None, '_disable_service': None}
2024-11-24 21:49:57,576 INFO    MainThread:29056 [wandb_setup.py:_flush():79] Inferring run settings from compute environment: {'program': '<python with no main file>'}
2024-11-24 21:49:57,577 INFO    MainThread:29056 [wandb_setup.py:_flush():79] Applying login settings: {}
2024-11-24 21:49:57,577 INFO    MainThread:29056 [wandb_init.py:_log_setup():533] Logging user logs to c:\Users\e_nkanungo\Desktop\MADS_Capstone\RUL_Prediction\models\wandb\run-20241124_214957-78dls8aa\logs\debug.log
2024-11-24 21:49:57,577 INFO    MainThread:29056 [wandb_init.py:_log_setup():534] Logging internal logs to c:\Users\e_nkanungo\Desktop\MADS_Capstone\RUL_Prediction\models\wandb\run-20241124_214957-78dls8aa\logs\debug-internal.log
2024-11-24 21:49:57,577 INFO    MainThread:29056 [wandb_init.py:_jupyter_setup():479] configuring jupyter hooks <wandb.sdk.wandb_init._WandbInit object at 0x000001FB3D965FD0>
2024-11-24 21:49:57,577 INFO    MainThread:29056 [wandb_init.py:init():619] calling init triggers
2024-11-24 21:49:57,578 INFO    MainThread:29056 [wandb_init.py:init():626] wandb.init called with sweep_config: {}
config: {}
2024-11-24 21:49:57,578 INFO    MainThread:29056 [wandb_init.py:init():669] starting backend
2024-11-24 21:49:57,578 INFO    MainThread:29056 [wandb_init.py:init():673] sending inform_init request
2024-11-24 21:49:57,582 INFO    MainThread:29056 [backend.py:_multiprocessing_setup():104] multiprocessing start_methods=spawn, using: spawn
2024-11-24 21:49:57,583 INFO    MainThread:29056 [wandb_init.py:init():686] backend started and connected
2024-11-24 21:49:57,598 INFO    MainThread:29056 [wandb_run.py:_label_probe_notebook():1341] probe notebook
2024-11-24 21:49:58,059 INFO    MainThread:29056 [wandb_init.py:init():781] updated telemetry
2024-11-24 21:49:58,132 INFO    MainThread:29056 [wandb_init.py:init():814] communicating run to backend with 90.0 second timeout
2024-11-24 21:49:58,370 INFO    MainThread:29056 [wandb_init.py:init():867] starting run threads in backend
2024-11-24 21:49:58,600 INFO    MainThread:29056 [wandb_run.py:_console_start():2451] atexit reg
2024-11-24 21:49:58,600 INFO    MainThread:29056 [wandb_run.py:_redirect():2299] redirect: wrap_raw
2024-11-24 21:49:58,601 INFO    MainThread:29056 [wandb_run.py:_redirect():2364] Wrapping output streams.
2024-11-24 21:49:58,601 INFO    MainThread:29056 [wandb_run.py:_redirect():2389] Redirects installed.
2024-11-24 21:49:58,605 INFO    MainThread:29056 [wandb_init.py:init():911] run started, returning control to user process
2024-11-24 21:49:58,608 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_21-49-51_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 21:49:58,610 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 21:49:58,610 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 21:49:59,430 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:49:59,430 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:50:31,973 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:50:31,978 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:50:31,978 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:50:32,020 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:50:32,026 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:50:32,026 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:50:32,084 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:50:32,090 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:50:32,090 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:50:32,208 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:50:32,214 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:50:32,214 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:50:34,404 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:50:37,614 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_21-50-35_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 21:50:37,618 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 21:50:37,618 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 21:50:38,058 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:50:38,058 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:50:55,626 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:50:55,636 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:50:55,636 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:50:55,661 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:50:55,669 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:50:55,670 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:50:55,701 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:50:55,706 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:50:55,706 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:51:00,079 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:51:03,411 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_21-51-00_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 21:51:03,414 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 21:51:03,414 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 21:51:04,055 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:51:04,055 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:51:15,052 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:51:18,173 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_21-51-15_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 21:51:18,177 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 21:51:18,177 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 21:51:18,547 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:51:18,547 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:52:49,858 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:52:49,901 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:52:49,901 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:52:49,971 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:52:49,992 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:52:49,992 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:52:50,060 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:52:50,076 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:52:50,076 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:52:50,141 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:52:50,160 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:52:50,165 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:52:50,220 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:52:50,236 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:52:50,242 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:52:50,298 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:52:50,311 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:52:50,312 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:52:50,369 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:52:50,385 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:52:50,386 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:52:50,464 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:52:50,475 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:52:50,476 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:52:50,533 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:52:50,547 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:52:50,547 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:52:50,612 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:52:50,631 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:52:50,631 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:53:07,258 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:53:07,263 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:53:07,263 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:53:07,282 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:53:07,287 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:53:07,287 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:53:07,390 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:53:07,395 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:53:07,396 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:53:07,876 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:53:07,881 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:53:07,881 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:53:12,013 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:53:12,020 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:53:12,020 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:53:12,185 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:53:12,189 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:53:12,189 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:53:12,325 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:53:12,332 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:53:12,332 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:53:12,471 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:53:12,478 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:53:12,478 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:53:12,628 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:53:12,634 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:53:12,634 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:53:20,663 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:53:20,668 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:53:20,669 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:53:20,878 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:53:20,884 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:53:20,884 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:53:21,050 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:53:21,056 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:53:21,056 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:53:21,242 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:53:21,245 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:53:21,245 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:53:21,376 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:53:21,381 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:53:21,381 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:53:26,082 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:53:26,087 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:53:26,087 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:53:26,103 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:53:26,108 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:53:26,108 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:53:26,171 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:53:26,176 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:53:26,176 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:53:26,251 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:53:26,256 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:53:26,256 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:53:27,178 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:53:27,186 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:53:27,186 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:53:27,201 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:53:27,206 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:53:27,206 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:53:27,684 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:53:27,690 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:53:27,692 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:53:32,854 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:53:36,127 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_21-53-33_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 21:53:36,132 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 21:53:36,132 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 21:53:39,956 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_21-53-37_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 21:53:39,960 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 21:53:39,960 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 21:53:45,647 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_21-53-40_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 21:53:45,651 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 21:53:45,651 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 21:53:47,418 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:53:47,418 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:53:59,617 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:53:59,624 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:53:59,624 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:53:59,650 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:53:59,658 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:53:59,658 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:53:59,741 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:53:59,745 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:53:59,745 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:54:11,368 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:54:11,372 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:54:11,372 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:54:11,834 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:54:11,839 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:54:11,839 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:54:13,002 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:54:13,008 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:54:13,008 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:54:13,347 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:54:13,352 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:54:13,352 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:54:13,625 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:54:13,629 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:54:13,629 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:54:13,782 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:54:13,787 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:54:13,787 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:54:13,920 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:54:13,924 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:54:13,924 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:54:14,168 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:54:14,172 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:54:14,172 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:54:14,338 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:54:14,343 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:54:14,343 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:54:14,526 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:54:14,531 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:54:14,531 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:54:14,652 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:54:14,658 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:54:14,658 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:54:26,831 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:54:26,836 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:54:26,836 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:54:28,047 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:54:28,055 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:54:28,055 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:54:28,218 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:54:28,224 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:54:28,224 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:54:28,454 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:54:28,458 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:54:28,458 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:54:28,545 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:54:28,550 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:54:28,550 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:54:28,818 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:54:28,824 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:54:28,824 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:54:28,953 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:54:28,958 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:54:28,958 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:54:29,127 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:54:29,131 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:54:29,133 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:54:29,275 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:54:29,279 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:54:29,279 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:54:32,418 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:54:32,425 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:54:32,425 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:54:32,454 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:54:32,461 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:54:32,461 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:54:32,560 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:54:32,566 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:54:32,566 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:54:33,479 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:54:33,485 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:54:33,486 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:54:33,850 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:54:33,856 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:54:33,856 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:54:34,585 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:54:34,594 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:54:34,594 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:54:34,747 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:54:34,753 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:54:34,753 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:54:34,896 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:54:34,902 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:54:34,902 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:54:35,033 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:54:35,039 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:54:35,039 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:54:35,316 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:54:35,320 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:54:35,320 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:54:35,502 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:54:35,506 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:54:35,506 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:54:35,647 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:54:35,655 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:54:35,655 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:54:35,811 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:54:35,817 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:54:35,817 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:54:42,135 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:54:42,140 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:54:42,140 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:54:42,152 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:54:42,156 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:54:42,156 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:54:42,990 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:54:42,995 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:54:42,995 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:54:43,161 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:54:43,166 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:54:43,166 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:54:43,378 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:54:43,382 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:54:43,383 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:54:43,799 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:54:43,805 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:54:43,805 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:54:44,170 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:54:44,178 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:54:44,178 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:54:44,623 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:54:44,630 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:54:44,630 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:54:47,687 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:54:47,696 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:54:47,696 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:54:47,738 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:54:47,745 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:54:47,745 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:54:48,161 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:54:48,188 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:54:48,188 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:54:48,561 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:54:48,566 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:54:48,566 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:55:00,252 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:55:03,939 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_21-55-01_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 21:55:03,943 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 21:55:03,943 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 21:55:04,852 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:55:04,852 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:55:34,314 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:55:34,321 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:55:34,321 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:55:34,457 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:55:34,463 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:55:34,464 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:55:34,716 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:55:34,722 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:55:34,722 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:55:34,859 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:55:34,863 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:55:34,863 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:55:35,107 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:55:35,112 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:55:35,112 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:55:35,279 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:55:35,284 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:55:35,284 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:55:35,404 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:55:35,408 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:55:35,408 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:55:35,795 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:55:35,801 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:55:35,801 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:55:37,353 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:55:37,359 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:55:37,360 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:55:37,379 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:55:37,385 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:55:37,385 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:55:38,329 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:55:38,335 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:55:38,335 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:55:42,413 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:55:42,417 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:55:42,418 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:55:42,610 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:55:42,615 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:55:42,615 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:55:42,748 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:55:42,754 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:55:42,754 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:55:42,904 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:55:42,909 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:55:42,909 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:55:43,078 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:55:43,082 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:55:43,082 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:55:59,991 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:55:59,996 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:55:59,997 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:56:00,893 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:56:00,897 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:56:00,897 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:56:01,254 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:56:01,259 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:56:01,259 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:56:01,580 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:56:01,593 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:56:01,593 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:56:01,689 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:56:01,694 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:56:01,694 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:56:01,798 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:56:01,803 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:56:01,804 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:56:02,005 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:56:02,010 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:56:02,010 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:56:02,204 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:56:02,209 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:56:02,209 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:56:02,332 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:56:02,337 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:56:02,337 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:56:02,700 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:56:02,704 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:56:02,704 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:56:02,875 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:56:02,879 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:56:02,880 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:56:02,996 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:56:03,000 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:56:03,001 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:56:03,173 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:56:03,178 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:56:03,178 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:56:03,323 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:56:03,327 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:56:03,328 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:56:16,858 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:56:20,484 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_21-56-18_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 21:56:20,488 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 21:56:20,488 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 21:56:24,547 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_21-56-21_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 21:56:24,550 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 21:56:24,550 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 21:56:30,406 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_21-56-25_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 21:56:30,408 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 21:56:30,408 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 21:56:35,693 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_21-56-31_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 21:56:35,697 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 21:56:35,697 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 21:56:41,006 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:56:41,006 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:56:42,652 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:56:45,973 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_21-56-43_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 21:56:45,977 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 21:56:45,977 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 21:56:49,623 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_21-56-46_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 21:56:49,626 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 21:56:49,626 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 21:56:55,291 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_21-56-50_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 21:56:55,295 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 21:56:55,295 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 21:57:00,210 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_21-56-56_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 21:57:00,214 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 21:57:00,214 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 21:57:05,436 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:57:05,436 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:57:14,653 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:57:17,843 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_21-57-15_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 21:57:17,847 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 21:57:17,848 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 21:57:18,172 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:57:18,172 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:57:35,161 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:57:35,165 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:57:35,165 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:57:35,186 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:57:35,190 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:57:35,190 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:57:35,266 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:57:35,270 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:57:35,271 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:57:35,388 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:57:35,418 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:57:35,418 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:57:35,443 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:57:35,451 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:57:35,451 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:57:36,171 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:57:36,176 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:57:36,176 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:57:36,208 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:57:36,213 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:57:36,213 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:57:36,222 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:57:36,228 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:57:36,229 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:57:52,283 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:57:52,289 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:57:52,289 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:57:58,707 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:57:58,713 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:57:58,713 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:57:58,738 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:57:58,745 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:57:58,745 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:57:58,802 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:57:58,808 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:57:58,808 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:57:58,868 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:57:58,874 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:57:58,874 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:57:59,534 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:57:59,543 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:57:59,543 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:57:59,573 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:57:59,585 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:57:59,586 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:58:00,211 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:58:03,234 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_21-58-00_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 21:58:03,239 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 21:58:03,239 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 21:58:03,519 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:58:03,519 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:58:48,439 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:58:48,447 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:58:48,447 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:58:48,601 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:58:48,606 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:58:48,606 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:58:48,855 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:58:48,861 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:58:48,861 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:58:49,006 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:58:49,011 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:58:49,011 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:58:49,207 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:58:49,212 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:58:49,212 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:58:50,250 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:58:50,258 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:58:50,258 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:58:50,397 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:58:50,403 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:58:50,403 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:58:50,516 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:58:50,519 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:58:50,521 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:58:50,955 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:58:50,961 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:58:50,961 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:58:51,740 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:58:51,746 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:58:51,746 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:58:51,847 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:58:51,853 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:58:51,853 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:58:52,035 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:58:52,040 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:58:52,040 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:58:52,159 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:58:52,163 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:58:52,164 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:58:52,314 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:58:52,319 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:58:52,319 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 21:59:07,531 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 21:59:10,705 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_21-59-08_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 21:59:10,710 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 21:59:10,710 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 21:59:10,980 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 21:59:10,980 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:00:28,672 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:00:28,691 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:00:28,702 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:00:50,044 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:00:53,011 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-00-50_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:00:53,015 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:00:53,015 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:00:53,113 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:00:53,113 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:01:40,008 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:01:40,044 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:01:40,056 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:01:40,104 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:01:40,122 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:01:40,123 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:01:40,169 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:01:40,182 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:01:40,182 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:01:40,235 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:01:40,246 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:01:40,247 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:01:40,300 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:01:40,311 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:01:40,317 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:01:40,363 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:01:40,381 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:01:40,382 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:01:40,427 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:01:40,439 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:01:40,439 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:01:40,492 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:01:40,505 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:01:40,505 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:01:40,574 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:01:40,587 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:01:40,587 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:01:40,647 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:01:40,659 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:01:40,659 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:01:40,710 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:01:40,726 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:01:40,727 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:01:48,854 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:01:51,723 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-01-49_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:01:51,725 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:01:51,725 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:01:52,034 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:01:52,034 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:03:38,245 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:03:38,250 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:03:38,250 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:03:38,443 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:03:38,448 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:03:38,448 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:03:38,692 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:03:38,698 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:03:38,698 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:03:39,520 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:03:39,526 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:03:39,526 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:03:40,021 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:03:40,025 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:03:40,026 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:03:40,144 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:03:40,149 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:03:40,149 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:03:43,270 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:03:43,276 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:03:43,276 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:03:43,305 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:03:43,310 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:03:43,310 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:03:43,474 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:03:43,479 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:03:43,479 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:03:53,333 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:03:53,339 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:03:53,339 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:03:53,357 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:03:53,362 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:03:53,362 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:03:53,459 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:03:53,464 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:03:53,464 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:04:06,708 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:04:09,548 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-04-07_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:04:09,552 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:04:09,552 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:04:10,280 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:04:10,280 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:04:35,526 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:04:35,570 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:04:35,575 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:04:35,623 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:04:35,641 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:04:35,642 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:04:35,693 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:04:35,708 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:04:35,708 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:04:35,782 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:04:35,805 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:04:35,806 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:04:35,884 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:04:35,899 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:04:35,902 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:04:35,951 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:04:35,969 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:04:35,969 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:04:36,018 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:04:36,030 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:04:36,031 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:04:36,085 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:04:36,098 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:04:36,098 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:04:36,174 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:04:36,188 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:04:36,188 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:04:36,242 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:04:36,255 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:04:36,255 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:04:36,306 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:04:36,323 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:04:36,324 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:04:49,292 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:04:49,301 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:04:49,302 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:04:49,337 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:04:49,363 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:04:49,364 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:04:51,340 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:04:51,344 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:04:51,345 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:04:51,357 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:04:51,362 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:04:51,362 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:04:51,506 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:04:51,526 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:04:51,526 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:04:53,077 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:04:53,081 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:04:53,081 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:04:53,105 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:04:53,110 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:04:53,110 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:04:53,144 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:04:53,159 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:04:53,160 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:04:53,269 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:04:53,282 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:04:53,282 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:04:57,094 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:04:59,943 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-04-57_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:04:59,947 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:04:59,947 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:05:00,306 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:05:00,306 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:05:40,470 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:05:40,488 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:05:40,496 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:05:40,568 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:05:40,585 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:05:40,586 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:05:40,633 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:05:40,646 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:05:40,646 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:05:40,699 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:05:40,712 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:05:40,713 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:05:40,767 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:05:40,778 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:05:40,783 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:05:40,830 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:05:40,844 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:05:40,849 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:05:40,903 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:05:40,918 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:05:40,918 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:05:41,333 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:05:41,346 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:05:41,346 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:05:41,426 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:05:41,442 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:05:41,442 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:05:41,500 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:05:41,512 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:05:41,512 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:05:41,564 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:05:41,581 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:05:41,582 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:05:48,675 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:05:51,501 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-05-49_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:05:51,503 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:05:51,503 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:05:51,793 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:05:51,793 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:13:17,198 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:13:17,517 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:13:17,517 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:13:17,547 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:13:17,551 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:13:17,551 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:13:17,578 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:13:17,601 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:13:17,603 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:13:17,634 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:13:17,635 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:13:17,635 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:13:17,647 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:13:17,658 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:13:17,658 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:13:17,667 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:13:17,677 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:13:17,677 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:13:17,690 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:13:19,805 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:13:19,805 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:13:19,818 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:13:19,895 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:13:19,895 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:13:19,906 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:13:20,324 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:13:20,324 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:13:20,338 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:13:20,340 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:13:20,340 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:13:20,351 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:13:20,354 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:13:20,354 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:13:20,366 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:13:20,370 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:13:20,370 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:13:20,380 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:13:20,384 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:13:20,385 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:13:20,402 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:13:23,183 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-13-21_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:13:23,186 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:13:23,186 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:13:23,502 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:13:23,502 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:13:48,878 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:13:48,883 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:13:48,883 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:13:49,082 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:13:49,086 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:13:49,086 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:13:49,348 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:13:49,353 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:13:49,353 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:13:50,400 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:13:50,407 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:13:50,408 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:13:50,553 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:13:50,559 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:13:50,559 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:13:50,725 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:13:50,728 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:13:50,729 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:13:55,286 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:13:55,292 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:13:55,292 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:13:55,456 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:13:55,461 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:13:55,461 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:13:55,788 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:13:55,793 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:13:55,793 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:13:58,674 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:13:58,751 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:13:58,752 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:13:58,779 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:13:58,782 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:13:58,782 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:13:58,798 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:13:58,809 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:13:58,809 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:13:58,820 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:13:58,822 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:13:58,822 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:13:58,831 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:13:58,839 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:13:58,839 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:13:58,852 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:13:58,862 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:13:58,862 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:13:58,884 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:14:01,074 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:14:01,074 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:14:01,088 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:14:01,169 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:14:01,169 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:14:01,181 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:14:01,681 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:14:01,681 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:14:01,694 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:14:01,697 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:14:01,697 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:14:01,708 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:14:01,709 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:14:01,709 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:14:01,722 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:14:01,727 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:14:01,727 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:14:01,741 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:14:01,746 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:14:01,747 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:14:01,777 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:14:04,993 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-14-02_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:14:04,996 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:14:04,996 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:14:05,321 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:14:05,321 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:15:12,183 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:15:12,221 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:15:12,230 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:15:12,279 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:15:12,296 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:15:12,296 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:15:12,345 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:15:12,360 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:15:12,360 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:15:12,413 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:15:12,425 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:15:12,425 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:15:12,506 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:15:12,520 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:15:12,524 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:15:12,575 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:15:12,593 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:15:12,594 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:15:12,645 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:15:12,659 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:15:12,659 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:15:12,712 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:15:12,725 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:15:12,725 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:15:12,802 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:15:12,814 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:15:12,815 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:15:13,243 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:15:13,258 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:15:13,258 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:15:13,317 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:15:13,336 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:15:13,336 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:15:21,388 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:15:24,211 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-15-22_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:15:24,215 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:15:24,215 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:15:24,288 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:15:24,289 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:15:42,896 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:15:42,980 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:15:42,980 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:15:43,015 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:15:43,017 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:15:43,018 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:15:43,034 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:15:43,043 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:15:43,043 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:15:43,074 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:15:43,076 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:15:43,076 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:15:43,088 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:15:43,097 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:15:43,097 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:15:43,105 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:15:43,116 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:15:43,116 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:15:43,127 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:15:45,262 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:15:45,262 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:15:45,271 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:15:45,348 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:15:45,349 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:15:45,357 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:15:45,720 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:15:45,720 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:15:45,735 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:15:45,736 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:15:45,736 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:15:45,748 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:15:45,749 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:15:45,749 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:15:45,760 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:15:45,766 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:15:45,766 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:15:45,788 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:15:45,793 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:15:45,793 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:15:45,811 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:15:48,559 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-15-46_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:15:48,563 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:15:48,563 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:15:48,654 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:15:48,654 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:15:48,662 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:15:48,669 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:15:48,669 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:15:50,892 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:15:50,978 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:15:50,979 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:15:50,999 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:15:51,001 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:15:51,002 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:15:51,014 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:15:51,025 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:15:51,025 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:15:51,034 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:15:51,035 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:15:51,035 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:15:51,043 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:15:51,054 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:15:51,055 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:15:51,067 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:15:51,079 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:15:51,080 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:15:51,096 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:15:53,061 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:15:53,062 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:15:53,071 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:15:53,146 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:15:53,146 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:15:53,156 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:15:53,521 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:15:53,521 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:15:53,534 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:15:53,536 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:15:53,536 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:15:53,545 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:15:53,547 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:15:53,548 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:15:53,560 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:15:53,563 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:15:53,564 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:15:53,574 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:15:53,580 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:15:53,580 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:15:53,607 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:15:56,431 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-15-54_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:15:56,435 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:15:56,435 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:15:59,642 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-15-57_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:15:59,645 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:15:59,645 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:16:04,599 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-16-00_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:16:04,601 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:16:04,601 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:16:09,362 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-16-05_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:16:09,365 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:16:09,365 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:16:13,672 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-16-10_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:16:13,675 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:16:13,675 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:16:18,141 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-16-14_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:16:18,144 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:16:18,144 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:16:23,890 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-16-18_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:16:23,893 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:16:23,893 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:16:25,406 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:16:25,406 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:17:20,886 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:17:20,899 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:17:20,948 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:17:20,998 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:17:21,016 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:17:21,016 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:17:21,448 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:17:21,463 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:17:21,464 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:17:21,515 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:17:21,528 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:17:21,528 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:17:21,582 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:17:21,594 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:17:21,598 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:17:21,646 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:17:21,664 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:17:21,664 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:17:21,737 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:17:21,754 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:17:21,754 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:17:21,813 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:17:21,827 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:17:21,828 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:17:21,903 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:17:21,917 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:17:21,917 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:17:21,974 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:17:21,986 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:17:21,987 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:17:22,044 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:17:22,063 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:17:22,063 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:17:32,725 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:17:35,758 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-17-33_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:17:35,761 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:17:35,761 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:17:36,146 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:17:36,146 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:18:52,657 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:18:52,704 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:18:52,704 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:18:52,755 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:18:52,771 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:18:52,771 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:18:52,823 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:18:52,836 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:18:52,837 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:18:52,888 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:18:52,902 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:18:52,903 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:18:52,955 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:18:52,966 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:18:52,966 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:18:53,023 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:18:53,037 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:18:53,040 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:18:53,094 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:18:53,107 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:18:53,109 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:18:53,164 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:18:53,178 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:18:53,178 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:18:53,252 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:18:53,267 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:18:53,267 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:18:53,699 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:18:53,711 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:18:53,711 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:18:53,777 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:18:53,806 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:18:53,806 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:19:53,815 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:19:56,605 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-19-54_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:19:56,609 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:19:56,609 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:19:56,919 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:19:56,919 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:20:20,893 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:20:20,898 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:20:20,898 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:20:20,914 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:20:20,920 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:20:20,921 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:20:20,997 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:20:21,002 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:20:21,003 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:20:21,115 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:20:21,119 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:20:21,120 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:20:22,043 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:20:22,049 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:20:22,049 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:20:22,075 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:20:22,078 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:20:22,078 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:20:23,218 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:20:23,238 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:20:23,238 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:20:29,122 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:20:29,130 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:20:29,130 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:20:31,001 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:20:31,013 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:20:31,013 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:20:31,038 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:20:31,057 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:20:31,057 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:20:31,165 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:20:31,170 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:20:31,170 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:20:32,855 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:20:32,860 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:20:32,860 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:20:33,230 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:20:33,237 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:20:33,237 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:20:33,269 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:20:33,276 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:20:33,276 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:20:34,222 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:20:34,228 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:20:34,228 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:20:34,257 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:20:34,261 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:20:34,262 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:20:34,534 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:20:34,539 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:20:34,539 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:20:34,652 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:20:34,657 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:20:34,657 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:20:35,502 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:20:35,508 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:20:35,509 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:20:35,602 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:20:35,608 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:20:35,608 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:20:37,612 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:20:37,619 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:20:37,619 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:20:37,866 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:20:37,871 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:20:37,871 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:20:37,989 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:20:37,993 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:20:37,994 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:20:38,129 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:20:38,133 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:20:38,133 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:20:38,284 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:20:38,290 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:20:38,290 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:20:41,899 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:20:41,905 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:20:41,905 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:20:48,390 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:20:51,224 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-20-49_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:20:51,229 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:20:51,229 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:20:51,960 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:20:51,960 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:21:22,283 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:21:25,147 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-21-23_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:21:25,150 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:21:25,150 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:21:25,456 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:21:25,456 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:21:48,182 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:21:48,189 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:21:48,189 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:21:48,205 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:21:48,212 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:21:48,212 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:21:48,253 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:21:48,282 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:21:48,282 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:21:49,519 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:21:49,525 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:21:49,525 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:21:49,697 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:21:49,702 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:21:49,702 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:21:58,680 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:22:01,687 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-21-59_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:22:01,690 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:22:01,690 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:22:01,995 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:22:01,996 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:22:24,454 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:22:27,211 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-22-25_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:22:27,214 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:22:27,214 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:22:30,620 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-22-28_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:22:30,624 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:22:30,624 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:22:35,816 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-22-31_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:22:35,819 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:22:35,819 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:22:40,469 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-22-36_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:22:40,472 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:22:40,472 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:22:45,375 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-22-41_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:22:45,378 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:22:45,378 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:22:50,156 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-22-46_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:22:50,159 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:22:50,159 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:22:56,234 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-22-51_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:22:56,237 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:22:56,237 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:23:02,457 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-22-57_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:23:02,460 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:23:02,460 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:23:06,249 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-23-03_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:23:06,254 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:23:06,255 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:23:08,401 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:23:08,401 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:24:16,804 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:24:19,730 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-24-17_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:24:19,735 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:24:19,735 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:24:23,328 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-24-20_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:24:23,332 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:24:23,332 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:24:28,512 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-24-24_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:24:28,515 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:24:28,515 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:24:33,336 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-24-29_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:24:33,339 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:24:33,339 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:24:37,971 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-24-34_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:24:37,974 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:24:37,974 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:24:42,733 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-24-39_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:24:42,736 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:24:42,736 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:24:50,514 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-24-43_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:24:50,519 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:24:50,519 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:24:57,377 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-24-51_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:24:57,380 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:24:57,380 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:25:01,054 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-24-58_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:25:01,056 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:25:01,056 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:25:08,177 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-25-02_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:25:08,180 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:25:08,180 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:25:12,764 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-25-09_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:25:12,767 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:25:12,767 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:25:20,727 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-25-14_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:25:20,730 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:25:20,730 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:25:27,901 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-25-21_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:25:27,904 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:25:27,904 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:25:31,574 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-25-28_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:25:31,577 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:25:31,577 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:25:36,163 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-25-32_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:25:36,165 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:25:36,165 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:25:41,766 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-25-37_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:25:41,769 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:25:41,769 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:25:47,883 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-25-42_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:25:47,885 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:25:47,885 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:25:53,396 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-25-49_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:25:53,400 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:25:53,400 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:25:58,723 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-25-54_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:25:58,726 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:25:58,726 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:26:05,452 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-25-59_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:26:05,455 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:26:05,455 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:26:11,395 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-26-06_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:26:11,398 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:26:11,398 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:26:14,666 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-26-12_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:26:14,669 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:26:14,669 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:26:19,988 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-26-15_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:26:19,992 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:26:19,992 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:26:23,084 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:26:23,084 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:27:20,936 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:27:20,959 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:27:20,965 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:27:21,036 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:27:21,056 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:27:21,056 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:27:21,106 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:27:21,120 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:27:21,120 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:27:21,175 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:27:21,188 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:27:21,188 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:27:21,241 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:27:21,255 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:27:21,260 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:27:21,767 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:27:21,786 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:27:21,786 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:27:21,837 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:27:21,851 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:27:21,851 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:27:21,910 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:27:21,923 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:27:21,923 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:27:22,014 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:27:22,030 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:27:22,031 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:27:22,124 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:27:22,138 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:27:22,138 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:27:22,201 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:27:22,223 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:27:22,224 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:27:28,410 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:27:28,495 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:27:28,497 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:27:28,538 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:27:28,540 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:27:28,540 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:27:28,560 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:27:28,568 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:27:28,568 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:27:28,585 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:27:28,586 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:27:28,586 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:27:28,603 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:27:28,612 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:27:28,612 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:27:28,622 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:27:28,634 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:27:28,634 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:27:28,665 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:27:31,154 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:27:31,154 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:27:31,167 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:27:31,253 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:27:31,253 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:27:31,266 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:27:31,652 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:27:31,653 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:27:31,667 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:27:31,668 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:27:31,669 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:27:31,681 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:27:31,682 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:27:31,682 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:27:31,694 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:27:31,697 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:27:31,697 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:27:31,709 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:27:31,714 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:27:31,714 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:27:31,735 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:27:34,485 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-27-32_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:27:34,488 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:27:34,489 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:27:38,048 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-27-35_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:27:38,051 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:27:38,051 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:27:43,268 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-27-39_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:27:43,271 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:27:43,271 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:27:48,246 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-27-44_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:27:48,250 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:27:48,250 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:27:52,892 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-27-49_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:27:52,895 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:27:52,895 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:27:57,498 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-27-53_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:27:57,501 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:27:57,501 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:28:03,364 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-27-58_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:28:03,367 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:28:03,367 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:28:09,443 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-28-04_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:28:09,446 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:28:09,446 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:28:13,051 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-28-10_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:28:13,053 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:28:13,053 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:28:20,383 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-28-14_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:28:20,386 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:28:20,386 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:28:25,123 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-28-21_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:28:25,125 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:28:25,125 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:28:33,756 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-28-26_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:28:33,760 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:28:33,760 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:28:40,418 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-28-34_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:28:40,421 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:28:40,421 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:28:43,745 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-28-41_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:28:43,748 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:28:43,748 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:28:47,830 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-28-44_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:28:47,833 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:28:47,833 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:28:52,605 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-28-48_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:28:52,609 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:28:52,609 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:28:58,417 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-28-53_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:28:58,419 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:28:58,419 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:29:03,605 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-28-59_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:29:03,608 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:29:03,608 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:29:09,111 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-29-04_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:29:09,113 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:29:09,113 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:29:15,357 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-29-10_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:29:15,361 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:29:15,361 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:29:21,199 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-29-16_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:29:21,202 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:29:21,202 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:29:24,462 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-29-22_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:29:24,465 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:29:24,465 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:29:29,692 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-29-25_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:29:29,695 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:29:29,695 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:29:36,487 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-29-30_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:29:36,492 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:29:36,492 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:29:40,874 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-29-38_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:29:40,877 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:29:40,877 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:29:45,110 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-29-41_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:29:45,112 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:29:45,112 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:29:50,994 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-29-46_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:29:50,998 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:29:50,998 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:29:57,170 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-29-52_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:29:57,173 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:29:57,173 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:30:03,572 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-29-58_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:30:03,574 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:30:03,574 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:30:10,296 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-30-04_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:30:10,299 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:30:10,299 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:30:17,416 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-30-11_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:30:17,419 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:30:17,419 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:30:23,429 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-30-18_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:30:23,432 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:30:23,432 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:30:27,412 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-30-24_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:30:27,415 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:30:27,415 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:30:35,457 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-30-28_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:30:35,460 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:30:35,460 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:30:42,613 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-30-36_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:30:42,616 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:30:42,616 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:30:49,086 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-30-44_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:30:49,088 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:30:49,088 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:30:55,062 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-30-50_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:30:55,065 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:30:55,065 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:31:00,421 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-30-56_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:31:00,423 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:31:00,423 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:31:03,909 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-31-01_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:31:03,912 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:31:03,913 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:31:09,463 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-31-04_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:31:09,466 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:31:09,467 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:31:14,791 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-31-10_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:31:14,793 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:31:14,793 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:31:21,157 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-31-16_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:31:21,161 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:31:21,161 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:31:27,629 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-31-22_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:31:27,632 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:31:27,632 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:31:31,505 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-31-28_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:31:31,509 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:31:31,509 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:31:37,650 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-31-32_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:31:37,653 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:31:37,653 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:31:44,474 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-31-38_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:31:44,478 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:31:44,478 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:31:49,379 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-31-45_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:31:49,383 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:31:49,383 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:31:54,595 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-31-51_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:31:54,598 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:31:54,598 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:32:04,635 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-31-55_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:32:04,638 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:32:04,638 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:32:09,303 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-32-05_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:32:09,306 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:32:09,306 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:32:15,213 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-32-10_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:32:15,216 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:32:15,216 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:32:22,212 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-32-16_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:32:22,215 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:32:22,215 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:32:28,662 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-32-23_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:32:28,664 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:32:28,664 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:32:34,070 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-32-29_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:32:34,075 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:32:34,075 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:32:39,312 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-32-35_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:32:39,315 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:32:39,315 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:32:45,026 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-32-40_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:32:45,030 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:32:45,030 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:32:51,669 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-32-46_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:32:51,672 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:32:51,672 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:32:58,679 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-32-52_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:32:58,682 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:32:58,682 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:33:03,473 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-32-59_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:33:03,476 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:33:03,476 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:33:09,643 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-33-04_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:33:09,645 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:33:09,645 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:33:16,052 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-33-10_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:33:16,056 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:33:16,056 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:33:24,259 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-33-17_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:33:24,262 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:33:24,262 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:33:30,904 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-33-25_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:33:30,908 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:33:30,908 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:33:38,148 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-33-32_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:33:38,152 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:33:38,152 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:33:42,544 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-33-39_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:33:42,547 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:33:42,547 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:33:48,583 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-33-43_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:33:48,586 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:33:48,586 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:33:52,863 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-33-49_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:33:52,866 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:33:52,866 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:33:59,627 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-33-53_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:33:59,630 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:33:59,630 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:34:03,570 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-34-00_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:34:03,573 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:34:03,573 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:34:09,557 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-34-04_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:34:09,560 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:34:09,560 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:34:13,903 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-34-10_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:34:13,907 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:34:13,907 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:34:19,641 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-34-15_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:34:19,645 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:34:19,645 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:34:24,796 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-34-20_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:34:24,799 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:34:24,799 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:34:30,504 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-34-25_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:34:30,509 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:34:30,509 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:34:35,126 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-34-31_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:34:35,129 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:34:35,129 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:34:42,100 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-34-36_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:34:42,103 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:34:42,103 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:34:48,273 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-34-43_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:34:48,276 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:34:48,276 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:34:52,583 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-34-49_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:34:52,587 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:34:52,587 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:34:57,968 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-34-53_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:34:57,970 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:34:57,970 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:35:04,517 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-34-59_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:35:04,522 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:35:04,522 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:35:12,302 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-35-05_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:35:12,307 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:35:12,307 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:35:19,163 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-35-13_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:35:19,167 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:35:19,167 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:35:23,545 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-35-20_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:35:23,548 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:35:23,548 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:35:30,010 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-35-24_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:35:30,013 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:35:30,013 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:35:33,549 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-35-31_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:35:33,551 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:35:33,551 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:35:38,549 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-35-34_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:35:38,552 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:35:38,552 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:35:42,265 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-35-39_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:35:42,268 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:35:42,268 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:35:46,271 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-35-43_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:35:46,275 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:35:46,275 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:35:52,874 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-35-47_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:35:52,877 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:35:52,877 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:35:58,885 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-35-54_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:35:58,888 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:35:58,888 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:36:06,848 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-36-00_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:36:06,853 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:36:06,853 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:36:13,106 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-36-08_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:36:13,109 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:36:13,109 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:36:21,336 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-36-14_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:36:21,339 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:36:21,339 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:36:27,267 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-36-22_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:36:27,271 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:36:27,271 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:36:32,498 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-36-28_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:36:32,502 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:36:32,502 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:36:37,344 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-36-33_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:36:37,347 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:36:37,347 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:36:43,100 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-36-38_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:36:43,104 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:36:43,104 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:36:48,484 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-36-44_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:36:48,487 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:36:48,487 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:36:53,956 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-36-50_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:36:53,959 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:36:53,959 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:37:01,197 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-36-55_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:37:01,200 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:37:01,200 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:37:01,792 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:37:01,792 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:37:01,900 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:37:02,159 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:37:02,160 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:40:14,142 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:40:14,250 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:40:14,250 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:40:14,295 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:40:14,300 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:40:14,300 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:40:14,347 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:40:14,359 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:40:14,360 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:40:14,372 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:40:14,373 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:40:14,373 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:40:14,387 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:40:14,396 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:40:14,396 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:40:14,409 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:40:14,419 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:40:14,419 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:40:14,446 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:40:17,040 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:40:17,040 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:40:17,079 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:40:17,177 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:40:17,177 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:40:17,197 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:40:17,644 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:40:17,644 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:40:17,662 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:40:17,664 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:40:17,665 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:40:17,680 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:40:17,682 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:40:17,682 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:40:17,695 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:40:17,698 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:40:17,699 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:40:17,716 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:40:17,721 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:40:17,721 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:40:17,756 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:40:19,778 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-40-18_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:40:19,781 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:40:19,781 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:40:22,660 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-40-20_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:40:22,664 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:40:22,664 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:40:27,300 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-40-23_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:40:27,302 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:40:27,302 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:40:31,450 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-40-28_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:40:31,453 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:40:31,453 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:40:35,306 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-40-32_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:40:35,310 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:40:35,311 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:40:39,488 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-40-36_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:40:39,491 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:40:39,491 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:40:40,890 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:40:40,890 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:41:02,670 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:41:02,754 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:41:02,754 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:41:02,784 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:41:02,786 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:41:02,786 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:41:02,797 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:41:02,809 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:41:02,809 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:41:02,820 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:41:02,821 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:41:02,821 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:41:02,833 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:41:02,844 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:41:02,845 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:41:02,853 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:41:02,861 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:41:02,862 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:41:02,877 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:41:05,288 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:41:05,288 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:41:05,314 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:41:05,404 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:41:05,404 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:41:05,416 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:41:05,983 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:41:05,983 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:41:06,017 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:41:06,019 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:41:06,020 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:41:06,040 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:41:06,042 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:41:06,042 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:41:06,061 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:41:06,065 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:41:06,065 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:41:06,086 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:41:06,093 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:41:06,093 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:41:06,125 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:41:07,043 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-41-06_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:41:07,047 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:41:07,047 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:41:07,156 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:41:07,156 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:41:19,255 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:41:20,284 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-41-20_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:41:20,288 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:41:20,288 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:41:22,926 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-41-22_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:41:22,928 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:41:22,928 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:41:28,938 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-41-26_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:41:28,941 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:41:28,941 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:41:37,114 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-41-35_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:41:37,117 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:41:37,117 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:41:40,466 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-41-38_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:41:40,470 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:41:40,470 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:41:43,834 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-41-42_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:41:43,837 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:41:43,837 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:41:51,200 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-41-47_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:41:51,203 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:41:51,203 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:41:58,433 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-41-55_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:41:58,437 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:41:58,438 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:42:06,331 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-42-05_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:42:06,334 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:42:06,334 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:42:12,923 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-42-09_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:42:12,926 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:42:12,926 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:42:15,726 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-42-14_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:42:15,731 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:42:15,731 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:42:22,216 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-42-17_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:42:22,219 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:42:22,219 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:42:27,920 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-42-23_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:42:27,923 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:42:27,923 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:42:29,905 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-42-29_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:42:29,908 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:42:29,908 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:42:32,668 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-42-31_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:42:32,670 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:42:32,670 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:42:36,123 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-42-34_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:42:36,129 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:42:36,129 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:42:40,931 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-42-37_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:42:40,933 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:42:40,933 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:42:45,218 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-42-42_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:42:45,222 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:42:45,222 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:42:50,117 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-42-47_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:42:50,120 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:42:50,120 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:42:55,376 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-42-51_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:42:55,379 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:42:55,379 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:43:00,293 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-42-56_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:43:00,297 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:43:00,297 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:43:02,587 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-43-02_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:43:02,591 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:43:02,591 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:43:06,754 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-43-04_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:43:06,757 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:43:06,757 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:43:12,004 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-43-08_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:43:12,007 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:43:12,007 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:43:13,842 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-43-13_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:43:13,845 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:43:13,845 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:43:16,433 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-43-15_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:43:16,436 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:43:16,436 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:43:20,702 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-43-18_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:43:20,705 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:43:20,705 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:43:25,188 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-43-22_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:43:25,193 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:43:25,193 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:43:29,844 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-43-26_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:43:29,846 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:43:29,846 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:43:33,946 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-43-31_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:43:33,949 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:43:33,949 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:43:39,413 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-43-35_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:43:39,417 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:43:39,417 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:43:43,547 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-43-40_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:43:43,550 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:43:43,551 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:43:45,568 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-43-45_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:43:45,571 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:43:45,571 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:43:51,014 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-43-47_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:43:51,017 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:43:51,017 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:43:56,410 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-43-52_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:43:56,412 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:43:56,412 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:44:00,253 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-43-57_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:44:00,257 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:44:00,257 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:44:03,982 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-44-01_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:44:03,985 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:44:03,985 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:44:07,741 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-44-05_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:44:07,745 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:44:07,745 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:44:09,407 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-44-09_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:44:09,410 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:44:09,410 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:44:13,975 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-44-11_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:44:13,978 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:44:13,978 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:44:17,715 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-44-15_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:44:17,719 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:44:17,719 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:44:22,092 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-44-19_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:44:22,096 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:44:22,096 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:44:26,949 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-44-23_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:44:26,952 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:44:26,952 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:44:29,018 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-44-28_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:44:29,022 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:44:29,022 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:44:33,399 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-44-30_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:44:33,402 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:44:33,402 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:44:38,519 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-44-34_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:44:38,522 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:44:38,522 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:44:41,101 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-44-40_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:44:41,105 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:44:41,105 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:44:44,155 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-44-42_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:44:44,158 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:44:44,158 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:44:52,357 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-44-46_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:44:52,360 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:44:52,360 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:44:55,054 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-44-54_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:44:55,057 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:44:55,057 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:44:59,652 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-44-56_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:44:59,655 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:44:59,655 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:45:05,089 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-45-01_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:45:05,092 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:45:05,092 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:45:10,330 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-45-07_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:45:10,332 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:45:10,332 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:45:14,264 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-45-12_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:45:14,268 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:45:14,268 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:45:18,175 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-45-16_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:45:18,178 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:45:18,178 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:45:22,282 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-45-19_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:45:22,285 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:45:22,285 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:45:26,842 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-45-23_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:45:26,844 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:45:26,844 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:45:31,726 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-45-28_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:45:31,728 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:45:31,728 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:45:35,656 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-45-34_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:45:35,659 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:45:35,659 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:45:40,181 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-45-37_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:45:40,185 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:45:40,186 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:45:44,798 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-45-41_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:45:44,801 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:45:44,801 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:45:51,387 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-45-46_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:45:51,390 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:45:51,390 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:45:55,953 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-45-53_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:45:55,956 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:45:55,956 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:46:00,772 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-45-57_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:46:00,775 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:46:00,775 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:46:03,335 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-46-02_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:46:03,338 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:46:03,338 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:46:08,827 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-46-05_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:46:08,831 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:46:08,831 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:46:11,324 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-46-10_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:46:11,327 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:46:11,327 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:46:16,873 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-46-13_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:46:16,876 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:46:16,876 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:46:19,019 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-46-18_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:46:19,022 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:46:19,022 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:46:23,524 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-46-20_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:46:23,526 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:46:23,526 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:46:26,233 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-46-25_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:46:26,236 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:46:26,236 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:46:30,393 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-46-28_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:46:30,396 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:46:30,396 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:46:33,935 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-46-32_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:46:33,937 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:46:33,937 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:46:38,408 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-46-35_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:46:38,411 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:46:38,411 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:46:41,420 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-46-40_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:46:41,424 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:46:41,424 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:46:47,046 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-46-43_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:46:47,050 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:46:47,050 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:46:52,260 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-46-49_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:46:52,263 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:46:52,263 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:46:54,880 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-46-53_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:46:54,883 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:46:54,883 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:46:58,081 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-46-56_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:46:58,085 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:46:58,085 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:47:01,984 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-46-59_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:47:01,987 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:47:01,987 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:47:07,568 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-47-03_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:47:07,571 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:47:07,572 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:47:12,143 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-47-09_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:47:12,146 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:47:12,146 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:47:14,656 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-47-13_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:47:14,659 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:47:14,659 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:47:19,453 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-47-16_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:47:19,457 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:47:19,457 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:47:21,405 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-47-21_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:47:21,409 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:47:21,409 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:47:24,854 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-47-23_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:47:24,858 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:47:24,858 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:47:27,150 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-47-26_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:47:27,153 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:47:27,153 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:47:29,818 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-47-28_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:47:29,821 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:47:29,821 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:47:34,906 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-47-31_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:47:34,910 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:47:34,911 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:47:39,220 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-47-36_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:47:39,223 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:47:39,223 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:47:45,539 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-47-40_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:47:45,542 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:47:45,542 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:47:50,131 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-47-47_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:47:50,135 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:47:50,135 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:47:56,887 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-47-51_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:47:56,891 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:47:56,891 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:48:01,517 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-47-58_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:48:01,520 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:48:01,520 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:48:04,857 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-48-03_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:48:04,860 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:48:04,860 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:48:08,630 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-48-07_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:48:08,634 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:48:08,634 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:48:12,612 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-48-10_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:48:12,615 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:48:12,615 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:48:16,311 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-48-14_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:48:16,314 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:48:16,314 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:48:19,546 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-48-18_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:48:19,550 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:48:19,550 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:48:25,048 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 96, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov24_22-48-21_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-24 22:48:25,051 INFO    MainThread:29056 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001FB3E158210>>
2024-11-24 22:48:25,051 INFO    MainThread:29056 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-24 22:48:26,161 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:48:26,161 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:48:26,167 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:48:26,173 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:48:26,173 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:48:26,183 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:48:26,189 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:48:26,189 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:48:26,199 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:48:26,215 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:48:26,215 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:48:26,243 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:48:26,275 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:48:26,275 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:48:26,319 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:48:26,329 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:48:26,329 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-24 22:51:27,669 INFO    MainThread:29056 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-24 22:51:27,886 INFO    MainThread:29056 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-24 22:51:27,886 INFO    MainThread:29056 [wandb_init.py:_pause_backend():444] pausing backend
