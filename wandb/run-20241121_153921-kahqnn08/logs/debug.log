2024-11-21 15:39:21,007 INFO    MainThread:26724 [wandb_setup.py:_flush():79] Current SDK version is 0.18.6
2024-11-21 15:39:21,007 INFO    MainThread:26724 [wandb_setup.py:_flush():79] Configure stats pid to 26724
2024-11-21 15:39:21,007 INFO    MainThread:26724 [wandb_setup.py:_flush():79] Loading settings from C:\Users\e_nkanungo\.config\wandb\settings
2024-11-21 15:39:21,007 INFO    MainThread:26724 [wandb_setup.py:_flush():79] Loading settings from c:\Users\e_nkanungo\Desktop\MADS_Capstone\RUL_Prediction\wandb\settings
2024-11-21 15:39:21,007 INFO    MainThread:26724 [wandb_setup.py:_flush():79] Loading settings from environment variables: {}
2024-11-21 15:39:21,007 INFO    MainThread:26724 [wandb_setup.py:_flush():79] Applying setup settings: {'mode': None, '_disable_service': None}
2024-11-21 15:39:21,007 INFO    MainThread:26724 [wandb_setup.py:_flush():79] Inferring run settings from compute environment: {'program': '<python with no main file>'}
2024-11-21 15:39:21,007 INFO    MainThread:26724 [wandb_setup.py:_flush():79] Applying login settings: {}
2024-11-21 15:39:21,008 INFO    MainThread:26724 [wandb_init.py:_log_setup():533] Logging user logs to c:\Users\e_nkanungo\Desktop\MADS_Capstone\RUL_Prediction\wandb\run-20241121_153921-kahqnn08\logs\debug.log
2024-11-21 15:39:21,008 INFO    MainThread:26724 [wandb_init.py:_log_setup():534] Logging internal logs to c:\Users\e_nkanungo\Desktop\MADS_Capstone\RUL_Prediction\wandb\run-20241121_153921-kahqnn08\logs\debug-internal.log
2024-11-21 15:39:21,008 INFO    MainThread:26724 [wandb_init.py:_jupyter_setup():479] configuring jupyter hooks <wandb.sdk.wandb_init._WandbInit object at 0x000001C8F5D71BD0>
2024-11-21 15:39:21,008 INFO    MainThread:26724 [wandb_init.py:init():619] calling init triggers
2024-11-21 15:39:21,008 INFO    MainThread:26724 [wandb_init.py:init():626] wandb.init called with sweep_config: {}
config: {}
2024-11-21 15:39:21,008 INFO    MainThread:26724 [wandb_init.py:init():669] starting backend
2024-11-21 15:39:21,008 INFO    MainThread:26724 [wandb_init.py:init():673] sending inform_init request
2024-11-21 15:39:21,011 INFO    MainThread:26724 [backend.py:_multiprocessing_setup():104] multiprocessing start_methods=spawn, using: spawn
2024-11-21 15:39:21,012 INFO    MainThread:26724 [wandb_init.py:init():686] backend started and connected
2024-11-21 15:39:21,020 INFO    MainThread:26724 [wandb_run.py:_label_probe_notebook():1341] probe notebook
2024-11-21 15:39:21,039 INFO    MainThread:26724 [wandb_run.py:_label_probe_notebook():1351] Unable to probe notebook: 'charmap' codec can't decode byte 0x81 in position 4688: character maps to <undefined>
2024-11-21 15:39:21,039 INFO    MainThread:26724 [wandb_init.py:init():781] updated telemetry
2024-11-21 15:39:21,588 INFO    MainThread:26724 [wandb_init.py:init():814] communicating run to backend with 90.0 second timeout
2024-11-21 15:39:23,070 INFO    MainThread:26724 [wandb_init.py:init():867] starting run threads in backend
2024-11-21 15:39:23,355 INFO    MainThread:26724 [wandb_run.py:_console_start():2451] atexit reg
2024-11-21 15:39:23,355 INFO    MainThread:26724 [wandb_run.py:_redirect():2299] redirect: wrap_raw
2024-11-21 15:39:23,356 INFO    MainThread:26724 [wandb_run.py:_redirect():2364] Wrapping output streams.
2024-11-21 15:39:23,356 INFO    MainThread:26724 [wandb_run.py:_redirect():2389] Redirects installed.
2024-11-21 15:39:23,360 INFO    MainThread:26724 [wandb_init.py:init():911] run started, returning control to user process
2024-11-21 15:39:23,364 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov21_15-39-14_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-21 15:39:23,367 INFO    MainThread:26724 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001C8F65066D0>>
2024-11-21 15:39:23,367 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-21 15:39:24,582 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 15:39:24,582 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 15:40:48,264 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 15:40:48,375 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 15:40:48,376 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 15:40:48,392 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 15:40:48,395 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 15:40:48,395 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 15:40:48,416 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 15:40:48,493 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 15:40:48,493 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 15:41:35,748 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 15:41:41,048 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 15:41:41,048 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 15:41:41,080 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 15:41:41,083 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 15:41:41,083 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 15:41:41,113 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 15:41:41,140 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 15:41:41,140 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 15:42:30,500 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 15:42:35,900 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 15:42:35,900 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 15:42:35,944 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 15:42:35,946 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 15:42:35,947 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 15:42:35,997 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 15:42:36,040 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 15:42:36,040 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 15:42:43,669 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 15:42:43,782 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 15:42:43,782 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 15:42:43,810 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 15:42:43,813 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 15:42:43,813 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 15:42:43,844 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 15:42:43,853 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 15:42:43,853 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 15:42:43,911 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 15:42:43,918 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 15:42:43,918 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 15:42:43,958 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 15:42:43,962 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 15:42:43,962 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 15:42:43,983 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 15:42:44,665 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 15:42:44,665 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 15:42:44,677 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 15:42:44,962 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 15:42:44,962 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 15:42:45,015 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 15:42:45,018 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 15:42:45,018 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 15:42:45,044 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 15:42:45,047 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 15:42:45,047 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 15:42:45,067 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 15:42:45,111 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 15:42:45,111 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 15:42:45,129 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 15:42:45,894 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov21_15-42-45_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-21 15:42:45,898 INFO    MainThread:26724 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001C8F65066D0>>
2024-11-21 15:42:45,898 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-21 15:42:46,649 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 15:42:46,649 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 15:43:30,666 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 15:43:30,769 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 15:43:30,770 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 15:43:30,793 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 15:43:30,796 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 15:43:30,796 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 15:43:30,810 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 15:43:30,817 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 15:43:30,817 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 15:43:30,829 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 15:43:30,840 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 15:43:30,840 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 15:43:30,856 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 15:43:30,859 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 15:43:30,859 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 15:43:30,874 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 15:43:31,609 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 15:43:31,609 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 15:43:31,621 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 15:43:31,906 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 15:43:31,906 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 15:43:31,959 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 15:43:31,962 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 15:43:31,962 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 15:43:32,025 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 15:43:32,028 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 15:43:32,028 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 15:43:32,057 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 15:43:32,066 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 15:43:32,066 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 15:43:32,085 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 15:43:32,798 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov21_15-43-32_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-21 15:43:32,802 INFO    MainThread:26724 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001C8F65066D0>>
2024-11-21 15:43:32,803 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-21 15:43:33,612 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 15:43:33,612 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 15:45:16,916 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 15:45:16,920 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 15:45:16,920 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 15:45:19,574 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 15:45:19,578 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 15:45:19,578 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 15:45:19,911 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 15:45:19,914 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 15:45:19,915 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 15:45:20,006 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 15:45:20,010 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 15:45:20,010 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 15:45:25,160 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 15:45:25,165 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 15:45:25,165 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 15:45:25,912 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 15:45:25,917 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 15:45:25,917 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 15:45:26,168 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 15:45:26,174 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 15:45:26,174 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 15:45:30,318 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 15:45:30,323 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 15:45:30,323 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 15:45:31,540 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 15:45:31,544 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 15:45:31,544 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 15:45:31,716 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 15:45:31,720 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 15:45:31,720 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 15:45:31,820 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 15:45:31,824 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 15:45:31,824 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 15:45:35,529 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 15:45:35,665 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 15:45:35,665 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 15:45:35,685 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 15:45:35,687 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 15:45:35,687 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 15:45:35,721 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 15:45:35,729 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 15:45:35,729 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 15:45:35,760 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 15:45:35,766 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 15:45:35,766 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 15:45:35,792 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 15:45:35,797 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 15:45:35,797 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 15:45:35,854 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 15:45:36,451 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 15:45:36,451 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 15:45:36,518 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 15:45:36,782 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 15:45:36,782 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 15:45:36,801 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 15:45:36,803 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 15:45:36,803 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 15:45:36,823 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 15:45:36,825 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 15:45:36,825 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 15:45:36,843 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 15:45:36,849 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 15:45:36,849 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 15:45:36,880 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 15:45:37,016 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 15:45:37,016 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 15:46:11,126 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 15:46:11,216 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 15:46:11,217 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 15:46:11,236 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 15:46:11,238 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 15:46:11,238 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 15:46:11,257 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 15:46:11,266 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 15:46:11,266 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 15:46:11,277 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 15:46:11,285 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 15:46:11,285 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 15:46:11,298 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 15:46:11,301 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 15:46:11,301 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 15:46:11,327 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 15:46:11,891 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 15:46:11,891 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 15:46:11,903 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 15:46:12,138 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 15:46:12,138 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 15:46:12,152 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 15:46:12,155 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 15:46:12,155 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 15:46:12,167 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 15:46:12,169 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 15:46:12,169 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 15:46:12,184 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 15:46:12,189 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 15:46:12,189 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 15:46:12,204 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 15:46:12,280 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 15:46:12,280 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 15:46:24,449 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 15:46:24,532 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 15:46:24,532 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 15:46:24,547 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 15:46:24,548 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 15:46:24,548 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 15:46:24,588 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 15:46:24,597 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 15:46:24,598 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 15:46:24,617 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 15:46:24,623 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 15:46:24,623 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 15:46:24,664 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 15:46:24,668 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 15:46:24,668 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 15:46:24,680 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 15:46:25,241 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 15:46:25,241 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 15:46:25,261 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 15:46:25,877 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 15:46:25,877 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 15:46:25,900 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 15:46:25,903 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 15:46:25,903 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 15:46:25,918 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 15:46:25,919 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 15:46:25,919 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 15:46:25,933 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 15:46:25,939 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 15:46:25,939 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 15:46:25,964 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 15:46:26,070 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 15:46:26,070 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 16:06:04,642 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 16:06:04,815 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 16:06:04,815 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 16:06:04,839 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 16:06:04,842 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 16:06:04,842 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 16:06:04,862 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 16:06:04,886 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 16:06:04,886 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 16:06:04,898 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 16:06:04,902 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 16:06:04,902 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 16:06:04,925 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 16:06:04,929 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 16:06:04,929 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 16:06:04,964 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 16:06:05,539 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 16:06:05,539 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 16:06:05,564 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 16:06:05,798 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 16:06:05,798 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 16:06:05,809 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 16:06:05,812 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 16:06:05,812 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 16:06:05,827 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 16:06:05,830 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 16:06:05,830 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 16:06:05,842 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 16:06:05,849 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 16:06:05,849 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 16:06:05,865 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 16:06:05,939 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 16:06:05,939 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 16:14:21,018 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 16:14:21,067 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 16:14:21,067 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 16:14:21,122 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 16:14:21,139 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 16:14:21,142 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 16:14:21,208 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 16:14:21,221 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 16:14:21,229 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 16:14:21,284 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 16:14:21,299 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 16:14:21,299 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 16:14:21,357 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 16:14:21,377 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 16:14:21,377 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 16:14:21,427 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 16:14:21,441 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 16:14:21,441 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 16:14:21,503 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 16:14:21,522 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 16:14:21,523 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 16:14:21,576 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 16:14:21,591 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 16:14:21,592 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 16:14:21,649 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 16:14:21,669 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 16:14:21,670 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 16:14:53,297 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 16:14:53,305 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 16:14:53,305 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 16:15:19,874 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 16:15:21,095 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 16:15:21,095 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 16:16:00,326 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 16:16:00,331 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 16:16:00,331 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 16:16:00,362 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 16:16:00,368 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 16:16:00,368 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 16:16:00,586 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 16:16:00,606 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 16:16:00,606 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 19:49:33,710 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 19:49:33,740 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 19:49:33,742 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 19:49:33,770 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 19:49:33,889 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 19:49:33,895 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 19:49:33,905 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 19:49:33,912 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 19:49:33,912 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 19:49:35,431 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 19:49:35,435 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 19:49:35,435 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 19:49:35,468 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 19:49:35,476 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 19:49:35,477 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 19:49:35,496 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 19:49:35,504 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 19:49:35,505 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 19:49:35,655 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 19:49:35,662 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 19:49:35,663 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 19:49:36,430 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 19:49:36,438 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 19:49:36,439 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 19:49:39,233 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 19:49:39,672 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 19:49:39,672 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 19:49:39,729 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 19:49:39,735 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 19:49:39,735 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 19:49:39,814 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 19:49:39,866 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 19:49:39,866 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 19:49:39,890 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 19:49:39,921 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 19:49:39,922 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 19:49:39,982 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 19:49:40,001 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 19:49:40,001 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 19:49:40,027 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 19:49:40,208 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 19:49:40,208 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 22:32:47,245 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 22:32:47,250 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 22:32:47,250 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 22:32:47,817 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 22:32:47,824 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 22:32:47,824 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 22:32:48,009 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 22:32:48,015 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 22:32:48,015 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 22:32:48,309 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 22:32:48,314 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 22:32:48,314 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 22:32:48,550 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 22:32:48,556 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 22:32:48,556 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 22:32:51,837 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 22:32:51,846 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 22:32:51,846 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 22:32:52,112 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 22:32:52,116 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 22:32:52,116 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 22:32:52,220 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 22:32:52,225 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 22:32:52,225 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 22:32:52,609 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 22:32:52,614 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 22:32:52,614 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 22:32:52,844 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 22:32:52,850 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 22:32:52,850 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 22:32:52,934 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 22:32:52,938 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 22:32:52,938 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 22:33:02,353 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 22:33:02,448 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 22:33:02,448 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 22:33:02,472 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 22:33:02,473 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 22:33:02,473 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 22:33:02,492 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 22:33:02,504 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 22:33:02,504 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 22:33:02,519 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 22:33:02,532 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 22:33:02,532 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 22:33:02,553 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 22:33:02,568 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 22:33:02,568 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 22:33:02,592 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 22:33:02,728 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 22:33:02,729 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 22:33:53,522 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 22:33:53,615 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 22:33:53,615 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 22:33:53,633 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 22:33:53,635 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 22:33:53,635 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 22:33:53,658 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 22:33:53,663 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 22:33:53,663 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 22:33:53,699 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 22:33:53,712 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 22:33:53,712 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 22:33:53,778 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 22:33:53,787 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 22:33:53,787 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 22:33:53,817 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 22:33:54,644 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 22:33:54,644 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 22:33:54,672 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 22:33:55,906 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 22:33:55,906 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 22:33:55,931 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 22:33:55,934 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 22:33:55,934 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 22:33:55,949 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 22:33:55,951 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 22:33:55,951 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 22:33:55,978 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 22:33:55,991 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 22:33:55,992 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 22:33:56,012 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 22:33:57,160 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov21_22-33-57_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-21 22:33:57,167 INFO    MainThread:26724 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001C8F65066D0>>
2024-11-21 22:33:57,167 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-21 22:33:57,977 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 22:33:57,977 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 22:34:17,737 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 22:34:17,831 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 22:34:17,831 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 22:34:17,846 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 22:34:17,849 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 22:34:17,849 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 22:34:17,866 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 22:34:17,871 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 22:34:17,871 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 22:34:17,895 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 22:34:17,909 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 22:34:17,909 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 22:34:17,954 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 22:34:18,743 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 22:34:18,744 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 22:34:18,773 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 22:34:18,824 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 22:34:18,824 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 22:34:18,864 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 22:34:18,955 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 22:34:18,955 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 22:34:39,550 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 22:34:39,634 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 22:34:39,634 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 22:34:39,650 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 22:34:39,652 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 22:34:39,652 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 22:34:39,694 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 22:34:39,700 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 22:34:39,700 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 22:34:39,757 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 22:34:39,767 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 22:34:39,767 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 22:34:39,781 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 22:34:40,617 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 22:34:40,617 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 22:34:40,649 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 22:34:40,679 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 22:34:40,680 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 22:34:40,712 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 22:34:41,008 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 22:34:41,008 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 22:34:41,097 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 22:34:41,100 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 22:34:41,100 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 22:34:41,159 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 22:34:41,160 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 22:34:41,160 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 22:34:41,186 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 22:34:41,191 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 22:34:41,191 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 22:34:41,210 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 22:34:41,949 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov21_22-34-41_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-21 22:34:41,954 INFO    MainThread:26724 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001C8F65066D0>>
2024-11-21 22:34:41,954 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-21 22:34:42,743 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 22:34:42,744 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 22:35:21,970 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 22:35:22,105 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 22:35:22,106 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 22:35:22,125 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 22:35:22,128 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 22:35:22,128 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 22:35:22,148 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 22:35:22,155 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 22:35:22,155 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 22:35:22,178 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 22:35:22,189 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 22:35:22,189 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 22:35:22,207 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 22:35:22,962 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 22:35:22,962 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 22:35:22,973 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 22:35:22,999 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 22:35:23,000 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 22:35:23,017 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 22:35:23,289 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 22:35:23,289 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 22:35:23,307 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 22:35:23,309 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 22:35:23,309 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 22:35:23,323 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 22:35:23,325 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 22:35:23,325 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 22:35:23,342 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 22:35:23,348 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 22:35:23,349 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 22:35:23,392 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 22:35:24,146 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov21_22-35-24_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-21 22:35:24,150 INFO    MainThread:26724 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001C8F65066D0>>
2024-11-21 22:35:24,150 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-21 22:35:25,076 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 22:35:25,076 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 23:05:59,804 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 23:05:59,810 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 23:05:59,811 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 23:05:59,844 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 23:05:59,849 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 23:05:59,850 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 23:06:00,064 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 23:06:00,072 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 23:06:00,073 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 23:06:01,983 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 23:06:01,997 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 23:06:01,997 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 23:06:02,046 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 23:06:02,052 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 23:06:02,052 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 23:06:03,948 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 23:06:03,954 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 23:06:03,954 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 23:06:03,985 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 23:06:03,990 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 23:06:03,990 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 23:07:14,689 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 23:07:15,048 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 23:07:15,048 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 23:07:15,065 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 23:07:15,068 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 23:07:15,068 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 23:07:15,098 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 23:07:15,142 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 23:07:15,142 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 23:07:15,163 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 23:07:15,174 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 23:07:15,174 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 23:07:15,218 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 23:07:15,903 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 23:07:15,903 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 23:07:15,913 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 23:07:15,938 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 23:07:15,938 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 23:07:15,952 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 23:07:16,205 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 23:07:16,205 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 23:07:16,228 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 23:07:16,230 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 23:07:16,230 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 23:07:16,248 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 23:07:16,251 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 23:07:16,251 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 23:07:16,264 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 23:07:16,270 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 23:07:16,270 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 23:07:16,289 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 23:07:17,273 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov21_23-07-17_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-21 23:07:17,276 INFO    MainThread:26724 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001C8F65066D0>>
2024-11-21 23:07:17,276 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-21 23:07:18,045 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 23:07:18,046 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 23:08:33,135 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 23:08:33,207 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 23:08:33,208 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 23:08:33,232 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 23:08:33,233 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 23:08:33,233 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 23:08:33,252 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 23:08:33,259 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 23:08:33,259 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 23:08:33,274 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 23:08:33,284 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 23:08:33,284 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 23:08:33,302 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 23:08:33,984 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 23:08:33,984 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 23:08:33,995 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 23:08:34,018 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 23:08:34,018 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 23:08:34,033 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 23:08:34,319 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 23:08:34,319 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 23:08:34,335 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 23:08:34,336 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 23:08:34,336 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 23:08:34,350 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 23:08:34,352 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 23:08:34,352 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 23:08:34,365 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 23:08:34,370 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 23:08:34,370 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 23:08:34,383 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 23:08:35,196 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov21_23-08-35_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-21 23:08:35,200 INFO    MainThread:26724 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001C8F65066D0>>
2024-11-21 23:08:35,200 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-21 23:08:35,994 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 23:08:35,994 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 23:11:26,702 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 23:11:26,811 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 23:11:26,812 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 23:11:26,831 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 23:11:26,834 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 23:11:26,835 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 23:11:26,857 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 23:11:26,863 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 23:11:26,863 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 23:11:26,880 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 23:11:26,889 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 23:11:26,889 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 23:11:26,905 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 23:11:27,569 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 23:11:27,569 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 23:11:27,579 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 23:11:27,600 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 23:11:27,600 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 23:11:27,610 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 23:11:27,855 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 23:11:27,855 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 23:11:27,870 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 23:11:27,871 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 23:11:27,871 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 23:11:27,888 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 23:11:27,889 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 23:11:27,889 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 23:11:27,903 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 23:11:27,909 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 23:11:27,909 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 23:11:27,937 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 23:11:28,648 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov21_23-11-28_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-21 23:11:28,653 INFO    MainThread:26724 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001C8F65066D0>>
2024-11-21 23:11:28,654 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-21 23:11:29,395 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 23:11:29,395 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 23:21:05,496 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 23:21:05,582 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 23:21:05,582 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 23:21:05,604 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 23:21:05,607 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 23:21:05,607 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 23:21:05,629 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 23:21:05,638 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 23:21:05,638 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 23:21:05,665 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 23:21:05,677 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 23:21:05,678 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 23:21:05,706 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 23:21:06,438 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 23:21:06,438 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 23:21:06,451 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 23:21:06,473 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 23:21:06,473 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 23:21:06,483 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 23:21:06,737 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 23:21:06,737 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 23:21:06,757 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 23:21:06,760 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 23:21:06,761 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 23:21:06,775 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 23:21:06,777 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 23:21:06,777 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 23:21:06,791 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 23:21:06,797 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 23:21:06,797 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 23:21:06,814 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 23:21:06,869 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 23:21:06,870 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 23:21:21,673 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 23:21:21,679 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 23:21:21,679 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 23:21:21,710 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 23:21:21,716 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 23:21:21,716 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 23:21:53,793 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 23:21:53,800 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 23:21:53,800 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 23:21:53,827 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 23:21:53,834 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 23:21:53,834 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 23:21:56,099 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 23:21:56,104 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 23:21:56,104 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 23:22:01,513 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 23:22:01,614 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 23:22:01,614 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 23:22:01,630 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 23:22:01,633 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 23:22:01,633 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 23:22:01,647 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 23:22:01,673 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 23:22:01,673 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 23:22:01,682 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 23:22:01,693 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 23:22:01,693 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 23:22:01,710 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 23:22:02,391 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 23:22:02,392 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 23:22:02,401 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 23:22:02,432 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 23:22:02,432 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 23:22:02,483 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 23:22:02,756 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 23:22:02,756 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 23:22:02,772 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 23:22:02,775 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 23:22:02,775 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 23:22:02,793 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 23:22:02,795 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 23:22:02,795 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 23:22:02,814 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 23:22:02,820 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 23:22:02,820 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 23:22:02,837 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 23:22:04,313 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov21_23-22-04_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-21 23:22:04,317 INFO    MainThread:26724 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001C8F65066D0>>
2024-11-21 23:22:04,318 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-21 23:22:05,232 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 23:22:05,232 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 23:24:52,031 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 23:24:52,038 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 23:24:52,038 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 23:24:52,071 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 23:24:52,076 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 23:24:52,076 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 23:24:52,467 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 23:24:52,472 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 23:24:52,472 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 23:24:52,732 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 23:24:52,738 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 23:24:52,738 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 23:24:52,871 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 23:24:52,876 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 23:24:52,876 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 23:24:53,112 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 23:24:53,120 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 23:24:53,120 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 23:24:53,407 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 23:24:53,411 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 23:24:53,411 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 23:24:53,689 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 23:24:53,694 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 23:24:53,694 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 23:24:53,781 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 23:24:53,786 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 23:24:53,786 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 23:24:55,019 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 23:24:55,160 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 23:24:55,160 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 23:25:10,509 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 23:25:10,616 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 23:25:10,616 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 23:25:10,630 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 23:25:10,633 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 23:25:10,633 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 23:25:10,649 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 23:25:10,656 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 23:25:10,656 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 23:25:10,681 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 23:25:10,689 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 23:25:10,689 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 23:25:10,703 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 23:25:11,367 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 23:25:11,367 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 23:25:11,377 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 23:25:11,399 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 23:25:11,399 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 23:25:11,410 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 23:25:11,659 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 23:25:11,659 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 23:25:11,678 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 23:25:11,679 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 23:25:11,679 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 23:25:11,693 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 23:25:11,696 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 23:25:11,696 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 23:25:11,720 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 23:25:11,728 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 23:25:11,728 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 23:25:11,752 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 23:25:11,898 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 23:25:11,898 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 23:26:54,513 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 23:26:54,521 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 23:26:54,521 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 23:26:54,543 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 23:26:54,552 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 23:26:54,552 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-21 23:27:05,702 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-21 23:27:06,498 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov21_23-27-06_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-21 23:27:06,502 INFO    MainThread:26724 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001C8F65066D0>>
2024-11-21 23:27:06,502 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-21 23:27:07,344 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-21 23:27:07,344 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:39:24,559 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:39:24,942 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:39:24,942 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:39:24,957 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:39:24,959 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:39:24,959 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:39:24,977 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:39:25,010 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:39:25,010 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:39:25,023 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:39:25,034 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:39:25,034 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:39:25,057 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:39:25,862 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:39:25,862 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:39:25,885 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:39:25,925 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:39:25,925 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:39:25,983 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:39:26,280 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:39:26,281 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:39:26,305 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:39:26,308 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:39:26,308 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:39:26,327 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:39:26,329 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:39:26,329 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:39:26,344 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:39:26,354 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:39:26,354 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:39:26,368 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:39:26,431 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:39:26,432 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:39:32,847 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:39:32,854 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:39:32,855 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:39:32,890 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:39:32,895 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:39:32,896 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:39:33,295 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:39:33,300 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:39:33,300 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:40:13,442 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:40:13,539 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:40:13,541 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:40:13,560 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:40:13,561 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:40:13,561 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:40:13,576 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:40:13,583 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:40:13,583 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:40:13,597 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:40:13,608 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:40:13,609 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:40:13,623 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:40:14,419 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:40:14,419 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:40:14,444 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:40:14,471 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:40:14,471 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:40:14,517 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:40:15,822 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:40:15,822 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:40:15,842 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:40:15,844 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:40:15,844 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:40:15,860 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:40:15,862 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:40:15,862 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:40:15,876 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:40:15,880 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:40:15,881 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:40:15,895 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:40:16,745 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov22_04-40-16_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-22 04:40:16,749 INFO    MainThread:26724 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001C8F65066D0>>
2024-11-22 04:40:16,749 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-22 04:40:17,528 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:40:17,528 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:41:47,242 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:41:47,247 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:41:47,248 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:41:47,271 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:41:47,309 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:41:47,309 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:41:47,320 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:41:47,340 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:41:47,340 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:41:47,502 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:41:47,506 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:41:47,510 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:42:03,757 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:42:03,855 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:42:03,856 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:42:03,880 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:42:03,883 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:42:03,883 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:42:03,896 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:42:03,905 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:42:03,905 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:42:03,958 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:42:03,970 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:42:03,971 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:42:04,005 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:42:04,764 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:42:04,764 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:42:04,778 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:42:04,809 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:42:04,809 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:42:04,827 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:42:05,095 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:42:05,095 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:42:05,115 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:42:05,118 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:42:05,118 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:42:05,159 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:42:05,161 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:42:05,161 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:42:05,221 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:42:05,229 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:42:05,229 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:42:05,318 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:42:06,045 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov22_04-42-05_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-22 04:42:06,048 INFO    MainThread:26724 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001C8F65066D0>>
2024-11-22 04:42:06,048 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-22 04:42:06,858 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:42:06,858 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:42:46,952 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:42:46,989 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:42:47,004 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:42:47,059 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:42:47,078 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:42:47,079 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:42:47,128 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:42:47,139 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:42:47,141 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:42:47,197 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:42:47,215 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:42:47,216 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:42:47,265 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:42:47,277 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:42:47,278 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:42:47,391 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:42:47,403 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:42:47,404 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:42:47,462 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:42:47,476 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:42:47,476 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:42:47,533 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:42:47,547 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:42:47,547 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:42:47,602 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:42:47,615 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:42:47,620 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:42:47,676 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:42:47,689 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:42:47,689 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:42:47,743 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:42:47,760 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:42:47,760 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:43:10,215 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:43:10,301 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:43:10,301 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:43:10,323 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:43:10,326 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:43:10,326 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:43:10,346 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:43:10,353 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:43:10,353 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:43:10,367 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:43:10,381 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:43:10,381 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:43:10,414 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:43:11,178 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:43:11,178 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:43:11,188 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:43:11,216 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:43:11,216 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:43:11,233 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:43:11,492 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:43:11,492 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:43:11,510 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:43:11,512 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:43:11,512 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:43:11,530 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:43:11,533 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:43:11,533 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:43:11,557 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:43:11,566 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:43:11,566 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:43:11,588 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:43:12,305 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov22_04-43-12_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-22 04:43:12,308 INFO    MainThread:26724 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001C8F65066D0>>
2024-11-22 04:43:12,308 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-22 04:43:13,134 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:43:13,134 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:43:46,030 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:43:46,035 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:43:46,035 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:43:46,064 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:43:46,096 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:43:46,096 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:43:46,134 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:43:46,137 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:43:46,139 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:43:47,467 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:43:47,475 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:43:47,478 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:43:50,546 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:43:50,650 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:43:50,651 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:43:50,674 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:43:50,677 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:43:50,677 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:43:50,729 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:43:50,736 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:43:50,736 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:43:50,746 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:43:50,759 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:43:50,759 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:43:50,775 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:43:51,499 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:43:51,499 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:43:51,518 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:43:51,547 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:43:51,547 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:43:51,562 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:43:52,257 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:43:52,257 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:43:52,280 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:43:52,282 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:43:52,282 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:43:52,335 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:43:52,338 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:43:52,338 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:43:52,386 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:43:52,406 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:43:52,406 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:43:52,472 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:43:53,167 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov22_04-43-53_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-22 04:43:53,172 INFO    MainThread:26724 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001C8F65066D0>>
2024-11-22 04:43:53,173 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-22 04:43:53,966 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:43:53,967 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:46:32,187 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:46:32,212 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:46:32,212 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:46:32,297 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:46:32,315 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:46:32,315 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:46:32,382 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:46:32,398 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:46:32,398 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:46:32,469 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:46:32,505 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:46:32,505 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:46:32,554 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:46:32,574 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:46:32,574 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:46:32,626 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:46:32,640 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:46:32,640 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:46:32,696 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:46:32,711 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:46:32,711 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:46:32,774 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:46:32,787 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:46:32,787 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:46:32,847 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:46:32,860 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:46:32,867 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:46:32,920 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:46:32,934 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:46:32,935 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:46:32,987 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:46:33,002 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:46:33,004 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:46:54,296 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:46:54,683 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:46:54,683 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:46:54,711 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:46:54,714 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:46:54,714 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:46:54,741 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:46:54,766 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:46:54,767 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:46:54,783 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:46:54,793 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:46:54,793 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:46:54,813 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:46:55,504 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:46:55,504 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:46:55,516 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:46:55,537 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:46:55,537 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:46:55,569 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:46:55,840 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:46:55,840 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:46:55,855 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:46:55,858 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:46:55,858 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:46:55,871 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:46:55,873 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:46:55,873 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:46:55,886 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:46:55,905 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:46:55,905 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:46:55,930 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:46:56,095 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:46:56,095 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:47:16,979 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:47:16,988 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:47:16,988 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:47:17,022 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:47:17,026 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:47:17,026 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:47:17,287 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:47:17,294 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:47:17,294 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:47:17,380 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:47:17,408 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:47:17,408 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:47:22,533 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:47:22,620 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:47:22,620 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:47:22,637 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:47:22,638 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:47:22,638 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:47:22,665 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:47:22,671 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:47:22,671 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:47:22,682 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:47:22,692 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:47:22,692 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:47:22,728 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:47:23,453 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:47:23,453 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:47:23,464 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:47:23,486 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:47:23,486 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:47:23,496 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:47:23,751 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:47:23,751 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:47:23,766 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:47:23,768 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:47:23,768 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:47:23,781 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:47:23,782 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:47:23,782 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:47:23,796 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:47:23,817 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:47:23,817 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:47:23,862 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:47:24,634 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov22_04-47-24_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-22 04:47:24,637 INFO    MainThread:26724 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001C8F65066D0>>
2024-11-22 04:47:24,637 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-22 04:47:25,426 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:47:25,426 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:53:00,606 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:53:00,688 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:53:00,688 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:53:00,707 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:53:00,709 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:53:00,709 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:53:00,725 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:53:00,731 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:53:00,731 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:53:00,743 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:53:00,753 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:53:00,753 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:53:00,770 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:53:01,509 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:53:01,509 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:53:01,519 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:53:01,543 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:53:01,543 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:53:01,588 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:53:01,871 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:53:01,871 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:53:01,905 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:53:01,908 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:53:01,909 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:53:01,935 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:53:01,937 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:53:01,937 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:53:01,960 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:53:01,980 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:53:01,980 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:53:02,012 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:53:03,203 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov22_04-53-03_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-22 04:53:03,205 INFO    MainThread:26724 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001C8F65066D0>>
2024-11-22 04:53:03,205 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-22 04:53:03,924 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:53:03,924 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:53:24,499 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:53:24,506 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:53:24,507 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:53:24,530 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:53:24,535 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:53:24,536 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:53:27,154 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:53:27,252 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:53:27,252 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:53:27,269 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:53:27,272 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:53:27,272 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:53:27,294 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:53:27,299 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:53:27,299 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:53:27,311 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:53:27,321 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:53:27,321 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:53:27,345 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:53:28,107 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:53:28,107 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:53:28,118 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:53:28,141 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:53:28,141 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:53:28,153 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:53:28,407 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:53:28,407 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:53:28,434 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:53:28,437 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:53:28,439 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:53:28,538 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:53:28,540 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:53:28,540 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:53:28,570 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:53:28,590 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:53:28,590 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:53:28,630 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:53:29,338 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov22_04-53-29_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-22 04:53:29,342 INFO    MainThread:26724 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001C8F65066D0>>
2024-11-22 04:53:29,343 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-22 04:53:30,097 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:53:30,098 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:55:28,184 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:55:28,233 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:55:28,233 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:55:28,284 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:55:28,301 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:55:28,301 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:55:28,368 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:55:28,387 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:55:28,387 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:55:28,458 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:55:28,486 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:55:28,487 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:55:28,582 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:55:28,606 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:55:28,606 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:55:28,703 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:55:28,723 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:55:28,723 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:55:28,781 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:55:28,794 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:55:28,794 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:55:28,853 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:55:28,865 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:55:28,865 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:55:28,921 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:55:28,934 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:55:28,938 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:55:28,990 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:55:29,003 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:55:29,003 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:55:29,055 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:55:29,078 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:55:29,078 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:56:33,371 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:56:33,377 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:56:33,377 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:56:33,402 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:56:33,407 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:56:33,407 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:56:33,671 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:56:33,676 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:56:33,676 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:56:51,733 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:56:51,823 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:56:51,824 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:56:51,852 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:56:51,855 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:56:51,855 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:56:51,923 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:56:51,949 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:56:51,949 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:56:51,994 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:56:52,003 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:56:52,003 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:56:52,018 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:56:52,745 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:56:52,745 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:56:52,755 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:56:52,809 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:56:52,809 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:56:52,819 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:56:53,076 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:56:53,076 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:56:53,094 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:56:53,096 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:56:53,096 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:56:53,108 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:56:53,110 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:56:53,111 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:56:53,123 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:56:53,124 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:56:53,125 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:56:53,137 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:56:53,154 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:56:53,154 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 04:56:53,189 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 04:56:54,037 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov22_04-56-53_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-22 04:56:54,041 INFO    MainThread:26724 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001C8F65066D0>>
2024-11-22 04:56:54,041 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-22 04:56:54,796 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 04:56:54,796 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:00:55,762 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:00:55,784 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:00:55,789 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:00:55,862 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:00:55,880 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:00:55,881 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:00:55,930 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:00:55,944 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:00:55,944 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:00:55,996 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:00:56,014 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:00:56,014 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:00:56,063 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:00:56,081 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:00:56,081 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:00:56,130 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:00:56,144 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:00:56,144 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:00:56,200 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:00:56,215 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:00:56,216 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:00:56,280 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:00:56,294 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:00:56,294 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:00:56,352 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:00:56,366 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:00:56,371 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:00:56,422 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:00:56,434 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:00:56,435 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:00:56,491 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:00:56,508 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:00:56,508 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:01:19,523 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:01:19,628 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:01:19,628 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:01:19,666 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:01:19,670 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:01:19,670 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:01:19,741 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:01:19,746 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:01:19,746 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:01:19,762 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:01:19,773 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:01:19,774 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:01:19,792 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:01:20,483 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:01:20,483 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:01:20,494 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:01:20,517 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:01:20,517 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:01:20,532 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:01:20,789 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:01:20,790 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:01:20,808 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:01:20,810 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:01:20,810 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:01:20,822 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:01:20,824 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:01:20,824 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:01:20,838 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:01:20,857 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:01:20,857 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:01:20,872 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:01:23,392 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:01:23,392 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:01:23,422 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:01:23,426 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:01:23,426 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:01:23,453 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:01:24,547 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:01:24,547 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:01:24,585 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:01:24,588 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:01:24,588 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:01:24,606 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:01:24,633 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:01:24,633 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:01:24,650 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:01:24,677 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:01:24,677 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:04:05,584 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:04:05,594 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:04:05,594 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:04:05,671 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:04:05,692 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:04:05,692 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:04:05,711 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:04:05,718 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:04:05,718 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:04:05,751 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:04:05,756 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:04:05,756 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:04:05,929 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:04:05,937 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:04:05,937 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:04:05,978 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:04:05,983 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:04:05,983 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:04:21,382 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:04:21,393 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:04:21,393 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:04:21,436 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:04:21,441 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:04:21,441 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:04:21,515 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:04:21,522 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:04:21,522 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:04:21,556 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:04:21,561 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:04:21,561 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:04:21,694 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:04:21,704 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:04:21,704 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:04:21,739 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:04:21,744 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:04:21,744 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:04:23,913 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:04:24,006 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:04:24,007 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:04:24,027 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:04:24,031 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:04:24,031 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:04:24,051 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:04:24,058 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:04:24,058 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:04:24,070 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:04:24,079 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:04:24,079 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:04:24,103 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:04:24,906 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:04:24,906 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:04:24,916 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:04:24,946 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:04:24,946 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:04:24,956 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:04:25,203 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:04:25,203 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:04:25,219 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:04:25,221 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:04:25,221 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:04:25,242 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:04:25,244 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:04:25,246 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:04:25,293 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:04:25,317 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:04:25,318 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:04:25,400 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:04:27,948 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:04:27,948 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:04:27,987 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:04:27,990 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:04:27,990 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:04:28,048 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:04:28,629 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:04:28,629 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:04:28,640 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:04:28,642 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:04:28,642 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:04:28,660 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:04:28,668 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:04:28,668 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:04:28,688 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:04:28,717 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:04:28,717 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:04:36,373 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:04:36,446 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:04:36,446 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:04:36,466 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:04:36,469 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:04:36,469 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:04:36,483 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:04:36,488 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:04:36,488 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:04:36,498 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:04:36,510 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:04:36,510 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:04:36,528 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:04:37,242 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:04:37,242 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:04:37,257 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:04:37,284 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:04:37,285 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:04:37,297 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:04:37,594 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:04:37,594 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:04:37,610 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:04:37,613 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:04:37,613 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:04:37,626 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:04:37,627 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:04:37,627 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:04:37,639 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:04:37,658 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:04:37,658 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:04:37,688 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:04:39,893 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:04:39,893 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:04:39,918 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:04:39,922 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:04:39,922 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:04:39,940 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:04:40,540 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:04:40,540 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:04:40,556 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:04:40,557 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:04:40,557 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:04:40,570 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:04:40,577 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:04:40,577 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:04:40,590 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:04:40,615 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:04:40,615 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:09:10,864 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:09:10,880 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:09:10,887 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:09:11,017 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:09:11,035 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:09:11,035 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:09:11,090 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:09:11,104 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:09:11,104 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:09:11,160 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:09:11,178 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:09:11,178 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:09:11,245 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:09:11,261 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:09:11,265 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:09:11,318 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:09:11,332 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:09:11,332 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:09:11,392 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:09:11,406 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:09:11,410 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:09:11,461 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:09:11,473 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:09:11,474 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:09:11,530 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:09:11,548 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:09:11,548 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:09:11,599 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:09:11,612 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:09:11,617 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:09:11,697 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:09:11,720 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:09:11,720 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:09:23,619 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:09:23,732 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:09:23,733 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:09:23,752 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:09:23,754 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:09:23,755 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:09:23,785 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:09:23,809 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:09:23,809 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:09:23,832 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:09:24,599 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:09:24,599 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:09:24,610 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:09:24,633 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:09:24,633 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:09:24,651 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:09:24,932 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:09:24,932 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:09:24,967 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:09:24,971 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:09:24,972 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:09:24,990 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:09:24,991 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:09:24,991 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:09:25,011 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:09:25,033 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:09:25,033 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:09:25,066 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:09:27,341 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:09:27,341 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:09:27,368 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:09:28,083 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:09:28,083 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:09:28,093 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:09:28,094 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:09:28,094 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:09:28,108 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:09:28,115 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:09:28,115 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:09:28,129 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:09:28,154 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:09:28,154 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:11:43,165 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:11:43,258 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:11:43,258 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:11:43,275 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:11:43,277 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:11:43,278 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:11:43,299 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:11:43,308 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:11:43,308 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:11:43,337 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:11:44,043 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:11:44,043 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:11:44,055 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:11:44,078 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:11:44,078 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:11:44,088 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:11:44,366 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:11:44,366 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:11:44,386 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:11:44,389 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:11:44,389 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:11:44,405 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:11:44,407 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:11:44,407 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:11:44,424 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:11:44,444 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:11:44,445 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:11:44,489 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:11:45,359 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov22_05-11-45_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-22 05:11:45,363 INFO    MainThread:26724 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001C8F65066D0>>
2024-11-22 05:11:45,363 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-22 05:11:46,214 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:11:46,214 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:17:57,543 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:17:57,552 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:17:57,552 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:17:57,569 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:17:57,574 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:17:57,574 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:18:07,831 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:18:07,914 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:18:07,914 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:18:07,934 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:18:07,938 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:18:07,938 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:18:07,978 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:18:07,985 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:18:07,986 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:18:08,025 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:18:08,753 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:18:08,753 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:18:08,770 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:18:08,796 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:18:08,796 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:18:08,813 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:18:09,103 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:18:09,103 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:18:09,138 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:18:09,140 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:18:09,140 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:18:09,161 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:18:09,164 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:18:09,164 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:18:09,184 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:18:09,209 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:18:09,209 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:18:09,235 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:18:10,387 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov22_05-18-10_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-22 05:18:10,389 INFO    MainThread:26724 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001C8F65066D0>>
2024-11-22 05:18:10,389 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-22 05:18:12,467 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov22_05-18-11_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-22 05:18:12,471 INFO    MainThread:26724 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001C8F65066D0>>
2024-11-22 05:18:12,471 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-22 05:18:13,482 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:18:13,483 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:25:49,272 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:25:49,365 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:25:49,365 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:25:49,382 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:25:49,384 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:25:49,384 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:25:49,409 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:25:49,416 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:25:49,416 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:25:49,430 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:25:50,166 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:25:50,167 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:25:50,180 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:25:50,218 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:25:50,218 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:25:50,239 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:25:50,572 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:25:50,573 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:25:50,604 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:25:50,608 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:25:50,608 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:25:50,634 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:25:50,636 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:25:50,637 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:25:50,663 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:25:50,688 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:25:50,688 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 05:25:50,762 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 05:25:51,591 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov22_05-25-51_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-22 05:25:51,596 INFO    MainThread:26724 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001C8F65066D0>>
2024-11-22 05:25:51,596 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-22 05:25:52,111 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 05:25:52,111 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 17:42:34,268 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 17:42:34,936 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 17:42:34,937 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 17:42:35,018 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 17:42:35,047 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 17:42:35,047 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 17:42:35,065 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 17:42:35,213 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 17:42:35,213 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 17:42:35,238 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 17:42:36,327 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 17:42:36,328 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 17:42:36,341 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 17:42:36,375 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 17:42:36,376 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 17:42:36,390 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 17:42:36,871 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 17:42:36,872 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 17:42:36,894 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 17:42:36,897 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 17:42:36,897 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 17:42:36,926 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 17:42:36,929 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 17:42:36,929 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 17:42:37,030 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 17:42:37,082 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 17:42:37,082 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 17:42:37,106 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 17:42:38,219 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov22_17-42-38_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-22 17:42:38,229 INFO    MainThread:26724 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001C8F65066D0>>
2024-11-22 17:42:38,230 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-22 17:42:38,748 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 17:42:38,748 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 17:42:58,060 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 17:42:58,813 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov22_17-42-58_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-22 17:42:58,816 INFO    MainThread:26724 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001C8F65066D0>>
2024-11-22 17:42:58,816 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-22 17:42:59,117 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 17:42:59,117 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 17:43:52,224 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 17:43:52,308 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 17:43:52,308 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 17:43:52,330 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 17:43:52,333 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 17:43:52,333 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 17:43:52,352 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 17:43:52,363 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 17:43:52,363 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 17:43:52,377 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 17:43:53,129 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 17:43:53,129 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 17:43:53,146 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 17:43:53,191 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 17:43:53,192 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 17:43:53,210 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 17:43:53,537 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 17:43:53,537 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 17:43:53,567 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 17:43:53,569 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 17:43:53,569 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 17:43:53,588 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 17:43:53,590 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 17:43:53,590 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 17:43:53,604 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 17:43:53,625 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 17:43:53,625 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 17:43:53,653 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 17:43:54,361 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov22_17-43-54_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-22 17:43:54,365 INFO    MainThread:26724 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001C8F65066D0>>
2024-11-22 17:43:54,365 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-22 17:43:54,720 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 17:43:54,720 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 17:44:15,361 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 17:44:15,481 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 17:44:15,481 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 17:44:15,501 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 17:44:15,504 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 17:44:15,504 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 17:44:15,522 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 17:44:15,531 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 17:44:15,531 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 17:44:15,546 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 17:44:16,290 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 17:44:16,290 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 17:44:16,303 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 17:44:16,335 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 17:44:16,335 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 17:44:16,348 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 17:44:16,635 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 17:44:16,635 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 17:44:16,653 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 17:44:16,655 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 17:44:16,655 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 17:44:16,689 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 17:44:16,692 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 17:44:16,692 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 17:44:16,711 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 17:44:16,737 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 17:44:16,737 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 17:44:16,760 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 17:44:17,549 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov22_17-44-17_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-22 17:44:17,553 INFO    MainThread:26724 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001C8F65066D0>>
2024-11-22 17:44:17,554 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-22 17:44:18,791 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov22_17-44-18_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-22 17:44:18,795 INFO    MainThread:26724 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001C8F65066D0>>
2024-11-22 17:44:18,796 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-22 17:44:19,792 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov22_17-44-19_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-22 17:44:19,795 INFO    MainThread:26724 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001C8F65066D0>>
2024-11-22 17:44:19,795 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-22 17:44:22,664 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov22_17-44-22_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-22 17:44:22,668 INFO    MainThread:26724 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001C8F65066D0>>
2024-11-22 17:44:22,668 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-22 17:44:23,762 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov22_17-44-23_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-22 17:44:23,766 INFO    MainThread:26724 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001C8F65066D0>>
2024-11-22 17:44:23,767 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-22 17:44:24,727 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov22_17-44-24_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-22 17:44:24,732 INFO    MainThread:26724 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001C8F65066D0>>
2024-11-22 17:44:24,732 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-22 17:44:25,727 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov22_17-44-25_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-22 17:44:25,730 INFO    MainThread:26724 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001C8F65066D0>>
2024-11-22 17:44:25,730 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-22 17:44:26,135 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 17:44:26,135 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 17:46:09,755 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 17:46:09,838 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 17:46:09,838 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 17:46:09,855 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 17:46:09,857 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 17:46:09,858 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 17:46:09,877 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 17:46:09,886 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 17:46:09,886 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 17:46:09,901 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 17:46:10,648 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 17:46:10,648 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 17:46:10,671 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 17:46:10,695 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 17:46:10,695 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 17:46:10,755 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 17:46:11,088 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 17:46:11,088 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 17:46:11,126 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 17:46:11,128 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 17:46:11,129 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 17:46:11,143 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 17:46:11,145 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 17:46:11,145 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 17:46:11,163 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 17:46:11,184 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 17:46:11,184 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 17:46:11,200 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 17:46:11,927 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov22_17-46-11_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-22 17:46:11,931 INFO    MainThread:26724 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001C8F65066D0>>
2024-11-22 17:46:11,932 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-22 17:46:13,278 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov22_17-46-13_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-22 17:46:13,280 INFO    MainThread:26724 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001C8F65066D0>>
2024-11-22 17:46:13,280 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-22 17:46:14,229 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov22_17-46-14_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-22 17:46:14,231 INFO    MainThread:26724 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001C8F65066D0>>
2024-11-22 17:46:14,231 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-22 17:46:15,262 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov22_17-46-15_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-22 17:46:15,265 INFO    MainThread:26724 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001C8F65066D0>>
2024-11-22 17:46:15,265 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-22 17:46:16,271 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov22_17-46-16_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-22 17:46:16,274 INFO    MainThread:26724 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001C8F65066D0>>
2024-11-22 17:46:16,274 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-22 17:46:17,239 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov22_17-46-17_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-22 17:46:17,241 INFO    MainThread:26724 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001C8F65066D0>>
2024-11-22 17:46:17,241 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-22 17:46:18,279 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov22_17-46-18_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-22 17:46:18,281 INFO    MainThread:26724 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001C8F65066D0>>
2024-11-22 17:46:18,281 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-22 17:46:18,685 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 17:46:18,686 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 17:54:19,677 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 17:54:19,685 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 17:54:19,685 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 17:54:19,729 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 17:54:19,734 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 17:54:19,735 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 17:54:19,788 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 17:54:19,794 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 17:54:19,794 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 17:54:20,839 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 17:54:20,847 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 17:54:20,847 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 17:54:20,874 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 17:54:20,879 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 17:54:20,880 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 17:54:24,902 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 17:54:25,008 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 17:54:25,008 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 17:54:25,026 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 17:54:25,029 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 17:54:25,029 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 17:54:25,055 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 17:54:25,061 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 17:54:25,061 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 17:54:25,111 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 17:54:25,837 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 17:54:25,837 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 17:54:25,848 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 17:54:25,872 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 17:54:25,872 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 17:54:25,888 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 17:54:26,155 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 17:54:26,156 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 17:54:26,178 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 17:54:26,180 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 17:54:26,180 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 17:54:26,200 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 17:54:26,202 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 17:54:26,202 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 17:54:26,236 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 17:54:26,260 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 17:54:26,260 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 17:54:26,290 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 17:54:27,131 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov22_17-54-27_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-22 17:54:27,135 INFO    MainThread:26724 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001C8F65066D0>>
2024-11-22 17:54:27,135 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-22 17:54:27,477 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 17:54:27,477 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 17:55:43,423 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 17:55:43,430 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 17:55:43,430 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 17:55:43,451 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 17:55:43,458 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 17:55:43,458 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 17:55:46,062 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 17:55:46,136 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 17:55:46,136 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 17:55:46,166 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 17:55:46,168 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 17:55:46,168 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 17:55:46,180 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 17:55:46,187 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 17:55:46,187 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 17:55:46,202 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 17:55:46,895 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 17:55:46,895 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 17:55:46,906 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 17:55:46,928 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 17:55:46,928 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 17:55:46,939 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 17:55:47,185 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 17:55:47,185 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 17:55:47,213 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 17:55:47,216 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 17:55:47,217 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 17:55:47,242 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 17:55:47,244 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 17:55:47,244 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 17:55:47,298 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 17:55:47,323 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 17:55:47,323 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 17:55:47,356 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 17:55:47,454 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 17:55:47,455 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 17:56:31,325 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 17:56:31,409 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 17:56:31,409 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 17:56:31,434 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 17:56:31,437 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 17:56:31,437 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 17:56:31,457 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 17:56:31,461 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 17:56:31,462 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 17:56:31,487 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 17:56:32,209 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 17:56:32,209 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 17:56:32,219 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 17:56:32,243 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 17:56:32,243 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 17:56:32,254 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 17:56:32,550 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 17:56:32,550 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 17:56:32,581 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 17:56:32,584 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 17:56:32,585 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 17:56:32,610 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 17:56:32,613 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 17:56:32,613 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 17:56:32,660 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 17:56:32,690 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 17:56:32,690 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 17:56:32,725 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 17:56:32,834 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 17:56:32,834 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 17:56:38,412 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 17:56:38,417 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 17:56:38,418 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 17:56:38,439 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 17:56:38,446 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 17:56:38,447 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 17:57:47,024 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 17:57:47,096 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 17:57:47,097 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 17:57:47,118 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 17:57:47,120 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 17:57:47,120 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 17:57:47,138 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 17:57:47,144 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 17:57:47,144 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 17:57:47,158 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 17:57:47,821 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 17:57:47,822 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 17:57:47,834 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 17:57:47,855 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 17:57:47,855 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 17:57:47,866 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 17:57:48,142 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 17:57:48,143 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 17:57:48,158 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 17:57:48,161 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 17:57:48,161 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 17:57:48,174 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 17:57:48,176 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 17:57:48,176 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 17:57:48,189 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 17:57:48,208 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 17:57:48,208 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 17:57:48,238 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 17:57:49,010 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov22_17-57-48_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-22 17:57:49,013 INFO    MainThread:26724 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001C8F65066D0>>
2024-11-22 17:57:49,013 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-22 17:57:49,345 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 17:57:49,345 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:02:41,118 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:02:41,229 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:02:41,229 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:02:41,278 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:02:41,280 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:02:41,280 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:02:41,310 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:02:41,315 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:02:41,316 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:02:41,335 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:02:42,115 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:02:42,115 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:02:42,127 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:02:42,149 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:02:42,149 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:02:42,165 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:02:42,471 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:02:42,471 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:02:42,495 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:02:42,498 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:02:42,498 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:02:42,514 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:02:42,517 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:02:42,518 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:02:42,583 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:02:42,607 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:02:42,607 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:02:42,646 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:02:43,787 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov22_18-02-43_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-22 18:02:43,792 INFO    MainThread:26724 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001C8F65066D0>>
2024-11-22 18:02:43,792 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-22 18:02:44,537 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:02:44,537 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:04:14,540 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:04:14,547 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:04:14,547 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:04:14,837 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:04:14,843 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:04:14,843 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:04:14,971 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:04:14,976 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:04:14,976 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:04:15,133 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:04:15,142 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:04:15,142 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:04:15,374 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:04:15,380 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:04:15,380 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:04:15,591 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:04:15,600 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:04:15,600 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:04:15,698 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:04:15,705 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:04:15,705 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:04:19,010 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:04:19,096 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:04:19,096 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:04:19,114 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:04:19,117 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:04:19,117 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:04:19,142 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:04:19,147 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:04:19,148 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:04:19,164 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:04:19,914 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:04:19,914 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:04:19,926 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:04:19,951 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:04:19,952 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:04:19,964 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:04:20,292 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:04:20,292 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:04:20,324 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:04:20,326 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:04:20,327 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:04:20,345 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:04:20,347 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:04:20,348 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:04:20,377 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:04:20,404 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:04:20,404 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:04:20,450 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:04:21,598 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov22_18-04-21_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-22 18:04:21,602 INFO    MainThread:26724 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001C8F65066D0>>
2024-11-22 18:04:21,602 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-22 18:04:21,914 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:04:21,914 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:05:27,047 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:05:27,131 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:05:27,131 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:05:27,147 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:05:27,150 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:05:27,151 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:05:27,173 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:05:27,179 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:05:27,179 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:05:27,193 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:05:27,979 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:05:27,979 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:05:27,993 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:05:28,017 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:05:28,017 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:05:28,029 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:05:28,332 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:05:28,333 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:05:28,379 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:05:28,382 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:05:28,383 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:05:28,414 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:05:28,416 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:05:28,416 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:05:28,443 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:05:28,464 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:05:28,464 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:05:28,487 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:05:29,239 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov22_18-05-29_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-22 18:05:29,243 INFO    MainThread:26724 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001C8F65066D0>>
2024-11-22 18:05:29,243 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-22 18:05:29,622 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:05:29,622 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:06:12,101 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:06:12,187 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:06:12,187 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:06:12,204 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:06:12,206 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:06:12,206 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:06:12,226 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:06:12,232 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:06:12,232 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:06:12,246 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:06:13,001 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:06:13,001 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:06:13,015 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:06:13,045 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:06:13,045 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:06:13,065 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:06:13,395 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:06:13,396 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:06:13,436 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:06:13,439 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:06:13,439 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:06:13,490 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:06:13,492 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:06:13,493 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:06:13,521 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:06:13,526 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:06:13,526 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:06:13,561 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:06:14,262 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov22_18-06-14_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-22 18:06:14,265 INFO    MainThread:26724 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001C8F65066D0>>
2024-11-22 18:06:14,265 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-22 18:06:14,600 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:06:14,601 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:06:34,969 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:06:35,051 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:06:35,051 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:06:35,072 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:06:35,075 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:06:35,075 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:06:35,095 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:06:35,106 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:06:35,106 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:06:35,131 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:06:35,902 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:06:35,902 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:06:35,944 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:06:35,970 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:06:35,970 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:06:35,984 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:06:36,309 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:06:36,309 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:06:36,330 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:06:36,333 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:06:36,333 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:06:36,373 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:06:36,374 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:06:36,374 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:06:36,419 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:06:36,424 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:06:36,425 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:06:36,582 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:06:37,378 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov22_18-06-37_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-22 18:06:37,381 INFO    MainThread:26724 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001C8F65066D0>>
2024-11-22 18:06:37,381 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-22 18:06:38,201 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:06:38,202 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:06:54,677 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:06:54,761 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:06:54,761 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:06:54,776 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:06:54,779 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:06:54,779 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:06:54,797 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:06:54,804 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:06:54,804 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:06:54,818 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:06:55,585 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:06:55,585 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:06:55,600 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:06:55,622 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:06:55,622 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:06:55,636 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:06:55,969 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:06:55,970 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:06:56,059 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:06:56,062 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:06:56,062 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:06:56,077 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:06:56,080 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:06:56,080 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:06:56,095 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:06:56,116 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:06:56,116 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:06:56,136 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:06:56,888 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov22_18-06-56_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-22 18:06:56,890 INFO    MainThread:26724 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001C8F65066D0>>
2024-11-22 18:06:56,890 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-22 18:06:57,716 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:06:57,716 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:07:18,815 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:07:18,921 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:07:18,922 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:07:18,936 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:07:18,939 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:07:18,939 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:07:18,956 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:07:18,962 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:07:18,962 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:07:18,985 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:07:19,712 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:07:19,712 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:07:19,724 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:07:19,748 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:07:19,748 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:07:19,762 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:07:20,016 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:07:20,016 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:07:20,045 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:07:20,047 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:07:20,047 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:07:20,064 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:07:20,066 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:07:20,066 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:07:20,083 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:07:20,106 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:07:20,106 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:07:20,128 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:07:21,299 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov22_18-07-20_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-22 18:07:21,302 INFO    MainThread:26724 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001C8F65066D0>>
2024-11-22 18:07:21,302 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-22 18:07:23,436 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov22_18-07-22_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-22 18:07:23,440 INFO    MainThread:26724 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001C8F65066D0>>
2024-11-22 18:07:23,441 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-22 18:07:24,505 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:07:24,506 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:08:05,962 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:08:05,969 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:08:05,971 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:08:05,991 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:08:05,996 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:08:05,997 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:08:12,854 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:08:12,927 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:08:12,928 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:08:12,959 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:08:12,960 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:08:12,961 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:08:12,973 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:08:12,979 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:08:12,979 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:08:12,994 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:08:13,746 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:08:13,746 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:08:13,758 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:08:13,783 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:08:13,783 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:08:13,798 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:08:14,148 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:08:14,149 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:08:14,168 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:08:14,171 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:08:14,171 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:08:14,192 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:08:14,193 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:08:14,193 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:08:14,220 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:08:14,240 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:08:14,240 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:08:14,280 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:08:15,038 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov22_18-08-14_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-22 18:08:15,041 INFO    MainThread:26724 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001C8F65066D0>>
2024-11-22 18:08:15,041 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-22 18:08:15,896 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:08:15,896 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:08:38,006 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:08:38,089 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:08:38,089 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:08:38,131 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:08:38,133 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:08:38,133 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:08:38,162 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:08:38,170 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:08:38,170 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:08:38,221 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:08:38,978 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:08:38,978 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:08:38,992 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:08:39,018 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:08:39,018 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:08:39,057 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:08:39,391 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:08:39,391 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:08:39,479 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:08:39,481 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:08:39,481 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:08:39,506 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:08:39,509 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:08:39,509 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:08:39,561 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:08:39,581 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:08:39,581 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:08:39,692 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:08:40,795 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov22_18-08-40_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-22 18:08:40,797 INFO    MainThread:26724 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001C8F65066D0>>
2024-11-22 18:08:40,797 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-22 18:08:41,157 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:08:41,157 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:09:23,941 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:09:24,036 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:09:24,036 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:09:24,062 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:09:24,063 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:09:24,063 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:09:24,081 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:09:24,092 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:09:24,092 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:09:24,115 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:09:24,896 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:09:24,897 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:09:24,909 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:09:24,934 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:09:24,934 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:09:24,956 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:09:25,258 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:09:25,258 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:09:25,308 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:09:25,310 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:09:25,310 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:09:25,334 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:09:25,336 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:09:25,337 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:09:25,354 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:09:25,375 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:09:25,375 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:09:25,395 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:09:26,494 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov22_18-09-26_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-22 18:09:26,496 INFO    MainThread:26724 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001C8F65066D0>>
2024-11-22 18:09:26,496 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-22 18:09:26,852 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:09:26,853 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:09:41,068 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:09:41,161 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:09:41,162 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:09:41,193 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:09:41,195 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:09:41,195 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:09:41,211 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:09:41,215 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:09:41,215 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:09:41,262 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:09:42,015 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:09:42,015 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:09:42,033 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:09:42,059 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:09:42,059 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:09:42,076 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:09:42,919 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:09:42,919 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:09:42,944 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:09:42,947 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:09:42,947 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:09:42,989 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:09:42,992 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:09:42,992 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:09:43,026 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:09:43,048 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:09:43,048 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:09:43,113 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:09:44,171 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov22_18-09-43_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-22 18:09:44,174 INFO    MainThread:26724 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001C8F65066D0>>
2024-11-22 18:09:44,174 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-22 18:09:46,285 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov22_18-09-45_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-22 18:09:46,288 INFO    MainThread:26724 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001C8F65066D0>>
2024-11-22 18:09:46,288 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-22 18:09:47,403 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:09:47,403 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:10:34,622 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:10:34,717 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:10:34,717 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:10:34,746 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:10:34,749 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:10:34,749 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:10:34,768 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:10:34,774 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:10:34,774 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:10:34,802 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:10:35,666 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:10:35,666 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:10:35,687 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:10:35,724 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:10:35,726 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:10:35,768 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:10:36,094 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:10:36,094 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:10:36,130 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:10:36,134 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:10:36,134 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:10:36,153 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:10:36,155 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:10:36,155 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:10:36,189 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:10:36,210 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:10:36,210 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:10:36,279 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:10:37,335 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov22_18-10-36_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-22 18:10:37,338 INFO    MainThread:26724 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001C8F65066D0>>
2024-11-22 18:10:37,338 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-22 18:10:37,710 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:10:37,710 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:16:50,645 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:16:50,763 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:16:50,763 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:16:50,798 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:16:50,801 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:16:50,801 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:16:50,877 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:16:50,885 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:16:50,886 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:16:50,972 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:16:51,852 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:16:51,852 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:16:51,866 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:16:51,912 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:16:51,912 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:16:51,944 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:16:52,286 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:16:52,287 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:16:52,320 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:16:52,325 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:16:52,325 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:16:52,467 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:16:52,471 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:16:52,471 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:16:52,498 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:16:52,520 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:16:52,520 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:16:52,559 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:16:53,855 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov22_18-16-53_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-22 18:16:53,859 INFO    MainThread:26724 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001C8F65066D0>>
2024-11-22 18:16:53,859 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-22 18:16:54,159 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:16:54,159 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:17:12,072 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:17:12,080 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:17:12,080 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:17:12,125 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:17:12,130 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:17:12,132 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:17:12,764 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:17:12,768 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:17:12,769 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:17:13,212 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:17:13,222 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:17:13,222 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:17:20,154 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:17:20,248 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:17:20,249 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:17:20,270 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:17:20,273 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:17:20,273 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:17:20,312 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:17:20,319 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:17:20,319 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:17:20,376 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:17:21,193 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:17:21,194 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:17:21,205 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:17:21,241 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:17:21,241 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:17:21,252 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:17:21,574 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:17:21,574 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:17:21,601 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:17:21,603 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:17:21,604 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:17:21,628 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:17:21,629 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:17:21,629 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:17:21,644 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:17:21,664 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:17:21,664 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:17:21,681 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:17:22,768 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov22_18-17-22_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-22 18:17:22,771 INFO    MainThread:26724 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001C8F65066D0>>
2024-11-22 18:17:22,771 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-22 18:17:23,132 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:17:23,132 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:21:49,866 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:21:49,948 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:21:49,948 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:21:49,980 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:21:49,982 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:21:49,983 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:21:50,021 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:21:50,028 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:21:50,028 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:21:50,067 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:21:50,909 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:21:50,909 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:21:50,922 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:21:50,952 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:21:50,952 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:21:50,965 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:21:51,288 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:21:51,288 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:21:51,312 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:21:51,314 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:21:51,314 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:21:51,329 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:21:51,331 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:21:51,331 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:21:51,346 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:21:51,368 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:21:51,368 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 18:21:51,384 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 18:21:52,534 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov22_18-21-52_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-22 18:21:52,537 INFO    MainThread:26724 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001C8F65066D0>>
2024-11-22 18:21:52,537 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-22 18:21:52,905 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 18:21:52,905 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:09:28,777 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:09:28,784 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:09:28,784 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:09:28,812 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:09:28,818 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:09:28,818 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:09:30,609 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:09:30,615 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:09:30,615 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:09:30,632 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:09:30,638 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:09:30,639 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:09:34,274 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:09:34,477 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:09:34,477 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:09:34,514 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:09:34,517 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:09:34,517 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:09:34,566 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:09:34,602 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:09:34,602 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:09:34,622 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:09:35,483 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:09:35,483 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:09:35,499 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:09:35,526 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:09:35,526 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:09:35,545 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:09:35,822 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:09:35,823 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:09:35,849 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:09:35,852 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:09:35,852 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:09:35,885 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:09:35,888 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:09:35,888 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:09:35,918 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:09:35,939 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:09:35,939 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:09:35,965 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:09:36,862 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov22_19-09-36_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-22 19:09:36,866 INFO    MainThread:26724 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001C8F65066D0>>
2024-11-22 19:09:36,867 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-22 19:09:37,321 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:09:37,322 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:11:29,817 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:11:29,828 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:11:29,828 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:11:32,059 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:11:32,068 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:11:32,068 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:11:35,522 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:11:35,649 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:11:35,650 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:11:35,672 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:11:35,675 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:11:35,675 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:11:35,707 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:11:35,715 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:11:35,715 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:11:35,731 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:11:36,536 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:11:36,536 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:11:36,550 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:11:36,576 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:11:36,577 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:11:36,608 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:11:37,010 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:11:37,010 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:11:37,041 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:11:37,044 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:11:37,044 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:11:37,064 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:11:37,067 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:11:37,067 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:11:37,127 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:11:37,154 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:11:37,155 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:11:37,250 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:11:38,014 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov22_19-11-37_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-22 19:11:38,020 INFO    MainThread:26724 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001C8F65066D0>>
2024-11-22 19:11:38,020 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-22 19:11:38,460 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:11:38,460 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:13:58,311 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:13:58,412 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:13:58,412 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:13:58,433 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:13:58,437 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:13:58,437 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:13:58,486 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:13:58,516 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:13:58,516 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:13:58,550 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:13:59,292 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:13:59,292 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:13:59,309 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:13:59,350 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:13:59,350 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:13:59,364 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:13:59,679 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:13:59,679 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:13:59,725 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:13:59,728 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:13:59,728 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:13:59,752 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:13:59,755 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:13:59,755 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:13:59,770 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:13:59,794 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:13:59,796 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:13:59,827 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:14:00,806 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov22_19-14-00_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-22 19:14:00,809 INFO    MainThread:26724 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001C8F65066D0>>
2024-11-22 19:14:00,809 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-22 19:14:01,165 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:14:01,165 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:14:49,040 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:14:49,047 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:14:49,047 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:14:49,175 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:14:49,181 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:14:49,182 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:14:49,215 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:14:49,221 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:14:49,222 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:14:49,239 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:14:49,245 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:14:49,246 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:14:49,279 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:14:49,285 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:14:49,285 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:14:51,471 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:14:52,214 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov22_19-14-52_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-22 19:14:52,218 INFO    MainThread:26724 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001C8F65066D0>>
2024-11-22 19:14:52,218 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-22 19:14:52,611 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:14:52,611 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:15:09,474 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:15:09,583 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:15:09,584 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:15:09,625 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:15:09,627 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:15:09,627 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:15:09,664 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:15:09,671 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:15:09,671 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:15:09,694 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:15:10,403 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:15:10,403 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:15:10,418 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:15:10,449 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:15:10,449 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:15:10,465 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:15:10,740 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:15:10,741 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:15:10,766 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:15:10,769 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:15:10,769 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:15:10,810 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:15:10,812 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:15:10,813 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:15:10,851 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:15:10,879 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:15:10,880 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:15:10,943 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:15:11,640 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov22_19-15-11_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-22 19:15:11,644 INFO    MainThread:26724 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001C8F65066D0>>
2024-11-22 19:15:11,644 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-22 19:15:12,043 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:15:12,043 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:16:42,937 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:16:42,944 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:16:42,944 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:16:42,984 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:16:42,989 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:16:42,989 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:16:45,286 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:16:46,550 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov22_19-16-46_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-22 19:16:46,555 INFO    MainThread:26724 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001C8F65066D0>>
2024-11-22 19:16:46,556 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-22 19:16:46,987 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:16:46,987 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:17:32,898 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:17:32,907 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:17:32,907 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:17:32,983 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:17:33,014 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:17:33,015 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:17:33,036 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:17:33,042 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:17:33,042 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:17:33,324 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:17:33,341 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:17:33,342 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:17:34,981 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:17:34,989 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:17:34,989 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:17:35,438 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:17:35,445 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:17:35,445 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:17:35,548 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:17:35,557 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:17:35,557 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:17:35,754 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:17:35,760 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:17:35,761 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:17:35,947 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:17:35,952 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:17:35,953 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:17:38,894 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:17:38,983 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:17:38,983 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:17:39,027 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:17:39,029 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:17:39,029 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:17:39,046 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:17:39,055 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:17:39,055 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:17:39,089 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:17:39,933 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:17:39,933 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:17:39,946 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:17:39,973 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:17:39,973 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:17:39,988 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:17:40,312 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:17:40,313 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:17:40,343 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:17:40,346 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:17:40,346 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:17:40,366 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:17:40,367 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:17:40,368 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:17:40,385 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:17:40,408 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:17:40,408 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:17:40,440 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:17:41,170 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov22_19-17-41_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-22 19:17:41,174 INFO    MainThread:26724 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001C8F65066D0>>
2024-11-22 19:17:41,174 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-22 19:17:41,564 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:17:41,565 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:18:52,322 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:18:52,416 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:18:52,416 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:18:52,438 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:18:52,440 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:18:52,440 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:18:52,519 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:18:52,526 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:18:52,526 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:18:52,619 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:18:53,438 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:18:53,439 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:18:53,451 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:18:53,484 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:18:53,484 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:18:53,500 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:18:53,811 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:18:53,811 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:18:53,844 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:18:53,847 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:18:53,847 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:18:53,872 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:18:53,874 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:18:53,875 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:18:53,918 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:18:53,947 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:18:53,947 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:18:54,065 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:18:55,308 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:18:55,308 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:19:09,067 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:19:09,150 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:19:09,150 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:19:09,188 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:19:09,190 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:19:09,190 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:19:09,215 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:19:09,224 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:19:09,224 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:19:09,242 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:19:10,052 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:19:10,052 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:19:10,083 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:19:10,127 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:19:10,127 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:19:10,174 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:19:10,508 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:19:10,508 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:19:10,545 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:19:10,547 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:19:10,547 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:19:10,564 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:19:10,565 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:19:10,565 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:19:10,605 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:19:10,626 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:19:10,626 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:19:10,669 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:19:11,525 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:19:11,525 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:19:52,549 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:19:52,554 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:19:52,554 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:19:55,754 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:19:55,850 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:19:55,850 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:19:55,870 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:19:55,872 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:19:55,872 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:19:55,901 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:19:55,909 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:19:55,910 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:19:55,979 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:19:57,688 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:19:57,688 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:19:57,705 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:19:57,805 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:19:57,805 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:19:57,826 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:19:58,109 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:19:58,109 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:19:58,123 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:19:58,127 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:19:58,127 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:19:58,139 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:19:58,142 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:19:58,142 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:19:58,156 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:19:58,186 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:19:58,186 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:19:58,213 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:19:59,480 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov22_19-19-58_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-22 19:19:59,484 INFO    MainThread:26724 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001C8F65066D0>>
2024-11-22 19:19:59,484 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-22 19:19:59,821 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:19:59,821 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:21:07,912 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:21:07,918 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:21:07,918 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:21:10,963 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:21:11,068 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:21:11,068 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:21:11,091 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:21:11,092 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:21:11,092 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:21:11,176 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:21:11,184 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:21:11,184 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:21:11,264 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:21:12,065 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:21:12,065 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:21:12,101 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:21:12,135 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:21:12,135 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:21:12,174 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:21:12,515 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:21:12,515 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:21:12,536 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:21:12,538 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:21:12,539 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:21:12,558 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:21:12,563 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:21:12,564 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:21:12,739 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:21:12,760 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:21:12,761 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:21:12,780 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:21:13,478 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov22_19-21-13_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-22 19:21:13,482 INFO    MainThread:26724 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001C8F65066D0>>
2024-11-22 19:21:13,482 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-22 19:21:13,846 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:21:13,846 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:22:31,523 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:22:31,535 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:22:31,536 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:22:31,563 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:22:31,590 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:22:31,590 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:22:39,003 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:22:39,099 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:22:39,099 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:22:39,131 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:22:39,133 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:22:39,133 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:22:39,151 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:22:39,157 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:22:39,158 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:22:39,176 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:22:40,029 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:22:40,029 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:22:40,042 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:22:40,076 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:22:40,076 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:22:40,090 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:22:40,403 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:22:40,403 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:22:40,432 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:22:40,436 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:22:40,436 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:22:40,458 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:22:40,460 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:22:40,460 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:22:40,481 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:22:40,505 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:22:40,505 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:22:40,529 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:22:41,385 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov22_19-22-41_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-22 19:22:41,390 INFO    MainThread:26724 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001C8F65066D0>>
2024-11-22 19:22:41,390 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-22 19:22:41,815 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:22:41,815 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:24:16,997 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:24:17,092 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:24:17,092 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:24:17,112 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:24:17,116 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:24:17,116 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:24:17,138 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:24:17,143 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:24:17,143 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:24:17,159 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:24:17,975 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:24:17,975 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:24:17,986 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:24:18,014 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:24:18,014 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:24:18,029 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:24:18,316 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:24:18,316 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:24:18,349 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:24:18,352 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:24:18,352 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:24:18,381 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:24:18,383 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:24:18,384 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:24:18,417 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:24:18,440 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:24:18,440 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:24:18,473 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:24:19,822 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov22_19-24-19_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-22 19:24:19,827 INFO    MainThread:26724 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001C8F65066D0>>
2024-11-22 19:24:19,827 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-22 19:24:20,184 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:24:20,184 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:28:18,992 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:28:19,008 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:28:19,008 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:28:19,112 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:28:19,116 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:28:19,117 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:28:19,137 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:28:19,143 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:28:19,143 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:28:19,177 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:28:19,183 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:28:19,183 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:28:20,010 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:28:20,034 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:28:20,034 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:28:20,102 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:28:20,109 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:28:20,110 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:28:20,193 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:28:20,198 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:28:20,199 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:28:22,031 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:28:22,825 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov22_19-28-22_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-22 19:28:22,829 INFO    MainThread:26724 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001C8F65066D0>>
2024-11-22 19:28:22,829 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-22 19:28:23,171 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:28:23,171 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:30:14,836 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:30:14,985 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:30:14,985 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:30:15,010 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:30:15,012 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:30:15,012 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:30:15,029 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:30:15,055 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:30:15,055 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:30:15,071 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:30:15,859 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:30:15,859 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:30:15,874 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:30:15,901 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:30:15,901 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:30:15,919 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:30:16,223 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:30:16,223 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:30:16,263 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:30:16,266 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:30:16,266 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:30:16,308 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:30:16,309 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:30:16,309 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:30:16,373 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:30:16,400 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:30:16,400 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:30:16,490 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:30:17,379 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:30:17,380 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:32:23,924 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:32:24,066 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:32:24,067 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:32:24,106 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:32:24,109 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:32:24,109 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:32:24,143 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:32:24,148 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:32:24,148 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:32:24,190 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:32:24,976 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:32:24,976 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:32:24,988 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:32:25,013 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:32:25,014 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:32:25,032 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:32:25,370 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:32:25,370 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:32:25,394 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:32:25,397 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:32:25,397 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:32:25,420 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:32:25,421 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:32:25,421 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:32:25,471 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:32:25,501 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:32:25,501 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:32:25,624 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:32:26,469 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:32:26,469 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:32:48,876 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:32:48,962 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:32:48,962 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:32:48,998 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:32:49,001 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:32:49,001 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:32:49,057 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:32:49,063 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:32:49,063 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:32:49,105 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:32:49,861 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:32:49,861 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:32:49,879 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:32:49,906 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:32:49,906 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:32:49,935 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:32:50,259 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:32:50,259 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:32:50,281 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:32:50,284 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:32:50,284 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:32:50,302 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:32:50,305 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:32:50,305 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:32:50,332 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:32:50,355 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:32:50,355 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:32:50,436 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:32:51,247 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov22_19-32-51_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-22 19:32:51,250 INFO    MainThread:26724 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001C8F65066D0>>
2024-11-22 19:32:51,250 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-22 19:32:51,625 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:32:51,626 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:34:44,545 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:34:44,665 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:34:44,665 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:34:44,688 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:34:44,690 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:34:44,691 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:34:44,727 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:34:44,733 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:34:44,733 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:34:44,764 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:34:45,557 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:34:45,557 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:34:45,573 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:34:45,621 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:34:45,621 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:34:45,659 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:34:45,982 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:34:45,982 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:34:46,015 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:34:46,018 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:34:46,018 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:34:46,035 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:34:46,037 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:34:46,037 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:34:46,055 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:34:46,076 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:34:46,076 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:34:46,091 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:34:46,825 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov22_19-34-46_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-22 19:34:46,830 INFO    MainThread:26724 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001C8F65066D0>>
2024-11-22 19:34:46,830 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-22 19:34:47,200 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:34:47,200 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:35:11,619 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:35:11,715 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:35:11,715 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:35:11,736 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:35:11,738 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:35:11,738 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:35:11,763 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:35:11,768 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:35:11,769 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:35:11,789 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:35:12,607 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:35:12,607 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:35:12,618 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:35:12,658 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:35:12,658 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:35:12,680 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:35:13,005 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:35:13,005 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:35:13,032 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:35:13,034 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:35:13,034 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:35:13,049 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:35:13,051 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:35:13,051 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:35:13,069 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:35:13,093 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:35:13,093 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:35:13,125 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:35:13,859 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov22_19-35-13_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-22 19:35:13,864 INFO    MainThread:26724 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001C8F65066D0>>
2024-11-22 19:35:13,864 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-22 19:35:14,083 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:35:14,083 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:38:40,906 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:38:40,936 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:38:40,936 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:38:41,039 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:38:41,065 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:38:41,065 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:38:41,115 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:38:41,129 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:38:41,130 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:38:41,219 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:38:41,247 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:38:41,248 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:38:41,317 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:38:41,335 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:38:41,336 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:38:41,386 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:38:41,401 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:38:41,401 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:38:41,457 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:38:41,471 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:38:41,471 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:38:41,982 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:38:41,995 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:38:41,995 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:38:42,053 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:38:42,068 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:38:42,068 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:38:42,124 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:38:42,137 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:38:42,137 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:38:42,201 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:38:42,218 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:38:42,220 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:40:02,868 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:40:03,004 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:40:03,004 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:40:03,066 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:40:03,068 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:40:03,068 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:40:03,092 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:40:03,099 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:40:03,099 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:40:03,117 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:40:13,942 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:40:13,942 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:40:14,003 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:40:14,530 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:40:14,530 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:40:14,564 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:40:15,737 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:40:15,737 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:40:15,755 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:40:15,758 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:40:15,758 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:40:15,787 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:40:15,790 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:40:15,790 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:40:15,823 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:40:15,985 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:40:15,985 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:40:16,029 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:40:26,022 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov22_19-40-16_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-22 19:40:26,027 INFO    MainThread:26724 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001C8F65066D0>>
2024-11-22 19:40:26,027 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-22 19:40:48,571 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov22_19-40-35_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-22 19:40:48,575 INFO    MainThread:26724 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001C8F65066D0>>
2024-11-22 19:40:48,575 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-22 19:41:11,495 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov22_19-41-03_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-22 19:41:11,499 INFO    MainThread:26724 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001C8F65066D0>>
2024-11-22 19:41:11,499 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-22 19:41:28,617 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov22_19-41-19_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-22 19:41:28,620 INFO    MainThread:26724 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001C8F65066D0>>
2024-11-22 19:41:28,620 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-22 19:41:50,262 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov22_19-41-38_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-22 19:41:50,267 INFO    MainThread:26724 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001C8F65066D0>>
2024-11-22 19:41:50,267 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-22 19:42:12,694 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov22_19-42-03_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-22 19:42:12,697 INFO    MainThread:26724 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001C8F65066D0>>
2024-11-22 19:42:12,697 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-22 19:42:34,720 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov22_19-42-22_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-22 19:42:34,725 INFO    MainThread:26724 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001C8F65066D0>>
2024-11-22 19:42:34,725 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-22 19:42:48,749 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:42:48,749 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:42:49,078 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:42:49,182 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:42:49,182 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:42:49,211 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:42:49,214 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:42:49,214 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:42:49,245 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:42:49,253 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:42:49,253 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:42:49,289 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:42:50,921 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:42:50,922 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:43:30,114 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:43:30,243 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:43:30,243 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:43:30,293 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:43:30,296 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:43:30,296 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:43:30,321 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:43:30,326 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:43:30,326 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:43:30,363 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:43:33,049 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:43:33,049 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:43:33,087 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:43:33,222 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:43:33,223 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:43:33,257 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:43:33,658 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:43:33,659 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:43:33,683 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:43:33,686 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:43:33,686 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:43:33,781 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:43:33,782 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:43:33,782 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:43:33,825 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:43:33,865 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:43:33,865 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:43:33,885 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:43:36,054 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov22_19-43-34_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-22 19:43:36,057 INFO    MainThread:26724 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001C8F65066D0>>
2024-11-22 19:43:36,057 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-22 19:43:36,163 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:43:36,163 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:44:02,686 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:44:02,783 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:44:02,783 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:44:02,812 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:44:02,814 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:44:02,815 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:44:02,832 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:44:02,839 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:44:02,839 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:44:02,865 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:44:08,007 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:44:08,008 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:44:08,044 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:44:08,402 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:44:08,403 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:44:08,444 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:44:09,011 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:44:09,011 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:44:09,027 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:44:09,030 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:44:09,030 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:44:09,049 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:44:09,051 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:44:09,051 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:44:09,069 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:44:09,152 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:44:09,152 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:44:09,174 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:44:14,208 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov22_19-44-09_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-22 19:44:14,211 INFO    MainThread:26724 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001C8F65066D0>>
2024-11-22 19:44:14,211 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-22 19:44:21,888 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:44:21,888 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:46:32,363 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:46:32,779 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:46:32,780 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:46:32,803 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:46:32,806 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:46:32,807 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:46:32,839 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:46:32,863 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:46:32,863 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:46:32,880 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:46:37,980 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:46:37,980 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:46:38,063 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:46:38,354 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:46:38,354 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:46:38,372 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:46:38,981 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:46:38,982 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:46:38,999 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:46:39,002 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:46:39,002 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:46:39,019 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:46:39,023 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:46:39,023 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:46:39,039 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:46:39,124 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:46:39,124 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:46:39,140 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:46:44,252 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov22_19-46-39_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-22 19:46:44,256 INFO    MainThread:26724 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001C8F65066D0>>
2024-11-22 19:46:44,256 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-22 19:46:53,511 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:46:53,513 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:46:58,260 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:46:58,368 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:46:58,369 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:46:58,398 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:46:58,401 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:46:58,401 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:46:58,421 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:46:58,428 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:46:58,428 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:46:58,449 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:47:03,369 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:47:03,369 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:47:03,429 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:47:03,729 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:47:03,729 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:47:03,748 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:47:04,362 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:47:04,363 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:47:04,382 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:47:04,385 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:47:04,385 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:47:04,408 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:47:04,410 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:47:04,411 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:47:04,435 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:47:04,518 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:47:04,519 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:47:04,553 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:47:09,521 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov22_19-47-05_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-22 19:47:09,524 INFO    MainThread:26724 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001C8F65066D0>>
2024-11-22 19:47:09,524 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-22 19:47:15,759 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:47:15,759 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:48:30,918 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:48:31,014 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:48:31,014 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:48:31,046 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:48:31,050 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:48:31,050 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:48:31,078 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:48:31,086 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:48:31,086 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:48:31,114 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:48:34,625 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:48:34,625 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:48:34,643 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:48:34,681 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:48:34,681 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:48:34,716 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:48:34,968 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:48:34,968 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:48:35,009 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:48:35,011 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:48:35,011 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:48:35,049 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:48:35,051 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:48:35,051 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:48:35,079 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:48:35,097 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:48:35,097 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:48:35,122 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:48:35,210 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:48:35,210 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:53:41,737 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:53:41,760 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:53:41,761 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:53:41,850 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:53:41,871 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:53:41,871 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:53:41,926 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:53:41,943 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:53:41,943 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:53:41,994 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:53:42,013 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:53:42,013 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:53:42,064 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:53:42,083 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:53:42,083 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:53:42,132 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:53:42,147 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:53:42,153 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:53:42,203 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:53:42,218 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:53:42,218 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:53:42,282 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:53:42,305 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:53:42,312 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:53:42,391 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:53:42,410 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:53:42,410 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:53:42,462 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:53:42,476 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:53:42,477 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:53:42,536 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:53:42,570 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:53:42,570 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:53:42,624 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:53:42,643 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:53:42,644 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:53:53,205 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:53:53,310 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:53:53,310 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:53:53,345 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:53:53,348 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:53:53,348 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:53:53,383 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:53:53,392 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:53:53,392 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:53:53,446 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:53:58,710 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:53:58,710 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:53:58,750 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:53:59,065 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:53:59,065 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:53:59,079 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:53:59,651 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:53:59,651 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:53:59,671 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:53:59,673 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:53:59,673 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:53:59,690 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:53:59,692 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:53:59,692 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:53:59,706 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:53:59,781 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:53:59,781 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:53:59,821 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:54:05,205 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov22_19-54-00_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-22 19:54:05,208 INFO    MainThread:26724 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001C8F65066D0>>
2024-11-22 19:54:05,208 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-22 19:54:15,345 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov22_19-54-08_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-22 19:54:15,349 INFO    MainThread:26724 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001C8F65066D0>>
2024-11-22 19:54:15,349 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-22 19:54:25,833 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov22_19-54-21_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-22 19:54:25,838 INFO    MainThread:26724 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001C8F65066D0>>
2024-11-22 19:54:25,838 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-22 19:54:33,409 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov22_19-54-29_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-22 19:54:33,414 INFO    MainThread:26724 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001C8F65066D0>>
2024-11-22 19:54:33,414 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-22 19:54:43,630 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov22_19-54-37_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-22 19:54:43,633 INFO    MainThread:26724 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001C8F65066D0>>
2024-11-22 19:54:43,633 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-22 19:54:53,421 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov22_19-54-49_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-22 19:54:53,423 INFO    MainThread:26724 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001C8F65066D0>>
2024-11-22 19:54:53,423 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-22 19:55:03,062 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov22_19-54-57_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-22 19:55:03,065 INFO    MainThread:26724 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001C8F65066D0>>
2024-11-22 19:55:03,066 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-22 19:55:11,951 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov22_19-55-08_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-22 19:55:11,954 INFO    MainThread:26724 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001C8F65066D0>>
2024-11-22 19:55:11,954 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-22 19:55:18,952 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov22_19-55-14_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-22 19:55:18,956 INFO    MainThread:26724 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001C8F65066D0>>
2024-11-22 19:55:18,956 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-22 19:55:27,752 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov22_19-55-22_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-22 19:55:27,756 INFO    MainThread:26724 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001C8F65066D0>>
2024-11-22 19:55:27,756 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-22 19:55:31,560 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:55:31,561 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:55:31,595 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:55:32,236 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:55:32,237 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:55:32,250 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:55:32,251 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:55:32,251 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:55:32,303 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:55:32,315 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:55:32,315 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 19:55:32,374 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 19:55:32,401 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 19:55:32,401 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:00:40,939 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:00:41,045 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:00:41,045 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:00:41,072 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:00:41,075 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:00:41,076 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:00:41,112 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:00:41,118 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:00:41,118 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:00:41,138 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:00:46,566 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:00:46,566 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:00:46,606 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:00:46,935 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:00:46,936 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:00:46,962 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:00:47,603 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:00:47,603 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:00:47,620 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:00:47,622 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:00:47,623 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:00:47,638 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:00:47,639 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:00:47,639 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:00:47,657 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:00:47,738 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:00:47,739 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:00:47,763 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:00:53,227 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov22_20-00-48_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-22 20:00:53,230 INFO    MainThread:26724 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001C8F65066D0>>
2024-11-22 20:00:53,230 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-22 20:00:56,373 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:00:56,373 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:01:05,834 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:01:05,939 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:01:05,939 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:01:05,967 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:01:05,971 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:01:05,971 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:01:06,007 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:01:06,014 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:01:06,014 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:01:06,054 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:01:10,714 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:01:10,714 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:01:10,746 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:01:11,014 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:01:11,015 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:01:11,032 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:01:11,648 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:01:11,648 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:01:11,671 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:01:11,674 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:01:11,674 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:01:11,692 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:01:11,696 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:01:11,696 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:01:11,713 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:01:11,819 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:01:11,819 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:01:11,876 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:01:17,298 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov22_20-01-12_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-22 20:01:17,300 INFO    MainThread:26724 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001C8F65066D0>>
2024-11-22 20:01:17,300 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-22 20:01:27,911 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov22_20-01-20_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-22 20:01:27,914 INFO    MainThread:26724 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001C8F65066D0>>
2024-11-22 20:01:27,915 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-22 20:01:39,042 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov22_20-01-34_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-22 20:01:39,046 INFO    MainThread:26724 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001C8F65066D0>>
2024-11-22 20:01:39,046 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-22 20:01:44,269 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:01:44,269 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:05:18,358 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:05:18,964 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:05:18,972 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:05:19,063 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:05:19,078 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:05:19,082 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:05:19,132 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:05:19,148 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:05:19,148 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:05:19,203 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:05:19,222 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:05:19,222 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:05:19,271 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:05:19,290 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:05:19,290 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:05:19,341 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:05:19,360 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:05:19,360 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:05:19,413 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:05:19,426 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:05:19,426 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:05:19,486 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:05:19,501 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:05:19,509 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:05:19,609 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:05:19,633 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:05:19,633 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:05:19,731 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:05:19,746 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:05:19,747 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:05:19,812 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:05:19,828 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:05:19,828 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:05:19,898 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:05:19,920 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:05:19,921 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:05:26,076 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:05:26,326 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:05:26,326 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:06:28,449 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:06:28,614 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:06:28,614 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:06:28,658 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:06:28,662 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:06:28,663 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:06:28,688 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:06:28,705 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:06:28,705 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:06:28,731 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:06:28,843 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:06:28,843 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:07:05,442 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:07:05,448 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:07:05,448 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:07:05,586 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:07:05,590 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:07:05,591 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:07:05,753 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:07:05,758 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:07:05,759 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:07:05,940 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:07:05,947 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:07:05,948 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:07:09,112 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:07:09,263 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:07:09,263 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:07:09,328 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:07:09,332 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:07:09,332 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:07:09,347 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:07:09,355 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:07:09,355 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:07:09,376 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:07:09,654 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:07:09,654 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:07:22,229 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:07:22,341 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:07:22,341 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:07:22,360 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:07:22,363 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:07:22,363 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:07:22,388 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:07:22,394 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:07:22,394 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:07:22,416 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:07:22,606 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:07:22,606 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:08:03,755 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:08:03,852 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:08:03,852 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:08:03,873 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:08:03,876 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:08:03,876 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:08:03,904 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:08:03,910 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:08:03,910 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:08:03,937 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:08:04,316 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:08:04,317 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:08:14,264 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:08:14,379 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:08:14,379 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:08:14,434 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:08:14,436 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:08:14,436 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:08:14,458 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:08:14,466 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:08:14,466 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:08:14,487 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:08:14,680 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:08:14,680 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:08:42,664 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:08:42,669 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:08:42,670 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:08:50,085 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:08:50,169 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:08:50,171 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:08:50,191 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:08:50,194 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:08:50,194 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:08:50,219 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:08:50,224 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:08:50,224 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:08:50,249 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:08:50,459 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:08:50,459 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:08:55,672 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:08:55,681 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:08:55,681 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:08:55,727 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:08:55,735 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:08:55,735 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:08:58,063 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:08:58,147 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:08:58,147 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:08:58,166 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:08:58,169 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:08:58,169 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:08:58,199 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:08:58,210 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:08:58,210 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:08:58,255 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:08:59,997 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:08:59,997 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:09:00,066 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:09:00,139 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:09:00,139 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:09:00,202 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:09:00,561 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:09:00,561 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:09:00,583 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:09:00,586 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:09:00,586 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:09:00,606 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:09:00,608 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:09:00,608 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:09:00,622 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:09:00,812 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:09:00,813 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:09:00,837 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:09:02,182 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov22_20-09-01_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-22 20:09:02,186 INFO    MainThread:26724 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001C8F65066D0>>
2024-11-22 20:09:02,186 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-22 20:09:02,289 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:09:02,289 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:09:23,041 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:09:23,202 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:09:23,202 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:09:23,249 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:09:23,250 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:09:23,250 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:09:23,270 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:09:23,279 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:09:23,279 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:09:23,294 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:09:24,145 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:09:24,145 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:09:24,174 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:09:24,205 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:09:24,205 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:09:24,316 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:09:24,707 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:09:24,707 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:09:24,736 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:09:24,739 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:09:24,739 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:09:24,778 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:09:24,780 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:09:24,780 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:09:24,834 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:09:25,056 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:09:25,056 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:09:25,083 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:09:25,772 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov22_20-09-25_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-22 20:09:25,775 INFO    MainThread:26724 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001C8F65066D0>>
2024-11-22 20:09:25,775 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-22 20:09:25,955 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:09:25,955 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:09:45,694 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:09:45,789 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:09:45,789 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:09:45,819 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:09:45,822 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:09:45,822 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:09:45,839 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:09:45,845 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:09:45,845 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:09:45,896 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:09:48,822 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:09:48,823 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:09:48,854 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:09:48,884 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:09:48,884 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:09:48,917 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:09:49,172 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:09:49,172 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:09:49,187 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:09:49,189 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:09:49,189 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:09:49,204 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:09:49,205 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:09:49,205 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:09:49,227 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:09:49,428 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:09:49,428 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:09:49,443 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:09:50,198 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov22_20-09-50_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-22 20:09:50,204 INFO    MainThread:26724 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001C8F65066D0>>
2024-11-22 20:09:50,204 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-22 20:09:50,320 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:09:50,320 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:10:21,559 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:10:21,686 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:10:21,687 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:10:21,729 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:10:21,733 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:10:21,733 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:10:21,759 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:10:21,765 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:10:21,766 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:10:21,781 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:10:29,580 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:10:29,580 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:10:29,639 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:10:29,673 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:10:29,673 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:10:29,701 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:10:29,962 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:10:29,962 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:10:29,976 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:10:29,978 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:10:29,978 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:10:29,997 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:10:29,998 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:10:29,998 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:10:30,017 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:10:30,248 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:10:30,248 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:10:30,269 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:10:31,053 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov22_20-10-31_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-22 20:10:31,057 INFO    MainThread:26724 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001C8F65066D0>>
2024-11-22 20:10:31,057 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-22 20:10:31,178 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:10:31,178 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:10:51,960 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:10:52,781 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb None None {'num_input_channels': 1, 'context_length': 512, 'patch_length': 64, 'expansion_factor': 2, 'num_layers': 2, 'dropout': 0.2, 'mode': 'common_channel', 'gated_attn': True, 'norm_mlp': 'LayerNorm', 'scaling': 'std', 'head_dropout': 0.2, 'patch_last': True, 'use_positional_encoding': False, 'positional_encoding_type': 'sincos', 'prediction_length': 96, 'prediction_channel_indices': None, 'self_attn': False, 'self_attn_heads': 1, 'init_std': 0.02, 'post_init': False, 'distribution_output': 'student_t', 'loss': 'mse', 'num_parallel_samples': 100, 'norm_eps': 1e-05, 'use_decoder': True, 'adaptive_patching_levels': 3, 'resolution_prefix_tuning': False, 'exogenous_channel_indices': None, 'decoder_num_layers': 2, 'decoder_adaptive_patching_levels': 0, 'decoder_raw_residual': False, 'decoder_mode': 'common_channel', 'fcm_gated_attn': True, 'fcm_context_length': 1, 'fcm_use_mixer': False, 'fcm_mix_layers': 2, 'fcm_prepend_past': True, 'fcm_prepend_past_offset': None, 'enable_forecast_channel_mixing': False, 'frequency_token_vocab_size': 5, 'd_model': 192, 'patch_stride': 64, 'decoder_d_model': 128, 'categorical_vocab_size_list': None, 'init_processing': True, 'prediction_filter_length': 24, 'init_linear': 'pytorch', 'init_embed': 'pytorch', 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['TinyTimeMixerForPrediction'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'ibm/TTM', '_attn_implementation_autoset': True, 'transformers_version': '4.46.1', 'model_type': 'tinytimemixer', 'num_patches': 8, 'output_dir': 'tmp_trainer', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'tmp_trainer\\runs\\Nov22_20-10-52_A-LPTP-AWRmmVlZ', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'tmp_trainer', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False}
2024-11-22 20:10:52,785 INFO    MainThread:26724 [wandb_config.py:__setitem__():154] config set model/num_parameters = 805280 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x000001C8F65066D0>>
2024-11-22 20:10:52,785 INFO    MainThread:26724 [wandb_run.py:_config_callback():1389] config_cb model/num_parameters 805280 None
2024-11-22 20:10:52,938 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:10:52,938 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:16:18,006 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:16:20,032 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:16:20,032 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:16:27,098 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:16:29,017 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:16:29,017 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:17:25,659 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:17:25,666 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:17:25,666 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:17:31,202 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:17:52,462 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:17:52,462 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:20:22,216 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:20:22,357 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:20:22,358 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:20:22,374 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:20:22,377 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:20:22,377 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:20:22,398 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:20:22,450 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:20:22,450 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:20:22,473 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:20:26,898 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:20:26,898 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:20:26,917 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:20:27,115 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:20:27,115 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:20:27,141 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:20:27,626 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:20:27,626 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:20:27,641 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:20:27,643 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:20:27,643 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:20:27,658 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:20:27,659 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:20:27,659 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:20:27,680 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:20:27,764 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:20:27,764 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
2024-11-22 20:20:27,796 INFO    MainThread:26724 [wandb_init.py:_resume_backend():449] resuming backend
2024-11-22 20:20:34,174 INFO    MainThread:26724 [jupyter.py:save_ipynb():387] not saving jupyter notebook
2024-11-22 20:20:34,174 INFO    MainThread:26724 [wandb_init.py:_pause_backend():444] pausing backend
