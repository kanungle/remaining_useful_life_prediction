{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25dc6926-0dfa-41ff-8854-fdffa77117bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d3867f0-613c-4e8a-87e6-e96621e4f89d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Train Data Shape: (17731, 30, 14)\n",
      "Processed Train Target Shape: (17731,)\n",
      "Processed Test Data Shape: (100, 30, 14)\n",
      "True RUL Shape: (100,)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "# define the directory where the pickle files are stored\n",
    "folder_path = '../batched_data_pickle_files/'\n",
    "\n",
    "# define the filenames for the pickle files\n",
    "file_names = ['processed_train_data.pkl', 'processed_train_targets.pkl', 'processed_test_data.pkl', 'true_rul.pkl']\n",
    "\n",
    "# loop through each file and load its contents as arrays\n",
    "for file_name in file_names:\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    \n",
    "    # read the pickle file\n",
    "    with open(file_path, 'rb') as file:\n",
    "        data = pickle.load(file)\n",
    "    \n",
    "    # ensure data is a numpy array, if not convert it\n",
    "    if isinstance(data, np.ndarray):\n",
    "        globals()[file_name.replace('.pkl', '')] = data\n",
    "    else:\n",
    "        globals()[file_name.replace('.pkl', '')] = np.array(data)\n",
    "\n",
    "print(\"Processed Train Data Shape:\", processed_train_data.shape)\n",
    "print(\"Processed Train Target Shape:\", processed_train_targets.shape)\n",
    "print(\"Processed Test Data Shape:\", processed_test_data.shape)\n",
    "print(\"True RUL Shape:\", true_rul.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "deb54693-b6f6-4189-88e6-91de8a0470b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = processed_train_data\n",
    "y = processed_train_targets\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_tensor = torch.FloatTensor(X)\n",
    "y_tensor = torch.FloatTensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c35acb4-aa0b-45e7-b75c-af3962f06009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "val_dataset = TensorDataset(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "327f4e3d-42e1-4eb1-b971-a42a4e2db834",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-21 07:14:31,254] A new study created in memory with name: no-name-bfeaf0e5-a313-4091-b218-438f36625d08\n",
      "/tmp/ipykernel_9994/934372295.py:32: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)  # Learning rate\n",
      "[I 2024-11-21 07:15:39,638] Trial 0 finished with value: 18.26929014223116 and parameters: {'hidden_size': 128, 'num_layers': 1, 'learning_rate': 0.0034954850746481783, 'batch_size': 32}. Best is trial 0 with value: 18.26929014223116.\n",
      "[I 2024-11-21 07:19:11,973] Trial 1 finished with value: 81.99670818873814 and parameters: {'hidden_size': 128, 'num_layers': 3, 'learning_rate': 1.2913086955327018e-05, 'batch_size': 128}. Best is trial 0 with value: 18.26929014223116.\n",
      "[I 2024-11-21 07:20:07,505] Trial 2 finished with value: 81.99715151105609 and parameters: {'hidden_size': 32, 'num_layers': 3, 'learning_rate': 8.4153976329118e-05, 'batch_size': 128}. Best is trial 0 with value: 18.26929014223116.\n",
      "[I 2024-11-21 07:29:03,564] Trial 3 finished with value: 50.597114426749094 and parameters: {'hidden_size': 128, 'num_layers': 3, 'learning_rate': 0.00039970812321555644, 'batch_size': 64}. Best is trial 0 with value: 18.26929014223116.\n",
      "[I 2024-11-21 07:29:50,160] Trial 4 finished with value: 70.27797753470284 and parameters: {'hidden_size': 64, 'num_layers': 1, 'learning_rate': 7.141529949888368e-05, 'batch_size': 64}. Best is trial 0 with value: 18.26929014223116.\n",
      "[I 2024-11-21 07:36:52,542] Trial 5 finished with value: 17.645047282313442 and parameters: {'hidden_size': 128, 'num_layers': 3, 'learning_rate': 0.0001206687008047001, 'batch_size': 32}. Best is trial 5 with value: 17.645047282313442.\n",
      "[I 2024-11-21 07:38:19,635] Trial 6 finished with value: 17.949977108410426 and parameters: {'hidden_size': 32, 'num_layers': 3, 'learning_rate': 0.0023079985287492904, 'batch_size': 64}. Best is trial 5 with value: 17.645047282313442.\n",
      "[I 2024-11-21 07:43:46,776] Trial 7 finished with value: 77.13022586277553 and parameters: {'hidden_size': 128, 'num_layers': 3, 'learning_rate': 1.623259116355229e-05, 'batch_size': 64}. Best is trial 5 with value: 17.645047282313442.\n",
      "[I 2024-11-21 07:46:21,950] Trial 8 finished with value: 20.39316850095182 and parameters: {'hidden_size': 128, 'num_layers': 2, 'learning_rate': 0.0046546132872746625, 'batch_size': 32}. Best is trial 5 with value: 17.645047282313442.\n",
      "[I 2024-11-21 07:47:35,567] Trial 9 finished with value: 83.65900938851493 and parameters: {'hidden_size': 64, 'num_layers': 2, 'learning_rate': 2.4670463977309012e-05, 'batch_size': 128}. Best is trial 5 with value: 17.645047282313442.\n",
      "[I 2024-11-21 07:49:43,138] Trial 10 finished with value: 23.597743936487145 and parameters: {'hidden_size': 96, 'num_layers': 2, 'learning_rate': 0.0004137006128711413, 'batch_size': 96}. Best is trial 5 with value: 17.645047282313442.\n",
      "[I 2024-11-21 07:51:56,018] Trial 11 finished with value: 14.427471349905202 and parameters: {'hidden_size': 32, 'num_layers': 3, 'learning_rate': 0.0008968894969160791, 'batch_size': 32}. Best is trial 11 with value: 14.427471349905202.\n",
      "[I 2024-11-21 07:56:06,703] Trial 12 finished with value: 17.620343672262656 and parameters: {'hidden_size': 96, 'num_layers': 3, 'learning_rate': 0.001068664952566299, 'batch_size': 32}. Best is trial 11 with value: 14.427471349905202.\n",
      "[I 2024-11-21 07:58:13,043] Trial 13 finished with value: 17.594926851289767 and parameters: {'hidden_size': 96, 'num_layers': 2, 'learning_rate': 0.0011438014661926264, 'batch_size': 32}. Best is trial 11 with value: 14.427471349905202.\n",
      "[I 2024-11-21 07:59:24,994] Trial 14 finished with value: 18.404775207107132 and parameters: {'hidden_size': 64, 'num_layers': 2, 'learning_rate': 0.0009519192397393097, 'batch_size': 96}. Best is trial 11 with value: 14.427471349905202.\n",
      "[I 2024-11-21 08:00:29,511] Trial 15 finished with value: 17.879953367216093 and parameters: {'hidden_size': 96, 'num_layers': 1, 'learning_rate': 0.0014226107959127285, 'batch_size': 32}. Best is trial 11 with value: 14.427471349905202.\n",
      "[I 2024-11-21 08:01:57,261] Trial 16 finished with value: 18.23255302669766 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.006871417103432853, 'batch_size': 32}. Best is trial 11 with value: 14.427471349905202.\n",
      "[I 2024-11-21 08:03:14,385] Trial 17 finished with value: 19.45039326435811 and parameters: {'hidden_size': 64, 'num_layers': 2, 'learning_rate': 0.0006048369597145335, 'batch_size': 96}. Best is trial 11 with value: 14.427471349905202.\n",
      "[I 2024-11-21 08:04:19,659] Trial 18 finished with value: 28.088258232389176 and parameters: {'hidden_size': 96, 'num_layers': 1, 'learning_rate': 0.00020057041950444714, 'batch_size': 64}. Best is trial 11 with value: 14.427471349905202.\n",
      "[I 2024-11-21 08:05:46,101] Trial 19 finished with value: 16.781884485536867 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.002239813619350319, 'batch_size': 32}. Best is trial 11 with value: 14.427471349905202.\n",
      "[I 2024-11-21 08:06:37,718] Trial 20 finished with value: 18.854682530675614 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.007853533206682549, 'batch_size': 64}. Best is trial 11 with value: 14.427471349905202.\n",
      "[I 2024-11-21 08:08:03,157] Trial 21 finished with value: 17.418994473981428 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.0026588804106187774, 'batch_size': 32}. Best is trial 11 with value: 14.427471349905202.\n",
      "[I 2024-11-21 08:09:28,669] Trial 22 finished with value: 17.901973028440732 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.0024079773140732643, 'batch_size': 32}. Best is trial 11 with value: 14.427471349905202.\n",
      "[I 2024-11-21 08:10:54,548] Trial 23 finished with value: 16.255720499399548 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.0021242947948904222, 'batch_size': 32}. Best is trial 11 with value: 14.427471349905202.\n",
      "[I 2024-11-21 08:11:45,619] Trial 24 finished with value: 17.97293422458408 and parameters: {'hidden_size': 32, 'num_layers': 1, 'learning_rate': 0.0017888664387461519, 'batch_size': 32}. Best is trial 11 with value: 14.427471349905202.\n",
      "[I 2024-11-21 08:13:10,285] Trial 25 finished with value: 13.188297646386284 and parameters: {'hidden_size': 64, 'num_layers': 2, 'learning_rate': 0.0007873570161414351, 'batch_size': 64}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 08:14:34,552] Trial 26 finished with value: 18.129638280187333 and parameters: {'hidden_size': 64, 'num_layers': 2, 'learning_rate': 0.0006525603485842986, 'batch_size': 64}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 08:17:40,129] Trial 27 finished with value: 50.39444371816274 and parameters: {'hidden_size': 64, 'num_layers': 3, 'learning_rate': 0.00026159809651673876, 'batch_size': 96}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 08:20:55,069] Trial 28 finished with value: 38.871063573019846 and parameters: {'hidden_size': 64, 'num_layers': 3, 'learning_rate': 0.0005729939153738538, 'batch_size': 64}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 08:21:44,922] Trial 29 finished with value: 18.111502509933334 and parameters: {'hidden_size': 32, 'num_layers': 1, 'learning_rate': 0.004420067376772651, 'batch_size': 32}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 08:22:31,014] Trial 30 finished with value: 55.09934445789882 and parameters: {'hidden_size': 64, 'num_layers': 1, 'learning_rate': 0.00018816218217644309, 'batch_size': 64}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 08:23:56,656] Trial 31 finished with value: 17.266466707796663 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.0035791588302249524, 'batch_size': 32}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 08:25:28,365] Trial 32 finished with value: 13.536478845922797 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.00086202058540801, 'batch_size': 32}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 08:27:00,515] Trial 33 finished with value: 13.246066196544751 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.0008875019227554725, 'batch_size': 32}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 08:28:35,904] Trial 34 finished with value: 17.426677214132773 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.00042071304289740646, 'batch_size': 32}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 08:30:20,873] Trial 35 finished with value: 41.82448761803763 and parameters: {'hidden_size': 32, 'num_layers': 3, 'learning_rate': 0.0008363402706242494, 'batch_size': 64}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 08:32:23,674] Trial 36 finished with value: 17.63049621410198 and parameters: {'hidden_size': 64, 'num_layers': 2, 'learning_rate': 0.0003431723992402962, 'batch_size': 32}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 08:33:03,341] Trial 37 finished with value: 26.353507791246688 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.0014517346675774205, 'batch_size': 128}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 08:34:42,345] Trial 38 finished with value: 74.29088456290108 and parameters: {'hidden_size': 32, 'num_layers': 3, 'learning_rate': 0.00010441153924133874, 'batch_size': 64}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 08:36:29,764] Trial 39 finished with value: 73.9866589137486 and parameters: {'hidden_size': 64, 'num_layers': 2, 'learning_rate': 4.877430775246577e-05, 'batch_size': 64}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 08:39:01,154] Trial 40 finished with value: 18.389854757635444 and parameters: {'hidden_size': 32, 'num_layers': 3, 'learning_rate': 0.0006638512253549368, 'batch_size': 32}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 08:40:29,132] Trial 41 finished with value: 16.764531384717237 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.0015871437607595828, 'batch_size': 32}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 08:41:59,010] Trial 42 finished with value: 16.711961127616263 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.0008873007807780179, 'batch_size': 32}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 08:43:30,560] Trial 43 finished with value: 14.329900801718772 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.0005419737240553957, 'batch_size': 32}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 08:45:04,667] Trial 44 finished with value: 13.657247062201973 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.0004885962737128061, 'batch_size': 32}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 08:46:41,380] Trial 45 finished with value: 34.014014905637445 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.0002400523453680341, 'batch_size': 32}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 08:48:42,375] Trial 46 finished with value: 16.65467732231896 and parameters: {'hidden_size': 64, 'num_layers': 2, 'learning_rate': 0.0004534550065723328, 'batch_size': 32}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 08:49:41,425] Trial 47 finished with value: 33.61631778308323 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.0004943915348271962, 'batch_size': 64}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 08:51:01,263] Trial 48 finished with value: 47.76932195715002 and parameters: {'hidden_size': 64, 'num_layers': 2, 'learning_rate': 0.00035719425106240255, 'batch_size': 96}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 08:52:38,474] Trial 49 finished with value: 56.24463557337855 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.00014759816829813932, 'batch_size': 32}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 08:55:04,189] Trial 50 finished with value: 18.254117037798906 and parameters: {'hidden_size': 128, 'num_layers': 2, 'learning_rate': 0.0013148803599334793, 'batch_size': 32}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 08:56:35,278] Trial 51 finished with value: 13.846786730998272 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.0007557028904506299, 'batch_size': 32}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 08:58:06,451] Trial 52 finished with value: 16.582150871689255 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.0007644029045854584, 'batch_size': 32}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 08:59:36,350] Trial 53 finished with value: 17.22699313120799 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.0010824266999393231, 'batch_size': 32}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 09:01:13,132] Trial 54 finished with value: 27.86729754198779 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.0002871938997422179, 'batch_size': 32}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 09:02:47,785] Trial 55 finished with value: 17.757658107860667 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.000500855079533416, 'batch_size': 32}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 09:04:32,655] Trial 56 finished with value: 17.525203193937028 and parameters: {'hidden_size': 96, 'num_layers': 2, 'learning_rate': 0.0010911093484253728, 'batch_size': 128}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 09:05:33,228] Trial 57 finished with value: 41.26214006968907 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.0006937226458317873, 'batch_size': 64}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 09:07:00,947] Trial 58 finished with value: 17.028583964786016 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.0018023986407908529, 'batch_size': 32}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 09:09:24,536] Trial 59 finished with value: 80.89869793041332 and parameters: {'hidden_size': 64, 'num_layers': 2, 'learning_rate': 1.1694961271805489e-05, 'batch_size': 32}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 09:10:23,997] Trial 60 finished with value: 29.882393223898752 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.000535755868699553, 'batch_size': 64}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 09:11:54,640] Trial 61 finished with value: 17.285234915243613 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.0008964812228662447, 'batch_size': 32}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 09:13:23,603] Trial 62 finished with value: 16.84856313413328 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.001310881638396012, 'batch_size': 32}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 09:16:03,007] Trial 63 finished with value: 18.193621910370148 and parameters: {'hidden_size': 32, 'num_layers': 3, 'learning_rate': 0.0007592162019229872, 'batch_size': 32}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 09:17:38,712] Trial 64 finished with value: 19.3151019843849 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.0003826804383221111, 'batch_size': 32}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 09:19:25,216] Trial 65 finished with value: 18.414492521200096 and parameters: {'hidden_size': 64, 'num_layers': 2, 'learning_rate': 0.0028000097093427534, 'batch_size': 32}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 09:20:17,738] Trial 66 finished with value: 17.831219217798733 and parameters: {'hidden_size': 32, 'num_layers': 1, 'learning_rate': 0.0010078211038441194, 'batch_size': 32}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 09:21:03,300] Trial 67 finished with value: 46.942739950644004 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.0006128286448163534, 'batch_size': 96}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 09:41:55,191] Trial 68 finished with value: 41.785654978709175 and parameters: {'hidden_size': 96, 'num_layers': 3, 'learning_rate': 0.0003063764765547167, 'batch_size': 32}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 09:45:51,293] Trial 69 finished with value: 16.649433393736143 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.0018296510246695625, 'batch_size': 32}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 09:50:24,223] Trial 70 finished with value: 62.23601382119315 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.0002102280814717856, 'batch_size': 64}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 09:55:05,456] Trial 71 finished with value: 16.08829539530986 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.002061560256978783, 'batch_size': 32}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 09:58:13,355] Trial 72 finished with value: 17.079998772423547 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.0038129532449024837, 'batch_size': 32}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 10:01:30,926] Trial 73 finished with value: 17.958008018699854 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.002924525898688821, 'batch_size': 32}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 10:05:42,421] Trial 74 finished with value: 18.549395483893317 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.006172361325715448, 'batch_size': 32}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 10:10:21,077] Trial 75 finished with value: 18.30234653026134 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.0012319932127727638, 'batch_size': 32}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 10:14:41,560] Trial 76 finished with value: 17.156109569308995 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.0008276950562989556, 'batch_size': 32}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 10:18:11,257] Trial 77 finished with value: 18.33171104740452 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.0004219473532445999, 'batch_size': 32}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 10:25:15,010] Trial 78 finished with value: 18.293320810472643 and parameters: {'hidden_size': 32, 'num_layers': 3, 'learning_rate': 0.0005630615027381257, 'batch_size': 32}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 10:29:19,102] Trial 79 finished with value: 17.40306762286595 and parameters: {'hidden_size': 64, 'num_layers': 2, 'learning_rate': 0.0019447907444251524, 'batch_size': 64}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 10:32:30,497] Trial 80 finished with value: 13.53264332676793 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.001469514104503506, 'batch_size': 32}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 10:36:04,126] Trial 81 finished with value: 16.936936928345276 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.0014711542767368844, 'batch_size': 32}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 10:40:41,914] Trial 82 finished with value: 14.239687747783488 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.0009138834315944984, 'batch_size': 32}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 10:45:15,784] Trial 83 finished with value: 18.35338289243681 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.0010167745615747496, 'batch_size': 32}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 10:49:02,133] Trial 84 finished with value: 13.908407280036995 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.00071270356941159, 'batch_size': 32}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 10:52:23,587] Trial 85 finished with value: 17.978851687800777 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.0006623766815724807, 'batch_size': 32}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 10:56:04,830] Trial 86 finished with value: 17.078854775643563 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.0004937816140558831, 'batch_size': 32}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 11:00:41,740] Trial 87 finished with value: 16.80024459770134 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.0007433086895911476, 'batch_size': 32}. Best is trial 25 with value: 13.188297646386284.\n",
      "[I 2024-11-21 11:05:19,736] Trial 88 finished with value: 12.834748358339876 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.0011843203255513526, 'batch_size': 32}. Best is trial 88 with value: 12.834748358339876.\n",
      "[I 2024-11-21 11:09:11,079] Trial 89 finished with value: 18.29032701200193 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.0015448313947718122, 'batch_size': 32}. Best is trial 88 with value: 12.834748358339876.\n",
      "[I 2024-11-21 11:15:25,754] Trial 90 finished with value: 18.437728684227746 and parameters: {'hidden_size': 64, 'num_layers': 2, 'learning_rate': 0.0011962566226361186, 'batch_size': 32}. Best is trial 88 with value: 12.834748358339876.\n",
      "[I 2024-11-21 11:20:08,026] Trial 91 finished with value: 16.46591075691017 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.0009987879437785335, 'batch_size': 32}. Best is trial 88 with value: 12.834748358339876.\n",
      "[I 2024-11-21 11:23:21,594] Trial 92 finished with value: 16.551094158275706 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.0008779987276004864, 'batch_size': 32}. Best is trial 88 with value: 12.834748358339876.\n",
      "[I 2024-11-21 11:26:43,468] Trial 93 finished with value: 17.754048081131668 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.0006335740981650382, 'batch_size': 32}. Best is trial 88 with value: 12.834748358339876.\n",
      "[I 2024-11-21 11:30:54,687] Trial 94 finished with value: 17.638701206928975 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.0004450921085714084, 'batch_size': 32}. Best is trial 88 with value: 12.834748358339876.\n",
      "[I 2024-11-21 11:40:28,203] Trial 95 finished with value: 18.063094542907166 and parameters: {'hidden_size': 128, 'num_layers': 2, 'learning_rate': 0.0007376231694127611, 'batch_size': 32}. Best is trial 88 with value: 12.834748358339876.\n",
      "[I 2024-11-21 11:45:15,730] Trial 96 finished with value: 16.909656902691264 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.0005551123936856655, 'batch_size': 32}. Best is trial 88 with value: 12.834748358339876.\n",
      "[I 2024-11-21 11:51:21,412] Trial 97 finished with value: 18.44897043382799 and parameters: {'hidden_size': 96, 'num_layers': 2, 'learning_rate': 0.0011600887591107136, 'batch_size': 32}. Best is trial 88 with value: 12.834748358339876.\n",
      "[I 2024-11-21 11:55:36,144] Trial 98 finished with value: 83.9870410646711 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 2.739950929039685e-05, 'batch_size': 64}. Best is trial 88 with value: 12.834748358339876.\n",
      "[I 2024-11-21 11:59:27,117] Trial 99 finished with value: 43.426560582341374 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.0008722134632950634, 'batch_size': 96}. Best is trial 88 with value: 12.834748358339876.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.0011843203255513526, 'batch_size': 32}\n",
      "Best Validation RMSE: 12.834748358339876\n",
      "Epoch [1/20], Validation Loss: 72.3708\n",
      "Epoch [2/20], Validation Loss: 65.1407\n",
      "Epoch [3/20], Validation Loss: 56.8781\n",
      "Epoch [4/20], Validation Loss: 48.4448\n",
      "Epoch [5/20], Validation Loss: 42.7630\n",
      "Epoch [6/20], Validation Loss: 41.8939\n",
      "Epoch [7/20], Validation Loss: 41.7637\n",
      "Epoch [8/20], Validation Loss: 41.7608\n",
      "Epoch [9/20], Validation Loss: 40.0594\n",
      "Epoch [10/20], Validation Loss: 20.2189\n",
      "Epoch [11/20], Validation Loss: 18.4146\n",
      "Epoch [12/20], Validation Loss: 18.4378\n",
      "Epoch [13/20], Validation Loss: 18.0692\n",
      "Epoch [14/20], Validation Loss: 18.1642\n",
      "Epoch [15/20], Validation Loss: 17.8939\n",
      "Epoch [16/20], Validation Loss: 17.6741\n",
      "Epoch [17/20], Validation Loss: 17.7719\n",
      "Epoch [18/20], Validation Loss: 17.1611\n",
      "Epoch [19/20], Validation Loss: 16.8547\n",
      "Epoch [20/20], Validation Loss: 16.9993\n",
      "Best RNN model saved to 'best_rnn_model_optuna.pth'\n",
      "Test RMSE: 18.9342\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import optuna\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "# RMSE loss function\n",
    "def rmse_loss(predictions, targets):\n",
    "    return torch.sqrt(nn.MSELoss()(predictions, targets))\n",
    "\n",
    "# Define the RNN model\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# Define the objective function for Optuna\n",
    "def objective(trial):\n",
    "    # Hyperparameter suggestions by Optuna\n",
    "    hidden_size = trial.suggest_int('hidden_size', 32, 128, step=32)  # Hidden units\n",
    "    num_layers = trial.suggest_int('num_layers', 1, 3)  # Number of RNN layers\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)  # Learning rate\n",
    "    batch_size = trial.suggest_int('batch_size', 32, 128, step=32)  # Batch size\n",
    "    \n",
    "    # Initialize the model with the suggested hyperparameters\n",
    "    model = RNNModel(input_size=X.shape[2], hidden_size=hidden_size, num_layers=num_layers, output_size=1)\n",
    "    \n",
    "    # Define loss function and optimizer\n",
    "    criterion = rmse_loss\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Create DataLoaders with the suggested batch size\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Training loop\n",
    "    num_epochs = 20  # You can adjust the number of epochs here\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs.squeeze(), batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_losses = []\n",
    "        for batch_X, batch_y in val_loader:\n",
    "            outputs = model(batch_X)\n",
    "            val_loss = criterion(outputs.squeeze(), batch_y)\n",
    "            val_losses.append(val_loss.item())\n",
    "        avg_val_loss = np.mean(val_losses)\n",
    "    \n",
    "    return avg_val_loss  # Return validation loss\n",
    "\n",
    "# Initialize Optuna study for hyperparameter optimization\n",
    "study = optuna.create_study(direction='minimize')  # We want to minimize the validation loss\n",
    "study.optimize(objective, n_trials=100)  # Run 100 trials for hyperparameter search\n",
    "\n",
    "# Print the best hyperparameters found by Optuna\n",
    "print(\"Best Hyperparameters:\", study.best_params)\n",
    "print(\"Best Validation RMSE:\", study.best_value)\n",
    "\n",
    "# Extract the best hyperparameters from the best trial\n",
    "best_trial = study.best_trial\n",
    "best_hidden_size = best_trial.params['hidden_size']\n",
    "best_num_layers = best_trial.params['num_layers']\n",
    "best_learning_rate = best_trial.params['learning_rate']\n",
    "best_batch_size = best_trial.params['batch_size']\n",
    "\n",
    "# Instantiate the model with the best hyperparameters\n",
    "best_model = RNNModel(input_size=X.shape[2], hidden_size=best_hidden_size, num_layers=best_num_layers, output_size=1)\n",
    "\n",
    "# Define optimizer and loss function\n",
    "optimizer = optim.Adam(best_model.parameters(), lr=best_learning_rate)\n",
    "criterion = rmse_loss\n",
    "\n",
    "# Create DataLoaders with the best batch size\n",
    "train_loader = DataLoader(train_dataset, batch_size=best_batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=best_batch_size, shuffle=False)\n",
    "\n",
    "# Final model training with the best hyperparameters\n",
    "num_epochs = 20  # You can increase this as needed\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "best_model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    best_model.train()\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        batch_X = batch_X.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "        outputs = best_model(batch_X)\n",
    "        loss = criterion(outputs.squeeze(), batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validation loop\n",
    "    best_model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_losses = []\n",
    "        for batch_X, batch_y in val_loader:\n",
    "            batch_X = batch_X.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            outputs = best_model(batch_X)\n",
    "            val_loss = criterion(outputs.squeeze(), batch_y)\n",
    "            val_losses.append(val_loss.item())\n",
    "        avg_val_loss = np.mean(val_losses)\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Validation Loss: {avg_val_loss:.4f}')\n",
    "\n",
    "# Save the best model to a file\n",
    "torch.save(best_model.state_dict(), 'best_rnn_model_optuna.pth')\n",
    "print(\"Best RNN model saved to 'best_rnn_model_optuna.pth'\")\n",
    "\n",
    "# Evaluate on test data (optional)\n",
    "best_model.eval()\n",
    "test_tensor = torch.FloatTensor(processed_test_data).to(device)\n",
    "with torch.no_grad():\n",
    "    test_predictions = best_model(test_tensor).numpy().squeeze()\n",
    "\n",
    "# Calculate RMSE on test data\n",
    "test_rmse = np.sqrt(np.mean((test_predictions - true_rul)**2))\n",
    "print(f'Test RMSE: {test_rmse:.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5dc0da4a-03ad-4569-a44e-9066804b61d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Validation Loss: 72.8571\n",
      "Epoch [2/30], Validation Loss: 60.1122\n",
      "Epoch [3/30], Validation Loss: 50.5848\n",
      "Epoch [4/30], Validation Loss: 44.9487\n",
      "Epoch [5/30], Validation Loss: 42.5667\n",
      "Epoch [6/30], Validation Loss: 41.8887\n",
      "Epoch [7/30], Validation Loss: 41.7864\n",
      "Epoch [8/30], Validation Loss: 22.7499\n",
      "Epoch [9/30], Validation Loss: 21.5080\n",
      "Epoch [10/30], Validation Loss: 18.3313\n",
      "Epoch [11/30], Validation Loss: 17.5140\n",
      "Epoch [12/30], Validation Loss: 17.0022\n",
      "Epoch [13/30], Validation Loss: 16.6556\n",
      "Epoch [14/30], Validation Loss: 16.9519\n",
      "Epoch [15/30], Validation Loss: 17.5586\n",
      "Epoch [16/30], Validation Loss: 16.4147\n",
      "Epoch [17/30], Validation Loss: 16.8203\n",
      "Epoch [18/30], Validation Loss: 16.9057\n",
      "Epoch [19/30], Validation Loss: 16.8761\n",
      "Epoch [20/30], Validation Loss: 16.3438\n",
      "Epoch [21/30], Validation Loss: 16.4679\n",
      "Epoch [22/30], Validation Loss: 16.4167\n",
      "Epoch [23/30], Validation Loss: 16.4471\n",
      "Epoch [24/30], Validation Loss: 19.6358\n",
      "Epoch [25/30], Validation Loss: 16.3047\n",
      "Epoch [26/30], Validation Loss: 16.0472\n",
      "Epoch [27/30], Validation Loss: 16.0645\n",
      "Epoch [28/30], Validation Loss: 17.3121\n",
      "Epoch [29/30], Validation Loss: 16.2764\n",
      "Epoch [30/30], Validation Loss: 16.3513\n",
      "Best RNN model saved to 'best_rnn_model_optuna.pth'\n",
      "Test RMSE: 18.0790\n"
     ]
    }
   ],
   "source": [
    "# Extract the best hyperparameters from the best trial\n",
    "best_trial = study.best_trial\n",
    "best_hidden_size = best_trial.params['hidden_size']\n",
    "best_num_layers = best_trial.params['num_layers']\n",
    "best_learning_rate = best_trial.params['learning_rate']\n",
    "best_batch_size = best_trial.params['batch_size']\n",
    "\n",
    "# Instantiate the model with the best hyperparameters\n",
    "best_model = RNNModel(input_size=X.shape[2], hidden_size=best_hidden_size, num_layers=best_num_layers, output_size=1)\n",
    "\n",
    "# Define optimizer and loss function\n",
    "optimizer = optim.Adam(best_model.parameters(), lr=best_learning_rate)\n",
    "criterion = rmse_loss\n",
    "\n",
    "# Create DataLoaders with the best batch size\n",
    "train_loader = DataLoader(train_dataset, batch_size=best_batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=best_batch_size, shuffle=False)\n",
    "\n",
    "# Final model training with the best hyperparameters\n",
    "num_epochs = 30  # You can increase this as needed\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "best_model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    best_model.train()\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        batch_X = batch_X.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "        outputs = best_model(batch_X)\n",
    "        loss = criterion(outputs.squeeze(), batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validation loop\n",
    "    best_model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_losses = []\n",
    "        for batch_X, batch_y in val_loader:\n",
    "            batch_X = batch_X.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            outputs = best_model(batch_X)\n",
    "            val_loss = criterion(outputs.squeeze(), batch_y)\n",
    "            val_losses.append(val_loss.item())\n",
    "        avg_val_loss = np.mean(val_losses)\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Validation Loss: {avg_val_loss:.4f}')\n",
    "\n",
    "# Save the best model to a file\n",
    "torch.save(best_model.state_dict(), 'best_rnn_model_optuna.pth')\n",
    "print(\"Best RNN model saved to 'best_rnn_model_optuna.pth'\")\n",
    "\n",
    "# Evaluate on test data (optional)\n",
    "best_model.eval()\n",
    "test_tensor = torch.FloatTensor(processed_test_data).to(device)\n",
    "with torch.no_grad():\n",
    "    test_predictions = best_model(test_tensor).numpy().squeeze()\n",
    "\n",
    "# Calculate RMSE on test data\n",
    "test_rmse = np.sqrt(np.mean((test_predictions - true_rul)**2))\n",
    "print(f'Test RMSE: {test_rmse:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c30fb12-6ad7-4916-8c64-11579c40f289",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
